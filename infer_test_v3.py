"""
JaT-AudioSR V3 æ¨ç†è„šæœ¬ï¼ˆæ”¯æŒè‡ªåŠ¨åˆ‡ç‰‡+æ‹¼æ¥ï¼‰

ç‰¹æ€§ï¼š
- å›ºå®š16ç§’chunkï¼ˆä¸è®­ç»ƒå¯¹é½ï¼‰
- è‡ªåŠ¨åˆ‡ç‰‡é•¿éŸ³é¢‘
- 2ç§’overlap + çº¿æ€§crossfadeæ‹¼æ¥
- æ”¯æŒæŒ‡å®šæ€»è¾“å‡ºé•¿åº¦
"""

import torch
import torchaudio
import os
import sys
import json
from pathlib import Path
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from models.jat_audiosr_v2 import JaT_AudioSR_V2
import dac


def load_model(checkpoint_path, device='cuda'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"ğŸ“¦ Loading checkpoint from: {checkpoint_path}")

    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    # ä»checkpointè·å–æ¨¡å‹é…ç½®
    model_config = checkpoint.get('config', {
        'input_channels': 1024,
        'cond_channels': 1024,
        'patch_len': 4,
        'hidden_size': 1280,
        'depth': 28,
        'num_q_heads': 20,
        'num_kv_heads': 4,
        'bottleneck_dim': 512,
        'mlp_ratio': 4.0,
        'dropout': 0.1,         # ğŸ”¥ ä¿®å¤è¿‡æ‹Ÿåˆï¼šå¼€å¯Dropout
        'drop_path_rate': 0.05
    })

    # åˆ›å»ºæ¨¡å‹
    model = JaT_AudioSR_V2(**model_config).to(device)

    # åŠ è½½æƒé‡ï¼ˆå¤„ç†å¯èƒ½çš„å…¼å®¹æ€§é—®é¢˜ï¼‰
    state_dict = checkpoint['model_state_dict']

    # 1. ç§»é™¤ torch.compile å‰ç¼€
    if any(k.startswith('_orig_mod.') for k in state_dict.keys()):
        state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}

    # 2. å¤„ç† Dropout å±‚ç´¢å¼•å˜åŒ–ï¼ˆæ—§checkpointæ²¡æœ‰Dropoutï¼‰
    # æ—§æ¨¡å‹: mlp = [Linear(0), GELU(1), Linear(2)]
    # æ–°æ¨¡å‹: mlp = [Linear(0), GELU(1), Dropout(2), Linear(3), Dropout(4)]
    # éœ€è¦å°†æ—§çš„ mlp.2.* æ˜ å°„åˆ°æ–°çš„ mlp.3.*
    new_state_dict = {}
    for k, v in state_dict.items():
        # æ£€æŸ¥æ˜¯å¦æ˜¯ blocks.*.mlp.2.* (æ—§çš„æœ€åä¸€å±‚Linear)
        if '.mlp.2.weight' in k or '.mlp.2.bias' in k:
            # æ˜ å°„åˆ° mlp.3.*
            k = k.replace('.mlp.2.', '.mlp.3.')
            print(f"  Remapping: mlp.2 â†’ mlp.3 (adding Dropout compatibility)")
        new_state_dict[k] = v

    model.load_state_dict(new_state_dict, strict=False)
    model.eval()

    epoch = checkpoint.get('epoch', 0)
    step = checkpoint.get('global_step', 0)
    print(f"âœ… Model loaded (Epoch {epoch}, Step {step})")

    return model


def load_dac_codec(device='cuda'):
    """åŠ è½½DACç¼–è§£ç å™¨"""
    print("ğŸ“¦ Loading DAC codec (44.1kHz)...")
    model_path = dac.utils.download(model_type="44khz")
    dac_model = dac.DAC.load(model_path).to(device)
    dac_model.eval()
    print("âœ… DAC codec loaded")
    return dac_model


@torch.no_grad()
def flow_matching_sample(model, lr_latent, num_steps=50, device='cuda', verbose=True):
    """
    Flow Matching ODEé‡‡æ ·ï¼ˆEuleræ–¹æ³•ï¼‰
    ä½¿ç”¨ x-prediction (é¢„æµ‹å¹²å‡€çš„ x_0)

    è®­ç»ƒæ—¶æ’å€¼å…¬å¼: z_t = t * x_0 + (1-t) * noise
    - t=0: çº¯å™ªå£°
    - t=1: å¹²å‡€æ•°æ®

    Args:
        model: JaTæ¨¡å‹
        lr_latent: [1, C, T] LRæ¡ä»¶latent (å·²å½’ä¸€åŒ–)
        num_steps: ODEæ±‚è§£æ­¥æ•°
        device: è®¾å¤‡
        verbose: æ˜¯å¦æ‰“å°è¿›åº¦
    Returns:
        [1, C, T] ç”Ÿæˆçš„HR latent (å·²å½’ä¸€åŒ–)
    """
    if verbose:
        print(f"  ğŸ² Flow Matching sampling ({num_steps} steps)...")

    B, C, T = lr_latent.shape

    # åˆå§‹åŒ–ï¼šä»çº¯å™ªå£°å¼€å§‹ (t=0)
    z_t = torch.randn(B, C, T, device=device)

    # æ—¶é—´æ­¥: 0.0 â†’ 1.0 (ä»å™ªå£°èµ°å‘å¹²å‡€æ•°æ®)
    timesteps = torch.linspace(0.0, 1.0, num_steps + 1, device=device)

    for i in range(num_steps):
        t_curr = timesteps[i]
        t_next = timesteps[i + 1]
        dt = t_next - t_curr  # æ­£æ•°ï¼Œä»0åˆ°1

        # æ¨¡å‹è¾“å…¥ï¼šå½“å‰æ—¶é—´æ­¥
        t_batch = torch.full((B,), t_curr, device=device)

        # æ¨¡å‹é¢„æµ‹ x_0 (å¹²å‡€çš„HR latent)
        x_pred = model(z_t, t_batch, lr_latent)

        # Flow Matching ODEæ¨å¯¼:
        # z_t = t * x_0 + (1-t) * noise
        # dz/dt = x_0 - noise = x_0 - (z_t - t*x_0)/(1-t) = (x_0 - z_t) / (1-t)

        if t_curr < 0.999:
            # æ­£å¸¸ODEæ­¥è¿›
            velocity = (x_pred - z_t) / (1 - t_curr + 1e-5)
            z_t = z_t + velocity * dt
        else:
            # tæ¥è¿‘1æ—¶ï¼Œç›´æ¥ä½¿ç”¨é¢„æµ‹å€¼ï¼ˆé¿å…é™¤ä»¥0ï¼‰
            z_t = x_pred

        # æ‰“å°è¿›åº¦
        if verbose and ((i + 1) % 10 == 0 or i == num_steps - 1):
            print(f"    Step {i+1}/{num_steps}, t={t_curr:.3f} â†’ {t_next:.3f}")

    return z_t


def crossfade_chunks(chunks, overlap_frames):
    """
    ä½¿ç”¨çº¿æ€§crossfadeæ‹¼æ¥å¤šä¸ªchunk

    Args:
        chunks: List of [1, C, T] tensors
        overlap_frames: overlapåŒºåŸŸçš„å¸§æ•°
    Returns:
        [1, C, total_T] æ‹¼æ¥åçš„tensor
    """
    if len(chunks) == 0:
        return None
    if len(chunks) == 1:
        return chunks[0]

    result = chunks[0]

    for i in range(1, len(chunks)):
        prev_chunk = result
        curr_chunk = chunks[i]

        if overlap_frames > 0 and prev_chunk.shape[-1] >= overlap_frames:
            # æå–overlapåŒºåŸŸ
            prev_overlap = prev_chunk[..., -overlap_frames:]  # [1, C, overlap]
            curr_overlap = curr_chunk[..., :overlap_frames]   # [1, C, overlap]

            # çº¿æ€§crossfadeæƒé‡
            fade_out = torch.linspace(1.0, 0.0, overlap_frames, device=prev_chunk.device)
            fade_in = torch.linspace(0.0, 1.0, overlap_frames, device=curr_chunk.device)
            fade_out = fade_out.view(1, 1, -1)
            fade_in = fade_in.view(1, 1, -1)

            # Crossfade
            blended = prev_overlap * fade_out + curr_overlap * fade_in

            # æ‹¼æ¥ï¼šprev[:-overlap] + blended + curr[overlap:]
            result = torch.cat([
                prev_chunk[..., :-overlap_frames],
                blended,
                curr_chunk[..., overlap_frames:]
            ], dim=-1)
        else:
            # æ— overlapï¼Œç›´æ¥æ‹¼æ¥
            result = torch.cat([prev_chunk, curr_chunk], dim=-1)

    return result


def main():
    parser = argparse.ArgumentParser(description='JaT-AudioSR V2 Inference Test')
    parser.add_argument('--checkpoint', type=str, default='checkpoints/v2_full_run/last.pt',
                        help='Checkpoint path')
    parser.add_argument('--val-dir', type=str, default='data_processed_v13_final/val',
                        help='Validation latents directory')
    parser.add_argument('--stats-file', type=str, default='data_processed_v13_final/global_stats_separated.json',
                        help='Normalization stats file (JSON or PT)')
    parser.add_argument('--output-dir', type=str, default='inference_output',
                        help='Output directory')
    parser.add_argument('--steps', type=int, default=50,
                        help='Number of sampling steps')
    parser.add_argument('--total-seconds', type=float, default=60.0,
                        help='Total output duration in seconds (None=use full input length)')
    parser.add_argument('--device', type=str, default='cuda',
                        help='Device (cuda or cpu)')
    parser.add_argument('--input-file', type=str, default=None,
                        help='Specific input file (e.g., "04 - æ‰“å¼€ä¿¡ç®±.pt"). If not specified, uses first file in val-dir')
    args = parser.parse_args()

    device = args.device
    os.makedirs(args.output_dir, exist_ok=True)

    print("=" * 60)
    print("JaT-AudioSR V3 æ¨ç†æµ‹è¯•")
    print("=" * 60)

    # 1. åŠ è½½æ¨¡å‹
    model = load_model(args.checkpoint, device=device)

    # 2. åŠ è½½DACç¼–è§£ç å™¨
    dac_codec = load_dac_codec(device=device)

    # 3. åŠ è½½éªŒè¯é›†æ–‡ä»¶
    if args.input_file:
        # ä½¿ç”¨æŒ‡å®šçš„æ–‡ä»¶
        input_path = Path(args.val_dir) / args.input_file
        if not input_path.exists():
            print(f"âŒ File not found: {input_path}")
            print(f"Available files in {args.val_dir}:")
            for f in sorted(Path(args.val_dir).glob("*.pt"))[:10]:
                print(f"  - {f.name}")
            return
        first_file = input_path
        print(f"\nğŸ“‚ Loading specified file: {first_file.name}")
    else:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
        val_files = sorted(Path(args.val_dir).glob("*.pt"))
        if len(val_files) == 0:
            print(f"âŒ No files found in {args.val_dir}")
            return
        first_file = val_files[0]
        print(f"\nğŸ“‚ Loading validation file (first): {first_file.name}")

    data = torch.load(first_file, map_location='cpu', mmap=True, weights_only=False)
    hr_latent = data['hr_latent'].float()  # [1024, T]
    lr_latent = data['lr_latent'].float()  # [1024, T]

    # å½’ä¸€åŒ–å‚æ•°
    print(f"\nğŸ“Š Loading normalization stats from: {args.stats_file}")

    # æ ¹æ®æ–‡ä»¶æ‰©å±•ååŠ è½½
    if args.stats_file.endswith('.json'):
        # JSONæ ¼å¼
        with open(args.stats_file, 'r') as f:
            stats = json.load(f)
        hr_mean = torch.tensor(stats['hr_mean']).view(1, -1, 1).to(device)
        hr_std = torch.tensor(stats['hr_std']).view(1, -1, 1).to(device)
        lr_mean = torch.tensor(stats['lr_mean']).view(1, -1, 1).to(device)
        lr_std = torch.tensor(stats['lr_std']).view(1, -1, 1).to(device)
    else:
        # PyTorchæ ¼å¼
        stats = torch.load(args.stats_file, weights_only=False)

        if 'hr_mean' in stats:
            # æ ¼å¼1: ç›´æ¥çš„ mean/std
            hr_mean = stats['hr_mean'].view(1, -1, 1).to(device)
            hr_std = stats['hr_std'].view(1, -1, 1).to(device)
            lr_mean = stats['lr_mean'].view(1, -1, 1).to(device)
            lr_std = stats['lr_std'].view(1, -1, 1).to(device)
        elif 'sum' in stats:
            # æ ¼å¼2: ç´¯ç§¯ç»Ÿè®¡é‡ (sum, sq_sum, count)
            print("  Computing mean/std from running stats...")
            count = stats['count']
            mean = stats['sum'] / count
            var = (stats['sq_sum'] / count) - (mean ** 2)
            std = torch.sqrt(var + 1e-8)

            # å‡è®¾å‰1024ç»´æ˜¯HRï¼Œå1024ç»´æ˜¯LR
            hr_mean = mean[:1024].view(1, -1, 1).to(device)
            hr_std = std[:1024].view(1, -1, 1).to(device)
            lr_mean = mean[1024:].view(1, -1, 1).to(device)
            lr_std = std[1024:].view(1, -1, 1).to(device)
        else:
            raise ValueError(f"Unknown stats format. Keys: {list(stats.keys())}")

    print(f"  HR mean: {hr_mean.mean().item():.4f}, std: {hr_std.mean().item():.4f}")
    print(f"  LR mean: {lr_mean.mean().item():.4f}, std: {lr_std.mean().item():.4f}")

    print(f"  Original length: {hr_latent.shape[-1]} frames")

    # 4. è‡ªåŠ¨åˆ‡ç‰‡å¤„ç†ï¼ˆä¸è®­ç»ƒå¯¹é½ï¼š16ç§’chunk + 2ç§’overlapï¼‰
    dac_sr = 44100
    dac_hop = 512
    CHUNK_DURATION = 16.0  # å›ºå®š16ç§’ï¼Œä¸è®­ç»ƒå¯¹é½
    OVERLAP_DURATION = 2.0  # 2ç§’overlapç”¨äºcrossfade

    chunk_frames = int(CHUNK_DURATION * dac_sr / dac_hop)  # 1378 frames
    overlap_frames = int(OVERLAP_DURATION * dac_sr / dac_hop)  # ~172 frames

    # è®¡ç®—éœ€è¦å¤„ç†çš„æ€»å¸§æ•°
    total_frames = hr_latent.shape[-1]
    if args.total_seconds is not None:
        total_frames = min(total_frames, int(args.total_seconds * dac_sr / dac_hop))

    print(f"\nğŸ¯ Processing configuration:")
    print(f"  Chunk size: {CHUNK_DURATION}s ({chunk_frames} frames)")
    print(f"  Overlap: {OVERLAP_DURATION}s ({overlap_frames} frames)")
    print(f"  Total to process: {total_frames} frames ({total_frames * dac_hop / dac_sr:.1f}s)")

    # è®¡ç®—éœ€è¦å¤šå°‘ä¸ªchunk
    stride = chunk_frames - overlap_frames
    num_chunks = (total_frames - overlap_frames + stride - 1) // stride
    print(f"  Number of chunks: {num_chunks}")

    # 5. é€ä¸ªchunkå¤„ç†
    print("\nğŸ¨ Generating HR audio (chunk by chunk)...")
    generated_chunks = []
    hr_chunks_list = []
    lr_chunks_list = []

    for chunk_idx in range(num_chunks):
        start = chunk_idx * stride
        end = min(start + chunk_frames, total_frames)

        print(f"\nğŸ“¦ Chunk {chunk_idx + 1}/{num_chunks} (frames {start}:{end})")

        # æå–chunk
        hr_chunk = hr_latent[:, start:end].unsqueeze(0).to(device)  # [1, 1024, T]
        lr_chunk = lr_latent[:, start:end].unsqueeze(0).to(device)  # [1, 1024, T]

        # å½’ä¸€åŒ–
        hr_chunk_norm = (hr_chunk - hr_mean) / hr_std
        lr_chunk_norm = (lr_chunk - lr_mean) / lr_std

        # Flow Matchingé‡‡æ ·
        gen_chunk = flow_matching_sample(
            model, lr_chunk_norm, num_steps=args.steps, device=device, verbose=True
        )

        # åå½’ä¸€åŒ–
        gen_chunk = gen_chunk * hr_std + hr_mean

        generated_chunks.append(gen_chunk)
        hr_chunks_list.append(hr_chunk)
        lr_chunks_list.append(lr_chunk)

    # 6. Crossfadeæ‹¼æ¥
    print(f"\nğŸ”— Crossfading {len(generated_chunks)} chunks...")
    generated_latent = crossfade_chunks(generated_chunks, overlap_frames)
    hr_chunk = crossfade_chunks(hr_chunks_list, overlap_frames)
    lr_chunk = crossfade_chunks(lr_chunks_list, overlap_frames)

    print(f"  Final length: {generated_latent.shape[-1]} frames ({generated_latent.shape[-1] * dac_hop / dac_sr:.1f}s)")

    # 8. DACè§£ç 
    print("\nğŸ”Š Decoding with DAC...")

    # è§£ç ç”Ÿæˆçš„éŸ³é¢‘
    with torch.no_grad():
        # DAC expects [B, latent_dim, n_codebooks, T]
        # ä½†æˆ‘ä»¬çš„latentæ˜¯ [B, 1024, T]ï¼Œ1024 = 72 codebooks * 1024 per book?
        # å®é™…ä¸ŠDACçš„latentç»´åº¦åº”è¯¥æ˜¯ [B, n_codebooks, T]
        # æ£€æŸ¥DACçš„è¾“å…¥æ ¼å¼

        # å°† [1, 1024, T] reshape ä¸º DACæ ¼å¼
        # DAC 44.1kHz: n_codebooks=9, codebook_size=1024
        # æ‰€ä»¥ generated_latent åº”è¯¥å·²ç»æ˜¯æ­£ç¡®çš„æ ¼å¼äº†

        generated_audio = dac_codec.decode(generated_latent)  # [1, 1, samples]
        hr_audio = dac_codec.decode(hr_chunk)  # [1, 1, samples]
        lr_audio = dac_codec.decode(lr_chunk)  # [1, 1, samples]

    generated_audio = generated_audio.squeeze(0).cpu()  # [1, samples]
    hr_audio = hr_audio.squeeze(0).cpu()
    lr_audio = lr_audio.squeeze(0).cpu()

    # 9. ä¿å­˜éŸ³é¢‘
    print("\nğŸ’¾ Saving results...")

    output_prefix = first_file.stem
    torchaudio.save(
        f"{args.output_dir}/{output_prefix}_generated.wav",
        generated_audio, dac_sr
    )
    torchaudio.save(
        f"{args.output_dir}/{output_prefix}_hr_gt.wav",
        hr_audio, dac_sr
    )
    torchaudio.save(
        f"{args.output_dir}/{output_prefix}_lr_input.wav",
        lr_audio, dac_sr
    )

    print(f"âœ… Saved to {args.output_dir}/")
    print(f"   - {output_prefix}_generated.wav (æ¨¡å‹ç”Ÿæˆ)")
    print(f"   - {output_prefix}_hr_gt.wav (Ground Truth)")
    print(f"   - {output_prefix}_lr_input.wav (LRè¾“å…¥)")
    print("=" * 60)


if __name__ == '__main__':
    main()
