"""
JaT-AudioSR V3 æ¨ç†è„šæœ¬ï¼ˆæ”¯æŒè‡ªåŠ¨åˆ‡ç‰‡+æ‹¼æ¥+CFGï¼‰

V3æ”¹è¿›ï¼š
- RMSNormï¼ˆPyTorch 2.8+å®˜æ–¹å®ç°ï¼‰
- æ›´å¤§æ¨¡å‹ï¼ˆ1280-dim, 28-layerï¼‰
- BFloat16è®­ç»ƒ
- CFGè®­ç»ƒï¼ˆ10% condition dropoutï¼‰

æ¨ç†ç‰¹æ€§ï¼š
- å›ºå®š16ç§’chunkï¼ˆä¸è®­ç»ƒå¯¹é½ï¼‰
- è‡ªåŠ¨åˆ‡ç‰‡é•¿éŸ³é¢‘
- 2ç§’overlap + çº¿æ€§crossfadeæ‹¼æ¥
- æ”¯æŒæŒ‡å®šæ€»è¾“å‡ºé•¿åº¦
- æ”¯æŒCFG guidance
"""

import torch
import torchaudio
import os
import sys
import json
from pathlib import Path
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from models.jat_audiosr_v3 import JaT_AudioSR_V3
import dac


def load_model(checkpoint_path, device='cuda'):
    """åŠ è½½è®­ç»ƒå¥½çš„V3æ¨¡å‹"""
    print(f"ğŸ“¦ Loading V3 checkpoint from: {checkpoint_path}")

    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    # V3æ¨¡å‹é…ç½®ï¼ˆé»˜è®¤å€¼ï¼Œå¦‚æœcheckpointæ²¡æœ‰ä¿å­˜configï¼‰
    model_config = checkpoint.get('config', {
        'input_channels': 1024,
        'cond_channels': 1024,
        'patch_len': 4,
        'hidden_size': 1280,      # V3: 1280 (V2æ˜¯1024)
        'depth': 28,              # V3: 28å±‚ (V2æ˜¯16å±‚)
        'num_q_heads': 20,        # V3: 20 (V2æ˜¯16)
        'num_kv_heads': 4,
        'bottleneck_dim': 512,
        'mlp_ratio': 4.0,
        'dropout': 0.1,
        'drop_path_rate': 0.05
    })

    print(f"  Model config: {model_config['hidden_size']}-dim, {model_config['depth']}-layer")

    # åˆ›å»ºV3æ¨¡å‹
    model = JaT_AudioSR_V3(**model_config).to(device)

    # åŠ è½½æƒé‡
    state_dict = checkpoint['model_state_dict']

    # 1. ç§»é™¤ torch.compile å‰ç¼€
    if any(k.startswith('_orig_mod.') for k in state_dict.keys()):
        state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}
        print(f"  Removed torch.compile prefix (_orig_mod.)")

    # 2. ç§»é™¤ DDP å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if any(k.startswith('module.') for k in state_dict.keys()):
        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
        print(f"  Removed DDP prefix (module.)")

    # åŠ è½½æƒé‡
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)

    if missing_keys:
        print(f"  âš ï¸  Missing keys: {len(missing_keys)}")
        if len(missing_keys) <= 5:
            for key in missing_keys:
                print(f"    - {key}")

    if unexpected_keys:
        print(f"  âš ï¸  Unexpected keys: {len(unexpected_keys)}")
        if len(unexpected_keys) <= 5:
            for key in unexpected_keys:
                print(f"    - {key}")

    model.eval()

    epoch = checkpoint.get('epoch', 0)
    step = checkpoint.get('global_step', 0)
    print(f"âœ… V3 Model loaded (Epoch {epoch}, Step {step})")

    return model


def load_dac_codec(device='cuda'):
    """åŠ è½½DACç¼–è§£ç å™¨"""
    print("ğŸ“¦ Loading DAC codec (44.1kHz)...")
    model_path = dac.utils.download(model_type="44khz")
    dac_model = dac.DAC.load(model_path).to(device)
    dac_model.eval()
    print("âœ… DAC codec loaded")
    return dac_model


@torch.no_grad()
def flow_matching_sample(model, lr_latent, num_steps=50, cfg_scale=1.0, device='cuda', verbose=True):
    """
    Flow Matching ODEé‡‡æ ·ï¼ˆEuleræ–¹æ³•ï¼‰+ CFGæ”¯æŒ
    ä½¿ç”¨ x-prediction (é¢„æµ‹å¹²å‡€çš„ x_0)

    è®­ç»ƒæ—¶æ’å€¼å…¬å¼: z_t = t * x_0 + (1-t) * noise
    - t=0: çº¯å™ªå£°
    - t=1: å¹²å‡€æ•°æ®

    Args:
        model: JaT V3æ¨¡å‹
        lr_latent: [1, C, T] LRæ¡ä»¶latent (å·²å½’ä¸€åŒ–)
        num_steps: ODEæ±‚è§£æ­¥æ•°
        cfg_scale: CFGå¼•å¯¼å¼ºåº¦ï¼ˆ1.0=æ— CFG, >1.0=å¢å¼ºæ¡ä»¶ï¼‰
        device: è®¾å¤‡
        verbose: æ˜¯å¦æ‰“å°è¿›åº¦
    Returns:
        [1, C, T] ç”Ÿæˆçš„HR latent (å·²å½’ä¸€åŒ–)
    """
    if verbose:
        print(f"  ğŸ² Flow Matching sampling ({num_steps} steps, CFG scale={cfg_scale})...")

    B, C, T = lr_latent.shape

    # åˆå§‹åŒ–ï¼šä»çº¯å™ªå£°å¼€å§‹ (t=0)
    z_t = torch.randn(B, C, T, device=device)

    # æ—¶é—´æ­¥: 0.0 â†’ 1.0 (ä»å™ªå£°èµ°å‘å¹²å‡€æ•°æ®)
    timesteps = torch.linspace(0.0, 1.0, num_steps + 1, device=device)

    # CFGéœ€è¦åŒæ—¶è¿è¡Œæ¡ä»¶å’Œæ— æ¡ä»¶
    use_cfg = cfg_scale != 1.0
    if use_cfg:
        # å‡†å¤‡æ— æ¡ä»¶è¾“å…¥ï¼ˆå…¨é›¶ï¼‰
        lr_uncond = torch.zeros_like(lr_latent)

    for i in range(num_steps):
        t_curr = timesteps[i]
        t_next = timesteps[i + 1]
        dt = t_next - t_curr

        # æ¨¡å‹è¾“å…¥ï¼šå½“å‰æ—¶é—´æ­¥
        t_batch = torch.full((B,), t_curr, device=device)

        if use_cfg:
            # CFG: åŒæ—¶é¢„æµ‹æ¡ä»¶å’Œæ— æ¡ä»¶
            z_t_input = torch.cat([z_t, z_t], dim=0)
            t_input = torch.cat([t_batch, t_batch], dim=0)
            lr_input = torch.cat([lr_latent, lr_uncond], dim=0)

            x_pred_both = model(z_t_input, t_input, lr_input)

            # åˆ†ç¦»æ¡ä»¶å’Œæ— æ¡ä»¶é¢„æµ‹
            x_pred_cond, x_pred_uncond = x_pred_both.chunk(2, dim=0)

            # CFGå…¬å¼: x_pred = x_uncond + scale * (x_cond - x_uncond)
            x_pred = x_pred_uncond + cfg_scale * (x_pred_cond - x_pred_uncond)
        else:
            # æ— CFGï¼šç›´æ¥é¢„æµ‹
            x_pred = model(z_t, t_batch, lr_latent)

        # Flow Matching ODEæ¨å¯¼:
        # z_t = t * x_0 + (1-t) * noise
        # dz/dt = x_0 - noise = (x_0 - z_t) / (1-t)

        if t_curr < 0.999:
            # æ­£å¸¸ODEæ­¥è¿›
            velocity = (x_pred - z_t) / (1 - t_curr + 1e-5)
            z_t = z_t + velocity * dt
        else:
            # tæ¥è¿‘1æ—¶ï¼Œç›´æ¥ä½¿ç”¨é¢„æµ‹å€¼ï¼ˆé¿å…é™¤ä»¥0ï¼‰
            z_t = x_pred

        # æ‰“å°è¿›åº¦
        if verbose and ((i + 1) % 10 == 0 or i == num_steps - 1):
            print(f"    Step {i+1}/{num_steps}, t={t_curr:.3f} â†’ {t_next:.3f}")

    return z_t


def crossfade_chunks(chunks, overlap_frames):
    """
    ä½¿ç”¨çº¿æ€§crossfadeæ‹¼æ¥å¤šä¸ªchunk

    Args:
        chunks: List of [1, C, T] tensors
        overlap_frames: overlapåŒºåŸŸçš„å¸§æ•°
    Returns:
        [1, C, total_T] æ‹¼æ¥åçš„tensor
    """
    if len(chunks) == 0:
        return None
    if len(chunks) == 1:
        return chunks[0]

    result = chunks[0]

    for i in range(1, len(chunks)):
        prev_chunk = result
        curr_chunk = chunks[i]

        if overlap_frames > 0 and prev_chunk.shape[-1] >= overlap_frames:
            # æå–overlapåŒºåŸŸ
            prev_overlap = prev_chunk[..., -overlap_frames:]  # [1, C, overlap]
            curr_overlap = curr_chunk[..., :overlap_frames]   # [1, C, overlap]

            # çº¿æ€§crossfadeæƒé‡
            fade_out = torch.linspace(1.0, 0.0, overlap_frames, device=prev_chunk.device)
            fade_in = torch.linspace(0.0, 1.0, overlap_frames, device=curr_chunk.device)
            fade_out = fade_out.view(1, 1, -1)
            fade_in = fade_in.view(1, 1, -1)

            # Crossfade
            blended = prev_overlap * fade_out + curr_overlap * fade_in

            # æ‹¼æ¥ï¼šprev[:-overlap] + blended + curr[overlap:]
            result = torch.cat([
                prev_chunk[..., :-overlap_frames],
                blended,
                curr_chunk[..., overlap_frames:]
            ], dim=-1)
        else:
            # æ— overlapï¼Œç›´æ¥æ‹¼æ¥
            result = torch.cat([prev_chunk, curr_chunk], dim=-1)

    return result


def main():
    parser = argparse.ArgumentParser(description='JaT-AudioSR V3 Inference Test')
    parser.add_argument('--checkpoint', type=str, default='checkpoints/v3_full_run/last.pt',
                        help='V3 Checkpoint path')
    parser.add_argument('--val-dir', type=str, default='data_processed_v13_final/val',
                        help='Validation latents directory')
    parser.add_argument('--stats-file', type=str, default='data_processed_v13_final/global_stats_separated.json',
                        help='Normalization stats file (JSON or PT)')
    parser.add_argument('--output-dir', type=str, default='inference_output_v3',
                        help='Output directory')
    parser.add_argument('--steps', type=int, default=50,
                        help='Number of sampling steps')
    parser.add_argument('--cfg-scale', type=float, default=1.0,
                        help='CFG guidance scale (1.0=no CFG, 1.5-3.0 recommended for stronger conditioning)')
    parser.add_argument('--total-seconds', type=float, default=None,
                        help='Total output duration in seconds (None=use full input length)')
    parser.add_argument('--device', type=str, default='cuda',
                        help='Device (cuda or cpu)')
    parser.add_argument('--input-file', type=str, default=None,
                        help='Specific input file (e.g., "04 - æ‰“å¼€ä¿¡ç®±.pt"). If not specified, uses first file in val-dir')
    args = parser.parse_args()

    device = args.device
    os.makedirs(args.output_dir, exist_ok=True)

    print("=" * 60)
    print("JaT-AudioSR V3 æ¨ç†æµ‹è¯•")
    print("=" * 60)

    # 1. åŠ è½½V3æ¨¡å‹
    model = load_model(args.checkpoint, device=device)

    # 2. åŠ è½½DACç¼–è§£ç å™¨
    dac_codec = load_dac_codec(device=device)

    # 3. åŠ è½½éªŒè¯é›†æ–‡ä»¶
    if args.input_file:
        # ä½¿ç”¨æŒ‡å®šçš„æ–‡ä»¶
        input_path = Path(args.val_dir) / args.input_file
        if not input_path.exists():
            print(f"âŒ File not found: {input_path}")
            print(f"Available files in {args.val_dir}:")
            for f in sorted(Path(args.val_dir).glob("*.pt"))[:10]:
                print(f"  - {f.name}")
            return
        first_file = input_path
        print(f"\nğŸ“‚ Loading specified file: {first_file.name}")
    else:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
        val_files = sorted(Path(args.val_dir).glob("*.pt"))
        if len(val_files) == 0:
            print(f"âŒ No files found in {args.val_dir}")
            return
        first_file = val_files[0]
        print(f"\nğŸ“‚ Loading validation file (first): {first_file.name}")

    data = torch.load(first_file, map_location='cpu', mmap=True, weights_only=False)
    hr_latent = data['hr_latent'].float()  # [1024, T]
    lr_latent = data['lr_latent'].float()  # [1024, T]

    # å½’ä¸€åŒ–å‚æ•°
    print(f"\nğŸ“Š Loading normalization stats from: {args.stats_file}")

    # æ ¹æ®æ–‡ä»¶æ‰©å±•ååŠ è½½
    if args.stats_file.endswith('.json'):
        # JSONæ ¼å¼
        with open(args.stats_file, 'r') as f:
            stats = json.load(f)
        hr_mean = torch.tensor(stats['hr_mean']).view(1, -1, 1).to(device)
        hr_std = torch.tensor(stats['hr_std']).view(1, -1, 1).to(device)
        lr_mean = torch.tensor(stats['lr_mean']).view(1, -1, 1).to(device)
        lr_std = torch.tensor(stats['lr_std']).view(1, -1, 1).to(device)
    else:
        # PyTorchæ ¼å¼
        stats = torch.load(args.stats_file, weights_only=False)

        if 'hr_mean' in stats:
            # æ ¼å¼1: ç›´æ¥çš„ mean/std
            hr_mean = stats['hr_mean'].view(1, -1, 1).to(device)
            hr_std = stats['hr_std'].view(1, -1, 1).to(device)
            lr_mean = stats['lr_mean'].view(1, -1, 1).to(device)
            lr_std = stats['lr_std'].view(1, -1, 1).to(device)
        elif 'sum' in stats:
            # æ ¼å¼2: ç´¯ç§¯ç»Ÿè®¡é‡ (sum, sq_sum, count)
            print("  Computing mean/std from running stats...")
            count = stats['count']
            mean = stats['sum'] / count
            var = (stats['sq_sum'] / count) - (mean ** 2)
            std = torch.sqrt(var + 1e-8)

            # å‡è®¾å‰1024ç»´æ˜¯HRï¼Œå1024ç»´æ˜¯LR
            hr_mean = mean[:1024].view(1, -1, 1).to(device)
            hr_std = std[:1024].view(1, -1, 1).to(device)
            lr_mean = mean[1024:].view(1, -1, 1).to(device)
            lr_std = std[1024:].view(1, -1, 1).to(device)
        else:
            raise ValueError(f"Unknown stats format. Keys: {list(stats.keys())}")

    print(f"  HR mean: {hr_mean.mean().item():.4f}, std: {hr_std.mean().item():.4f}")
    print(f"  LR mean: {lr_mean.mean().item():.4f}, std: {lr_std.mean().item():.4f}")

    print(f"  Original length: {hr_latent.shape[-1]} frames")

    # 4. è‡ªåŠ¨åˆ‡ç‰‡å¤„ç†ï¼ˆä¸è®­ç»ƒå¯¹é½ï¼š16ç§’chunk + 2ç§’overlapï¼‰
    dac_sr = 44100
    dac_hop = 512
    CHUNK_DURATION = 16.0  # å›ºå®š16ç§’ï¼Œä¸è®­ç»ƒå¯¹é½
    OVERLAP_DURATION = 2.0  # 2ç§’overlapç”¨äºcrossfade

    chunk_frames = int(CHUNK_DURATION * dac_sr / dac_hop)  # 1378 frames
    overlap_frames = int(OVERLAP_DURATION * dac_sr / dac_hop)  # ~172 frames

    # è®¡ç®—éœ€è¦å¤„ç†çš„æ€»å¸§æ•°
    total_frames = hr_latent.shape[-1]
    if args.total_seconds is not None:
        total_frames = min(total_frames, int(args.total_seconds * dac_sr / dac_hop))

    print(f"\nğŸ¯ Processing configuration:")
    print(f"  Chunk size: {CHUNK_DURATION}s ({chunk_frames} frames)")
    print(f"  Overlap: {OVERLAP_DURATION}s ({overlap_frames} frames)")
    print(f"  Total to process: {total_frames} frames ({total_frames * dac_hop / dac_sr:.1f}s)")
    print(f"  CFG scale: {args.cfg_scale}")

    # è®¡ç®—éœ€è¦å¤šå°‘ä¸ªchunk
    stride = chunk_frames - overlap_frames
    num_chunks = (total_frames - overlap_frames + stride - 1) // stride
    print(f"  Number of chunks: {num_chunks}")

    # 5. é€ä¸ªchunkå¤„ç†
    print("\nğŸ¨ Generating HR audio (chunk by chunk)...")
    generated_chunks = []
    hr_chunks_list = []
    lr_chunks_list = []

    for chunk_idx in range(num_chunks):
        start = chunk_idx * stride
        end = min(start + chunk_frames, total_frames)

        print(f"\nğŸ“¦ Chunk {chunk_idx + 1}/{num_chunks} (frames {start}:{end})")

        # æå–chunk
        hr_chunk = hr_latent[:, start:end].unsqueeze(0).to(device)  # [1, 1024, T]
        lr_chunk = lr_latent[:, start:end].unsqueeze(0).to(device)  # [1, 1024, T]

        # å½’ä¸€åŒ–
        hr_chunk_norm = (hr_chunk - hr_mean) / hr_std
        lr_chunk_norm = (lr_chunk - lr_mean) / lr_std

        # Flow Matchingé‡‡æ ·ï¼ˆæ”¯æŒCFGï¼‰
        gen_chunk = flow_matching_sample(
            model, lr_chunk_norm,
            num_steps=args.steps,
            cfg_scale=args.cfg_scale,
            device=device,
            verbose=True
        )

        # åå½’ä¸€åŒ–
        gen_chunk = gen_chunk * hr_std + hr_mean

        generated_chunks.append(gen_chunk)
        hr_chunks_list.append(hr_chunk)
        lr_chunks_list.append(lr_chunk)

    # 6. Crossfadeæ‹¼æ¥
    print(f"\nğŸ”— Crossfading {len(generated_chunks)} chunks...")
    generated_latent = crossfade_chunks(generated_chunks, overlap_frames)
    hr_chunk = crossfade_chunks(hr_chunks_list, overlap_frames)
    lr_chunk = crossfade_chunks(lr_chunks_list, overlap_frames)

    print(f"  Final length: {generated_latent.shape[-1]} frames ({generated_latent.shape[-1] * dac_hop / dac_sr:.1f}s)")

    # 8. DACè§£ç 
    print("\nğŸ”Š Decoding with DAC...")

    with torch.no_grad():
        generated_audio = dac_codec.decode(generated_latent)  # [1, 1, samples]
        hr_audio = dac_codec.decode(hr_chunk)  # [1, 1, samples]
        lr_audio = dac_codec.decode(lr_chunk)  # [1, 1, samples]

    generated_audio = generated_audio.squeeze(0).cpu()  # [1, samples]
    hr_audio = hr_audio.squeeze(0).cpu()
    lr_audio = lr_audio.squeeze(0).cpu()

    # 9. ä¿å­˜éŸ³é¢‘
    print("\nğŸ’¾ Saving results...")

    output_prefix = first_file.stem
    cfg_suffix = f"_cfg{args.cfg_scale:.1f}" if args.cfg_scale != 1.0 else ""

    torchaudio.save(
        f"{args.output_dir}/{output_prefix}_generated{cfg_suffix}.wav",
        generated_audio, dac_sr
    )
    torchaudio.save(
        f"{args.output_dir}/{output_prefix}_hr_gt.wav",
        hr_audio, dac_sr
    )
    torchaudio.save(
        f"{args.output_dir}/{output_prefix}_lr_input.wav",
        lr_audio, dac_sr
    )

    print(f"âœ… Saved to {args.output_dir}/")
    print(f"   - {output_prefix}_generated{cfg_suffix}.wav (V3æ¨¡å‹ç”Ÿæˆ)")
    print(f"   - {output_prefix}_hr_gt.wav (Ground Truth)")
    print(f"   - {output_prefix}_lr_input.wav (LRè¾“å…¥)")

    if args.cfg_scale != 1.0:
        print(f"\nğŸ’¡ Tip: å°è¯•ä¸åŒçš„CFG scaleå€¼:")
        print(f"   - 1.0: æ— CFGï¼ˆæ ‡å‡†ç”Ÿæˆï¼‰")
        print(f"   - 1.5-2.0: è½»åº¦å¼•å¯¼ï¼ˆæ¨èï¼‰")
        print(f"   - 2.5-3.0: å¼ºå¼•å¯¼ï¼ˆæ›´ç¬¦åˆæ¡ä»¶ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆï¼‰")

    print("=" * 60)


if __name__ == '__main__':
    main()
