"""
JaT æ•°æ®ä¿®æ­£è„šæœ¬ (LR Only Update) - æœ€ç»ˆä¿®å¤ç‰ˆ
================================================
åŠŸèƒ½: 
1. è¯»å–å·²ç”Ÿæˆçš„ .pt æ–‡ä»¶
2. ä¿æŒ hr_latent ä¸å˜ (èŠ‚çœ50%æ—¶é—´)
3. é‡æ–°è¯»å–æºéŸ³é¢‘ -> Resample 32k -> Encode -> è¦†ç›– lr_latent
4. ä½¿ç”¨ "redone" çŠ¶æ€æ ‡è®°é¿å…é‡å¤å¤„ç†

æ ¸å¿ƒé…ç½®:
- LOW_SR: 32000 (é’ˆå¯¹ MP3 ä¿®å¤ä¼˜åŒ–)
- Batch Size: 8
"""

import os
# ç¯å¢ƒå˜é‡é…ç½®
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64,garbage_collection_threshold:0.8,expandable_segments:True"

import torch
import torchaudio
import torchaudio.functional as AF
from pathlib import Path
from tqdm import tqdm
import json
import torch.multiprocessing as mp
import time
import math
import traceback
import dac
from audiotools import AudioSignal

# ==================== âš™ï¸ é…ç½®å‚æ•° ====================

# æŒ‡å‘ä½ ä¹‹å‰ç”Ÿæˆçš„ç›®å½• (è„šæœ¬ä¼šåŸåœ°ä¿®æ”¹è¿™é‡Œé¢çš„æ–‡ä»¶)
OUTPUT_DIR = 'data_processed_v13_final' 
TRAIN_DIR = os.path.join(OUTPUT_DIR, 'train')
VAL_DIR = os.path.join(OUTPUT_DIR, 'val')

# æ–°çš„æ—¥å¿—æ–‡ä»¶å’Œç»Ÿè®¡æ–‡ä»¶
LOG_FILE = os.path.join(OUTPUT_DIR, 'processed_files_redone.jsonl') 
STATS_FILE = os.path.join(OUTPUT_DIR, 'running_stats_redone.pt')
FINAL_STATS_JSON = os.path.join(OUTPUT_DIR, 'global_stats_redone.json')

# å…³é”®å‚æ•°ï¼š32kHz (16k Bandwidth)
HIGH_SR = 48000
LOW_SR = 32000

# ä¿æŒä¸ V13 ä¸€è‡´çš„åˆ‡ç‰‡å‚æ•°
CHUNK_DURATION = 7.0    
OVERLAP_DURATION = 0.5  
MIN_DURATION = 1.0     

DAC_BATCH_SIZE = 1  
DAC_MODEL_TYPE = "44khz"

# ==================== ğŸ› ï¸ å·¥å…·å‡½æ•° ====================

def save_jsonl(data, filepath):
    with open(filepath, 'a') as f:
        f.write(json.dumps(data, ensure_ascii=False) + '\n')

def load_redone_files(filepath):
    """åªåŠ è½½çŠ¶æ€ä¸º 'redone' çš„æ–‡ä»¶"""
    processed = set()
    if not os.path.exists(filepath):
        return processed
    print(f"ğŸ“‚ è¯»å–ä¿®æ­£æ—¥å¿—: {filepath}")
    with open(filepath, 'r') as f:
        for line in f:
            try:
                line = line.strip()
                if not line: continue
                data = json.loads(line)
                if data.get('status') == 'redone':
                    processed.add(data['path'])
            except Exception:
                pass
    return processed

# ==================== ğŸ—ï¸ GPU Worker (ä¿®æ­£ç‰ˆ) ====================

def gpu_worker(gpu_id, input_queue, result_queue):
    try:
        # 1. åˆå§‹åŒ–
        device = f'cuda:{gpu_id}'
        torch.cuda.set_device(device)
        
        print(f"ğŸš€ [GPU {gpu_id}] Refine Worker å°±ç»ª (Target: {LOW_SR}Hz)")

        # 2. åŠ è½½æ¨¡å‹
        model_path = str(dac.utils.download(model_type=DAC_MODEL_TYPE))
        dac_model = dac.DAC.load(model_path)
        dac_model = dac_model.to(device)
        dac_model.eval()
        
        # 3. å¾ªç¯å¤„ç†
        while True:
            task = input_queue.get()
            if task is None: break
            
            pt_path = task # è¿™é‡Œçš„ task æ˜¯ .pt æ–‡ä»¶çš„è·¯å¾„
            
            try:
                # --- A. è¯»å–ç°æœ‰çš„ .pt æ–‡ä»¶ ---
                existing_data = torch.load(pt_path, map_location='cpu', weights_only=False)
                
                original_src_path = existing_data['metadata']['path']
                hr_latent_ref = existing_data['hr_latent'] # [1024, T]
                sr = existing_data['metadata']['sr']
                
                if not os.path.exists(original_src_path):
                    result_queue.put(('error', pt_path, f"Source not found: {original_src_path}"))
                    continue

                # --- B. é‡æ–°åŠ è½½åŸå§‹éŸ³é¢‘ (CPU) ---
                audio, actual_sr = torchaudio.load(original_src_path)

                # éªŒè¯é‡‡æ ·ç‡
                if actual_sr != sr:
                    result_queue.put(('error', pt_path, f"SR mismatch: metadata={sr}, actual={actual_sr}"))
                    continue

                # é¢„å¤„ç†
                if audio.shape[0] > 1: audio = audio.mean(dim=0, keepdim=True)
                peak = audio.abs().max()
                if peak > 1.0: audio = audio / peak

                total_samples_original = audio.shape[1]
                duration_sec = total_samples_original / sr
                
                # --- C. åˆ‡åˆ†é€»è¾‘ ---
                chunks_raw_cpu = []
                chunk_valid_sec = CHUNK_DURATION
                overlap_sec = OVERLAP_DURATION
                num_chunks = math.ceil(duration_sec / chunk_valid_sec)
                
                for i in range(num_chunks):
                    t_start = i * chunk_valid_sec - overlap_sec
                    t_end = t_start + chunk_valid_sec + (2 * overlap_sec)
                    idx_start = int(t_start * sr)
                    idx_end = int(t_end * sr)
                    
                    pad_left = 0
                    if idx_start < 0:
                        pad_left = -idx_start
                        idx_start = 0
                    pad_right = 0
                    if idx_end > total_samples_original:
                        pad_right = idx_end - total_samples_original
                        idx_end = total_samples_original
                    
                    chunk = audio[:, idx_start:idx_end]
                    if pad_left > 0 or pad_right > 0:
                        chunk = torch.nn.functional.pad(chunk, (pad_left, pad_right))
                    chunks_raw_cpu.append(chunk)

                # --- D. GPU å¤„ç† (åªåš LR) ---
                latents_lr_trimmed = []
                hop_length_48k = None 
                
                torch.cuda.empty_cache()

                for i in range(0, len(chunks_raw_cpu), DAC_BATCH_SIZE):
                    batch_cpu_list = chunks_raw_cpu[i : i + DAC_BATCH_SIZE]
                    
                    # å¯¹é½ Batch
                    target_len = batch_cpu_list[0].shape[-1]
                    clean_batch_list = []
                    for c in batch_cpu_list:
                        if c.shape[-1] != target_len:
                            c = torch.nn.functional.pad(c, (0, target_len - c.shape[-1]))
                        clean_batch_list.append(c)
                        
                    batch_raw = torch.stack(clean_batch_list).to(device, non_blocking=True)
                    
                    # 1. é‡é‡‡æ · (48k -> 32k -> 48k)
                    if sr != HIGH_SR:
                        batch_hr = AF.resample(batch_raw, orig_freq=sr, new_freq=HIGH_SR)
                    else:
                        batch_hr = batch_raw
                    
                    batch_lr = AF.resample(AF.resample(batch_hr, HIGH_SR, LOW_SR), LOW_SR, HIGH_SR)
                    
                    if batch_lr.shape[-1] != batch_hr.shape[-1]:
                        batch_lr = torch.nn.functional.pad(batch_lr, (0, batch_hr.shape[-1] - batch_lr.shape[-1]))

                    # 2. Encode LR Only
                    with torch.autocast(device_type="cuda", dtype=torch.float16):
                        signal_lr = AudioSignal(batch_lr, sample_rate=HIGH_SR)
                        if dac_model.sample_rate != HIGH_SR:
                            signal_lr = signal_lr.resample(dac_model.sample_rate)
                        
                        with torch.no_grad():
                            z_lr, _, _, _, _ = dac_model.encode(signal_lr.audio_data)

                    # 3. Trim
                    if hop_length_48k is None:
                        # ç”¨ batch_hr ä¼°ç®— hop_length ä¿è¯å¯¹é½
                        hop_length_48k = round(batch_hr.shape[-1] / z_lr.shape[-1])

                        overlap_samples_48k = int(OVERLAP_DURATION * HIGH_SR)
                        valid_samples_48k = int(CHUNK_DURATION * HIGH_SR)
                        trim_frames = int(overlap_samples_48k / hop_length_48k)
                        valid_frames_latent = int(valid_samples_48k / hop_length_48k)

                    z_lr_valid = z_lr[..., trim_frames : trim_frames + valid_frames_latent]
                    
                    for b in range(z_lr_valid.shape[0]):
                        latents_lr_trimmed.append(z_lr_valid[b].cpu())

                # --- E. æ‹¼æ¥ä¸å¯¹é½ ---
                full_lr = torch.cat(latents_lr_trimmed, dim=-1)
                target_frames = hr_latent_ref.shape[-1]
                
                if full_lr.shape[-1] > target_frames:
                    full_lr = full_lr[..., :target_frames]
                elif full_lr.shape[-1] < target_frames:
                    diff = target_frames - full_lr.shape[-1]
                    full_lr = torch.nn.functional.pad(full_lr, (0, diff))
                
                if full_lr.shape != hr_latent_ref.shape:
                    result_queue.put(('error', pt_path, f"Shape mismatch: Old {hr_latent_ref.shape} vs New {full_lr.shape}"))
                    continue

                # --- F. è¦†ç›–å†™å…¥ ---
                existing_data['lr_latent'] = full_lr.half()
                torch.save(existing_data, pt_path)

                # --- G. ç»Ÿè®¡é‡ ---
                f_sum_hr = hr_latent_ref.double().sum(dim=1)
                f_sq_sum_hr = (hr_latent_ref.double()**2).sum(dim=1)
                f_sum_lr = full_lr.double().sum(dim=1)
                f_sq_sum_lr = (full_lr.double()**2).sum(dim=1)
                f_count = full_lr.shape[1]

                # âœ… å…³é”®ä¿®æ­£ï¼šè¿”å› pt_path ä»¥ä¾¿å»é‡
                result_queue.put(('success', pt_path, (f_sum_hr, f_sq_sum_hr, f_sum_lr, f_sq_sum_lr, f_count)))

            except Exception as e:
                result_queue.put(('error', pt_path, str(e)))
                traceback.print_exc()

    except Exception as e:
        print(f"ğŸ”¥ [GPU {gpu_id}] å´©æºƒ: {e}")

# ==================== ğŸ§  ä¸»æ§æµç¨‹ ====================

def main():
    try: mp.set_start_method('spawn', force=True)
    except: pass
    
    print("=" * 60)
    print("ğŸš€ JaT æ•°æ®ä¿®æ­£è„šæœ¬ (LR Only Update) - Final Fix")
    print(f"âš™ï¸  New Low SR: {LOW_SR} Hz | Target: .pt files replacement")
    print("=" * 60)

    # 1. æ‰«ææ–‡ä»¶
    print("ğŸ” æ‰«æç°æœ‰çš„ .pt æ–‡ä»¶...")
    pt_files = []
    for d in [TRAIN_DIR, VAL_DIR]:
        if os.path.exists(d):
            pt_files.extend([str(p) for p in Path(d).rglob('*.pt')])
            
    if not pt_files:
        print("âŒ æœªæ‰¾åˆ° .pt æ–‡ä»¶")
        return

    # 2. è¿‡æ»¤å·²å®Œæˆ
    redone_set = load_redone_files(LOG_FILE)
    tasks_to_do = [f for f in pt_files if f not in redone_set]
    print(f"ğŸ“‚ æ€»æ–‡ä»¶: {len(pt_files)} | âœ… å·²ä¿®æ­£: {len(redone_set)} | âš¡ï¸ å¾…ä¿®æ­£: {len(tasks_to_do)}")

    if not tasks_to_do:
        print("âœ… æ‰€æœ‰æ–‡ä»¶å·²æ›´æ–°å®Œæ¯•ã€‚")
        return

    # 3. åˆå§‹åŒ–ç»Ÿè®¡é‡
    if os.path.exists(STATS_FILE):
        stats_data = torch.load(STATS_FILE)
        global_sum_hr = stats_data['sum_hr']
        global_sq_sum_hr = stats_data['sq_sum_hr']
        global_sum_lr = stats_data['sum_lr']
        global_sq_sum_lr = stats_data['sq_sum_lr']
        global_count = stats_data['count']
    else:
        global_sum_hr = torch.zeros(1024, dtype=torch.float64)
        global_sq_sum_hr = torch.zeros(1024, dtype=torch.float64)
        global_sum_lr = torch.zeros(1024, dtype=torch.float64)
        global_sq_sum_lr = torch.zeros(1024, dtype=torch.float64)
        global_count = 0

    # 4. å¯åŠ¨è¿›ç¨‹
    num_gpus = torch.cuda.device_count()
    manager = mp.Manager()
    input_queue = manager.Queue()
    result_queue = manager.Queue()
    
    for t in tasks_to_do: input_queue.put(t)
    for _ in range(num_gpus): input_queue.put(None)

    workers = []
    for i in range(num_gpus):
        p = mp.Process(target=gpu_worker, args=(i, input_queue, result_queue))
        p.start()
        workers.append(p)
    
    pbar = tqdm(total=len(tasks_to_do), desc="Refining", unit="file")
    completed_curr = 0
    
    try:
        while completed_curr < len(tasks_to_do):
            # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
            if result_queue.empty():
                # å¦‚æœæ‰€æœ‰ worker éƒ½æŒ‚äº†ï¼Œé€€å‡º
                if all(not p.is_alive() for p in workers): 
                    print("\nâš ï¸ æ‰€æœ‰ Worker å·²é€€å‡º")
                    break
                time.sleep(0.1)
                continue
            
            # è·å–ç»“æœ
            res = result_queue.get()
            status, pt_path, data = res
            
            if status == 'success':
                # è§£åŒ…ç»Ÿè®¡æ•°æ®
                s_sum_hr, s_sq_hr, s_sum_lr, s_sq_lr, s_count = data
                
                # æ›´æ–°å…¨å±€ç»Ÿè®¡
                global_sum_hr += s_sum_hr
                global_sq_sum_hr += s_sq_hr
                global_sum_lr += s_sum_lr
                global_sq_sum_lr += s_sq_lr
                global_count += s_count
                
                # å†™å…¥æ—¥å¿— (çŠ¶æ€: redone)
                save_jsonl({'path': pt_path, 'status': 'redone'}, LOG_FILE)
                
                completed_curr += 1
                pbar.update(1)
                
                # å®šæœŸä¿å­˜ç»Ÿè®¡é‡ Checkpoint
                if completed_curr % 50 == 0:
                    torch.save({
                        'sum_hr': global_sum_hr, 'sq_sum_hr': global_sq_sum_hr,
                        'sum_lr': global_sum_lr, 'sq_sum_lr': global_sq_sum_lr,
                        'count': global_count
                    }, STATS_FILE)

            elif status == 'error':
                # è®°å½•é”™è¯¯
                save_jsonl({'path': pt_path, 'status': 'error', 'msg': data}, LOG_FILE)
                completed_curr += 1
                pbar.update(1)

    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
    finally:
        pbar.close()
        print("ğŸ§¹ æ¸…ç† Worker è¿›ç¨‹...")
        for p in workers:
            if p.is_alive():
                p.terminate()
            p.join()
        
        # æ— è®ºå¦‚ä½•ä¿å­˜ä¸€æ¬¡å½“å‰çš„ç»Ÿè®¡é‡
        torch.save({
            'sum_hr': global_sum_hr, 'sq_sum_hr': global_sq_sum_hr,
            'sum_lr': global_sum_lr, 'sq_sum_lr': global_sq_sum_lr,
            'count': global_count
        }, STATS_FILE)

    # 5. è®¡ç®—æœ€ç»ˆç»Ÿè®¡ JSON
    print("\nğŸ“Š è®¡ç®—æœ€ç»ˆå…¨å±€ç»Ÿè®¡é‡...")
    if global_count > 0:
        def calc_stats(g_sum, g_sq):
            mean = g_sum / float(global_count)
            var = (g_sq / float(global_count)) - (mean ** 2)
            std = torch.sqrt(torch.clamp(var, min=1e-6))
            return mean.float().tolist(), std.float().tolist()
        
        hr_mean, hr_std = calc_stats(global_sum_hr, global_sq_sum_hr)
        lr_mean, lr_std = calc_stats(global_sum_lr, global_sq_sum_lr)
        
        final_stats = {
            'hr_mean': hr_mean,
            'hr_std': hr_std,
            'lr_mean': lr_mean,
            'lr_std': lr_std,
            'total_frames': int(global_count)
        }
        with open(FINAL_STATS_JSON, 'w') as f:
            json.dump(final_stats, f, indent=4)
        print(f"âœ… ç»Ÿè®¡é‡å·²ä¿å­˜: {FINAL_STATS_JSON}")
    else:
        print("âš ï¸ æœªå¤„ç†ä»»ä½•æ–‡ä»¶ï¼Œæ— æ³•è®¡ç®—ç»Ÿè®¡é‡ã€‚")

if __name__ == '__main__':
    main()