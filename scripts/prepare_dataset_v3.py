"""
éŸ³é¢‘æ•°æ®é›†é¢„å¤„ç†è„šæœ¬ V3 (é«˜æ€§èƒ½ä¿®å¤ç‰ˆ - åˆ†å—å¤„ç†)
ä¿®å¤:
1. ç§»é™¤åµŒå¥—è¿›ç¨‹æ± ï¼Œæ”¹ç”¨ PyTorch DataLoader å¤„ç†å¤šè¿›ç¨‹æ•°æ®åŠ è½½
2. ç§»é™¤é‡å¤æ¨¡å‹åŠ è½½ï¼Œæ¯ä¸ª GPU è¿›ç¨‹åªåŠ è½½ä¸€æ¬¡æ¨¡å‹
3. è§£å†³æ­»é”å’Œèµ„æºåˆ©ç”¨ç‡ä½çš„é—®é¢˜
4. ä¿®å¤ Pylance ç±»å‹æ£€æŸ¥é”™è¯¯ (torch.clamp å’Œ tolist)
5. ä¿®å¤ "daemonic processes are not allowed to have children" é”™è¯¯ (æ›¿æ¢ mp.Pool ä¸º mp.Process)
6. ä¿®å¤ Literal[0] has no attribute 'double' é”™è¯¯ (æ­£ç¡®åˆå§‹åŒ–ç´¯åŠ å™¨)
7. æ–°å¢ï¼šåˆ†å—å¤„ç† (Chunking) ä»¥è§£å†³ OOM é—®é¢˜
"""

import os
import torch
import torchaudio
import torchaudio.functional as AF
import numpy as np
from pathlib import Path
from tqdm import tqdm
import random
import json
import torch.multiprocessing as mp
from torch.utils.data import Dataset, DataLoader
import time
import math

# DAC
import dac
from audiotools import AudioSignal

# ==================== é…ç½®å‚æ•° ====================
SOURCE_DIRS = [
    '1_source_audio',
    'extra_audio'
]

OUTPUT_DIR = 'data_processed'
TRAIN_DIR = os.path.join(OUTPUT_DIR, 'train')
VAL_DIR = os.path.join(OUTPUT_DIR, 'val')

# éŸ³é¢‘å‚æ•°
HIGH_SR = 48000
LOW_SR = 16000
CHUNK_DURATION = 15.0  # æ¯ä¸ªåˆ‡ç‰‡ 15 ç§’ï¼Œé¿å… OOM
MIN_DURATION = 1.0     # æœ€å°é•¿åº¦ï¼Œå°äºæ­¤é•¿åº¦ä¸¢å¼ƒ

# æ•°æ®é›†åˆ’åˆ†
VAL_RATIO = 0.1
RANDOM_SEED = 42

# DAC æ¨¡å‹
DAC_MODEL_TYPE = "44khz"

# å¹¶è¡Œé…ç½®
# æ¯ä¸ª GPU åˆ†é…çš„ CPU æ ¸å¿ƒæ•°ç”¨äºæ•°æ®åŠ è½½
# å»ºè®®ï¼šæ€» CPU æ ¸å¿ƒæ•° / GPU æ•°é‡ - 2
NUM_WORKERS_PER_GPU = 8  
DAC_BATCH_SIZE = 32      # å¢å¤§ Batch Size ä»¥å–‚é¥± W7900

# ==================== æ ¸å¿ƒ Dataset ç±» ====================

class AudioLoadingDataset(Dataset):
    """
    è´Ÿè´£éŸ³é¢‘çš„è¯»å–ã€é‡é‡‡æ ·å’Œåˆ†å—ï¼Œè¿è¡Œåœ¨ CPU Worker è¿›ç¨‹ä¸­
    """
    def __init__(self, file_list, chunk_duration=CHUNK_DURATION):
        self.files = file_list
        self.chunk_duration = chunk_duration
        self.chunk_samples = int(chunk_duration * HIGH_SR)

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        filepath = self.files[idx]
        try:
            # 1. åŠ è½½éŸ³é¢‘
            # info = torchaudio.info(filepath) # å¯é€‰ï¼šå…ˆåªè¯»å…ƒæ•°æ®åˆ¤æ–­é•¿åº¦
            
            audio, sr = torchaudio.load(filepath)
            
            # è½¬å•å£°é“
            if audio.shape[0] > 1:
                audio = audio.mean(dim=0, keepdim=True)
            
            # å½’ä¸€åŒ– (é˜²æ­¢ç ´éŸ³)
            if audio.abs().max() > 1.0:
                audio = audio / audio.abs().max()

            segment = audio.squeeze(0) # [T]
            
            # 2. é‡é‡‡æ ·åˆ° 48k (HR)
            if sr != HIGH_SR:
                high_audio = AF.resample(segment, orig_freq=sr, new_freq=HIGH_SR)
            else:
                high_audio = segment
                
            # æ£€æŸ¥é•¿åº¦
            total_samples = high_audio.shape[0]
            if total_samples < int(MIN_DURATION * HIGH_SR):
                return {'valid': False} # å¤ªçŸ­

            # 3. åˆ†å—é€»è¾‘ (Chunking)
            # ä¸ºäº†ç»Ÿè®¡å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬å°½å¯èƒ½å¤šåœ°åˆ‡åˆ†å‡ºç‰‡æ®µ
            # è®­ç»ƒæ—¶é€šå¸¸éšæœºåˆ‡ï¼Œä½†è¿™é‡Œæ˜¯é¢„å¤„ç†ï¼Œæˆ‘ä»¬è¦æŠŠæ•´é¦–æ­Œå­˜ä¸‹æ¥
            # ç­–ç•¥ï¼šæŒ‰é¡ºåºåˆ‡åˆ†ï¼Œæœ€åä¸€æ®µå¦‚æœå¤ªçŸ­åˆ™ä¸¢å¼ƒæˆ–Pad
            
            chunks = []
            
            # å¦‚æœéŸ³é¢‘æ¯” chunk_duration çŸ­ï¼Œpad ä¸€ä¸‹
            if total_samples < self.chunk_samples:
                pad_len = self.chunk_samples - total_samples
                high_audio_padded = torch.nn.functional.pad(high_audio, (0, pad_len))
                chunks.append(high_audio_padded)
            else:
                # æ­£å¸¸åˆ‡åˆ†
                num_chunks = math.ceil(total_samples / self.chunk_samples)
                for i in range(num_chunks):
                    start = i * self.chunk_samples
                    end = min(start + self.chunk_samples, total_samples)
                    chunk = high_audio[start:end]
                    
                    # åªæœ‰æœ€åä¸€æ®µå¯èƒ½é•¿åº¦ä¸è¶³
                    if chunk.shape[0] < self.chunk_samples:
                        pad_len = self.chunk_samples - chunk.shape[0]
                        chunk = torch.nn.functional.pad(chunk, (0, pad_len))
                    
                    chunks.append(chunk)
            
            # 4. å¯¹æ¯ä¸ª Chunk åˆ¶ä½œ LR
            processed_chunks = []
            filename = Path(filepath).stem
            
            for i, chunk_hr in enumerate(chunks):
                # åˆ¶ä½œ LR (16k -> 48k)
                low_audio = AF.resample(chunk_hr, orig_freq=HIGH_SR, new_freq=LOW_SR)
                low_audio_upsampled = AF.resample(low_audio, orig_freq=LOW_SR, new_freq=HIGH_SR)
                
                # ç¡®ä¿é•¿åº¦ä¸€è‡´ (é‡é‡‡æ ·å¯èƒ½å¯¼è‡´ 1-2 sample çš„è¯¯å·®)
                if low_audio_upsampled.shape[0] != chunk_hr.shape[0]:
                    min_len = min(low_audio_upsampled.shape[0], chunk_hr.shape[0])
                    low_audio_upsampled = low_audio_upsampled[:min_len]
                    chunk_hr = chunk_hr[:min_len]

                processed_chunks.append({
                    'hr': chunk_hr,
                    'lr': low_audio_upsampled,
                    'name': f"{filename}_part{i:03d}", # åŠ ä¸Šåˆ†å—åç¼€
                    'path': str(filepath),
                    'duration': chunk_hr.shape[0] / HIGH_SR,
                    'valid': True
                })
            
            # è¿”å› listï¼Œcollate_fn éœ€è¦å¤„ç† list of lists
            return processed_chunks

        except Exception as e:
            print(f"âŒ Error loading {filepath}: {e}")
            return [{'valid': False}]

def chunk_collate(batch):
    """
    å¤„ç†åˆ†å—åçš„æ•°æ®
    batch æ˜¯ä¸€ä¸ª list of lists (æ¯ä¸ªæ–‡ä»¶äº§ç”Ÿå¤šä¸ª chunk)
    æˆ‘ä»¬éœ€è¦æŠŠå®ƒå±•å¹³æˆä¸€ä¸ªå¤§çš„ batch
    """
    flat_batch = []
    for chunks in batch:
        for chunk in chunks:
            if chunk['valid']:
                flat_batch.append(chunk)
                
    if len(flat_batch) == 0:
        return None

    # Stack (ç°åœ¨æ‰€æœ‰ chunk é•¿åº¦åº”è¯¥æ˜¯ä¸€æ ·çš„ï¼Œæˆ–è€…æ¥è¿‘)
    # å› ä¸ºæˆ‘ä»¬åšäº† Paddingï¼Œé•¿åº¦åº”è¯¥æ˜¯ chunk_samples
    
    # äºŒæ¬¡æ£€æŸ¥é•¿åº¦å¹¶ Pad (ä»¥é˜²ä¸‡ä¸€é‡é‡‡æ ·è¯¯å·®)
    max_len = max([b['hr'].shape[0] for b in flat_batch])
    
    hr_batch = []
    lr_batch = []
    metadata = []
    
    for b in flat_batch:
        curr_len = b['hr'].shape[0]
        pad_len = max_len - curr_len
        
        if pad_len > 0:
            hr_padded = torch.nn.functional.pad(b['hr'], (0, pad_len))
            lr_padded = torch.nn.functional.pad(b['lr'], (0, pad_len))
        else:
            hr_padded = b['hr']
            lr_padded = b['lr']
            
        hr_batch.append(hr_padded)
        lr_batch.append(lr_padded)
        
        metadata.append({
            'name': b['name'],
            'path': b['path'],
            'len': curr_len 
        })
        
    return {
        'hr': torch.stack(hr_batch),
        'lr': torch.stack(lr_batch),
        'metadata': metadata
    }

# ==================== GPU å·¥ä½œè¿›ç¨‹ ====================

def gpu_worker(gpu_id, files, output_dir, split_name, stats, mode, return_dict):
    """
    æŒä¹…åŒ–çš„ GPU å·¥ä½œè¿›ç¨‹
    mode: 'stats' (Pass 1) æˆ– 'process' (Pass 2)
    return_dict: ç”¨äºè¿”å›ç»“æœçš„å…±äº«å­—å…¸
    """
    try:
        device = f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸš€ [GPU {gpu_id}] å¯åŠ¨ ({split_name}, æ¨¡å¼: {mode})...")
        
        # 1. åŠ è½½æ¨¡å‹ (åªåšä¸€æ¬¡!)
        print(f"   [GPU {gpu_id}] åŠ è½½ DAC æ¨¡å‹...")
        model_path = dac.utils.download(model_type=DAC_MODEL_TYPE)
        dac_model = dac.DAC.load(str(model_path)).to(device)
        dac_model.eval()
        
        # 2. åˆ›å»º DataLoader
        dataset = AudioLoadingDataset(files, chunk_duration=CHUNK_DURATION)
        # æ³¨æ„: batch_size è¿™é‡ŒæŒ‡â€œæ–‡ä»¶æ•°â€ï¼Œå®é™…äº§ç”Ÿçš„ chunk æ•°å¯èƒ½æ˜¯ batch_size * chunks_per_file
        # ä¸ºäº†æ§åˆ¶æ˜¾å­˜ï¼Œæˆ‘ä»¬ç¨å¾®å‡å° DataLoader çš„ batch_size
        file_batch_size = max(1, DAC_BATCH_SIZE // 4) 
        
        loader = DataLoader(
            dataset, 
            batch_size=file_batch_size, 
            shuffle=False, 
            num_workers=NUM_WORKERS_PER_GPU, 
            collate_fn=chunk_collate,
            pin_memory=True,
            prefetch_factor=2,
            persistent_workers=True
        )
        
        # ç»Ÿè®¡å˜é‡
        local_sum = torch.zeros(1024, dtype=torch.float64)
        local_sq_sum = torch.zeros(1024, dtype=torch.float64)
        local_count = 0
        save_count = 0
        
        # å¦‚æœæ˜¯å¤„ç†æ¨¡å¼ï¼Œå‡†å¤‡å½’ä¸€åŒ–å‚æ•°
        if mode == 'process' and stats is not None:
            hr_mean = torch.tensor(stats['hr_mean']).to(device).view(-1, 1)
            hr_std = torch.tensor(stats['hr_std']).to(device).view(-1, 1)
            lr_mean = torch.tensor(stats['lr_mean']).to(device).view(-1, 1)
            lr_std = torch.tensor(stats['lr_std']).to(device).view(-1, 1)
        
        # 3. ä¸»å¾ªç¯
        desc = f"[GPU {gpu_id}] {split_name}"
        for batch in tqdm(loader, desc=desc, position=gpu_id, leave=False):
            if batch is None: continue
            
            # batch['hr'] åŒ…å«çš„æ˜¯æ‰å¹³åŒ–åçš„ chunk åˆ—è¡¨
            # å¦‚æœ chunk å¤ªå¤šå¯¼è‡´æ˜¾å­˜ä¸å¤Ÿï¼Œæˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œå†æ¬¡åˆ‡åˆ† (micro-batch)
            
            total_chunks = batch['hr'].shape[0]
            
            # ä½¿ç”¨ Micro-batch é˜²æ­¢ OOM
            micro_batch_size = DAC_BATCH_SIZE
            
            for i in range(0, total_chunks, micro_batch_size):
                hr_wav = batch['hr'][i:i+micro_batch_size].unsqueeze(1).to(device)
                lr_wav = batch['lr'][i:i+micro_batch_size].unsqueeze(1).to(device)
                batch_meta = batch['metadata'][i:i+micro_batch_size]
                
                # DAC Encode
                with torch.no_grad():
                    if dac_model.sample_rate != HIGH_SR:
                        hr_wav = AF.resample(hr_wav, HIGH_SR, dac_model.sample_rate)
                        lr_wav = AF.resample(lr_wav, HIGH_SR, dac_model.sample_rate)
                        
                    hr_z = dac_model.encode(hr_wav)[0] 
                    lr_z = dac_model.encode(lr_wav)[0]
                
                # å¤„ç†ç»“æœ
                if mode == 'stats':
                    B, C, T = hr_z.shape
                    flat_z = hr_z.permute(1, 0, 2).reshape(C, -1).double().cpu()
                    local_sum += flat_z.sum(dim=1)
                    local_sq_sum += (flat_z ** 2).sum(dim=1)
                    local_count += flat_z.shape[1]
                    
                elif mode == 'process':
                    hr_z_norm = (hr_z - hr_mean) / hr_std
                    lr_z_norm = (lr_z - lr_mean) / lr_std
                    
                    for j in range(hr_z.shape[0]):
                        meta = batch_meta[j]
                        save_path = os.path.join(output_dir, f"{meta['name']}.pt")
                        
                        torch.save({
                            'hr_latent': hr_z_norm[j].half().cpu(),
                            'lr_latent': lr_z_norm[j].half().cpu(),
                            'metadata': meta
                        }, save_path)
                        save_count += 1

        if mode == 'stats':
            return_dict[gpu_id] = (local_sum, local_sq_sum, local_count)
        else:
            return_dict[gpu_id] = save_count
            
    except Exception as e:
        print(f"âŒ [GPU {gpu_id}] å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return_dict[gpu_id] = None

# ==================== ä¸»æµç¨‹ ====================

def main():
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    
    print("=" * 60)
    print("ğŸš€ JaT æ•°æ®é¢„å¤„ç† V3 (W7900 åŒå¡å…¨é€Ÿç‰ˆ - åˆ†å—é˜²OOMç‰ˆ)")
    print("=" * 60)
    print(f"Chunk Duration: {CHUNK_DURATION}s")
    
    # 1. è·å–æ–‡ä»¶
    files = []
    for d in SOURCE_DIRS:
        if os.path.exists(d):
            files.extend([os.path.join(r, f) for r, _, fs in os.walk(d) for f in fs if f.endswith(('.wav','.flac'))])
    files.sort()
    random.shuffle(files)
    print(f"æ‰¾åˆ° {len(files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
    
    # åˆ’åˆ† GPU ä»»åŠ¡
    num_gpus = torch.cuda.device_count()
    if num_gpus == 0: num_gpus = 1 # Fallback CPU
    
    chunk_size = len(files) // num_gpus
    file_chunks = [files[i:i + chunk_size] for i in range(0, len(files), chunk_size)]
    if len(file_chunks) > num_gpus:
        file_chunks[-2].extend(file_chunks[-1])
        file_chunks.pop()
        
    print(f"ä»»åŠ¡åˆ†é…: {num_gpus} GPUs, æ¯ä¸ªå¤„ç†çº¦ {len(file_chunks[0])} æ–‡ä»¶")
    
    # ================= Pass 1: ç»Ÿè®¡ =================
    print("\nğŸ“Š Pass 1: å…¨å±€ç»Ÿè®¡é‡è®¡ç®—...")
    
    manager = mp.Manager()
    return_dict = manager.dict()
    processes = []

    for i in range(num_gpus):
        p = mp.Process(
            target=gpu_worker, 
            args=(i, file_chunks[i], None, 'Pass 1', None, 'stats', return_dict)
        )
        p.start()
        processes.append(p)
    
    for p in processes:
        p.join()
        
    total_sum = torch.zeros(1024, dtype=torch.float64)
    total_sq_sum = torch.zeros(1024, dtype=torch.float64)
    total_count = 0
    
    if len(return_dict) != num_gpus or any(v is None for v in return_dict.values()):
        print("âŒ Pass 1 å¤±è´¥ï¼Œéƒ¨åˆ†è¿›ç¨‹æœªè¿”å›ç»“æœ")
        return

    for i in range(num_gpus):
        s, sq, c = return_dict[i]
        total_sum += s
        total_sq_sum += sq
        total_count += c
            
    total_sum = total_sum.double()
    total_sq_sum = total_sq_sum.double()
    
    global_mean = total_sum / float(total_count)
    global_var = (total_sq_sum / float(total_count)) - (global_mean ** 2)
    global_var = torch.as_tensor(global_var) 
    global_std = torch.sqrt(torch.clamp(global_var, min=1e-6))
    
    def to_list_safe(tensor_or_scalar):
        if hasattr(tensor_or_scalar, 'tolist'):
            return tensor_or_scalar.tolist()
        elif isinstance(tensor_or_scalar, (float, int)):
            return [float(tensor_or_scalar)]
        elif torch.is_tensor(tensor_or_scalar):
            return tensor_or_scalar.float().tolist()
        return float(tensor_or_scalar)

    stats = {
        'hr_mean': to_list_safe(global_mean),
        'hr_std': to_list_safe(global_std),
        'lr_mean': to_list_safe(global_mean),
        'lr_std': to_list_safe(global_std)
    }
    
    with open('data_processed/global_stats.json', 'w') as f:
        json.dump(stats, f)
    print("âœ… ç»Ÿè®¡é‡å·²ä¿å­˜")
    
    # ================= Pass 2: è½¬æ¢ =================
    print("\nğŸ’¾ Pass 2: ç¼–ç å¹¶ä¿å­˜...")
    
    split_idx = int(len(files) * (1 - VAL_RATIO))
    train_files = files[:split_idx]
    val_files = files[split_idx:]
    
    train_chunks = np.array_split(train_files, num_gpus)
    os.makedirs(TRAIN_DIR, exist_ok=True)
    
    return_dict = manager.dict()
    processes = []

    for i in range(num_gpus):
        p = mp.Process(
            target=gpu_worker,
            args=(i, train_chunks[i], TRAIN_DIR, 'Train Set', stats, 'process', return_dict)
        )
        p.start()
        processes.append(p)
    
    for p in processes:
        p.join()
        
    os.makedirs(VAL_DIR, exist_ok=True)
    val_return_dict = manager.dict()
    p_val = mp.Process(
        target=gpu_worker,
        args=(0, val_files, VAL_DIR, 'Val Set', stats, 'process', val_return_dict)
    )
    p_val.start()
    p_val.join()
    
    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")

if __name__ == '__main__':
    main()