"""
JaT V6 æ€§èƒ½éªŒè¯è„šæœ¬ (åŒé‡ä¼˜åŒ–ç‰ˆ)
ä¿®å¤:
1. CPUç“¶é¢ˆ: å°†é‡é‡‡æ · (Resample) ç§»è‡³ GPU è¿›è¡Œï¼Œæ¶ˆé™¤ 200s+ çš„åŠ è½½å»¶è¿Ÿã€‚
2. GPUç“¶é¢ˆ: Batch Size é™ä¸º 4ï¼Œé…åˆç¯å¢ƒå˜é‡è§£å†³ MIOpen æ˜¾å­˜ç¢ç‰‡åŒ–ã€‚
"""

import os
import torch
import torchaudio
import torchaudio.functional as AF
import numpy as np
from pathlib import Path
import random
import torch.multiprocessing as mp
import time
import math
import traceback
import dac
from audiotools import AudioSignal

# ==================== é…ç½® ====================
SOURCE_DIRS = ['1_source_audio', 'extra_audio']
DEBUG_DIR = "debug_output"

# ğŸš€ ä¼˜åŒ–é…ç½®
HIGH_SR = 48000
LOW_SR = 16000
CHUNK_DURATION = 6.0  
DAC_BATCH_SIZE = 1     # â¬‡ï¸ é™çº§: ä» 8 é™ä¸º 4 (è§£å†³ MIOpen è­¦å‘Šçš„æ ¸å¿ƒ)
DAC_MODEL_TYPE = "44khz"

# ==================== è®¡æ—¶å™¨ ====================
class Timer:
    def __init__(self, name):
        self.name = name
        self.start = 0
        self.duration = 0
    def __enter__(self):
        self.start = time.time()
        return self
    def __exit__(self, *args):
        self.duration = time.time() - self.start

# ==================== Worker ====================
def benchmark_worker(gpu_id, file_path, result_queue):
    log_prefix = f"[GPU {gpu_id}]"
    stats = {}
    
    try:
        print(f"{log_prefix} ğŸš€ å¯åŠ¨ä»»åŠ¡: {Path(file_path).name}")
        
        # 1. åˆå§‹åŒ–
        device = f'cuda:{gpu_id}'
        torch.cuda.set_device(device)
        # é¢„åˆ†é…ä¸€ç‚¹æ˜¾å­˜é˜²æ­¢ç¢ç‰‡
        torch.cuda.reset_peak_memory_stats()
        
        with Timer("Load Model") as t_model:
            model_path = str(dac.utils.download(model_type=DAC_MODEL_TYPE))
            dac_model = dac.DAC.load(model_path).to(device)
            dac_model.eval()
        stats['model_load_time'] = t_model.duration

        # 2. åŠ è½½éŸ³é¢‘ (ğŸš€ ä¼˜åŒ–ç‚¹ A: GPU é‡é‡‡æ ·)
        with Timer("Load & Resample") as t_load:
            # 2.1 å¿«é€Ÿè¯»å– (IO)
            audio, sr = torchaudio.load(file_path)
            
            # 2.2 âš¡ï¸ ç«‹å³ç§»åŠ¨åˆ° GPU
            audio = audio.to(device) 
            
            # 2.3 é¢„å¤„ç† (åœ¨ GPU ä¸Šè¿›è¡Œï¼Œé€Ÿåº¦æå¿«)
            if audio.shape[0] > 1: 
                audio = audio.mean(dim=0, keepdim=True)
            peak = audio.abs().max()
            if peak > 1.0: 
                audio = audio / peak
            
            # 2.4 é‡é‡‡æ · (åœ¨ GPU ä¸Šè¿›è¡Œ)
            if sr != HIGH_SR:
                audio_hr = AF.resample(audio, orig_freq=sr, new_freq=HIGH_SR)
            else:
                audio_hr = audio
                
        stats['audio_load_time'] = t_load.duration
        total_samples = audio_hr.shape[1]
        duration_sec = total_samples / HIGH_SR
        print(f"{log_prefix} åŠ è½½&é¢„å¤„ç†å®Œæˆ: {t_load.duration:.2f}s (éŸ³é¢‘é•¿åº¦: {duration_sec:.2f}s)")

        # 3. åˆ‡åˆ†
        chunk_samples = int(CHUNK_DURATION * HIGH_SR)
        chunks_hr = []
        num_chunks = math.ceil(total_samples / chunk_samples)
        pad_len = (num_chunks * chunk_samples) - total_samples
        
        # GPU ä¸Šç›´æ¥ Pad å’Œåˆ‡ç‰‡
        audio_hr_padded = torch.nn.functional.pad(audio_hr, (0, pad_len))
        for i in range(num_chunks):
            start = i * chunk_samples
            chunks_hr.append(audio_hr_padded[:, start : start + chunk_samples])
            
        print(f"{log_prefix} åˆ‡åˆ†å®Œæˆ: {num_chunks} chunks")
        # æ¸…ç©ºæ˜¾å­˜
        torch.cuda.empty_cache()
        # 4. ç¼–ç 
        latents_hr_list = []
        hop_length = None
        t_encode_start = time.time()
        
        # ğŸš€ ä¼˜åŒ–ç‚¹ B: Batch Size = 4
        for i in range(0, len(chunks_hr), DAC_BATCH_SIZE):
            batch_list = chunks_hr[i : i + DAC_BATCH_SIZE]
            batch_tensor = torch.stack(batch_list) # å·²ç»åœ¨ GPU ä¸Šäº†
            
            # æ¨¡æ‹Ÿ LR å¤„ç† (GPU)
            batch_lr = AF.resample(AF.resample(batch_tensor, HIGH_SR, LOW_SR), LOW_SR, HIGH_SR)
            if batch_lr.shape[-1] != batch_tensor.shape[-1]:
                 batch_lr = torch.nn.functional.pad(batch_lr, (0, batch_tensor.shape[-1] - batch_lr.shape[-1]))

            def encode_helper(wav):
                # wav: [B, 1, T] on GPU
                signal = AudioSignal(wav, sample_rate=HIGH_SR)
                if dac_model.sample_rate != HIGH_SR:
                    signal = signal.resample(dac_model.sample_rate)
                with torch.no_grad():
                    z, _, _, _, _ = dac_model.encode(signal.audio_data)
                return z

            z_hr = encode_helper(batch_tensor)
            z_lr = encode_helper(batch_lr) # åŠ ä¸Š LR ç¼–ç ä»¥æ¨¡æ‹ŸçœŸå®è´Ÿè½½

            if hop_length is None:
                hop_length = batch_tensor.shape[-1] / z_hr.shape[-1]

            for b in range(z_hr.shape[0]):
                latents_hr_list.append(z_hr[b].cpu()) # ç§»å› CPU æš‚å­˜

        stats['encoding_time'] = time.time() - t_encode_start
        print(f"{log_prefix} ç¼–ç å®Œæˆ: {stats['encoding_time']:.3f}s")

        # 5. æ‹¼æ¥
        full_hr = torch.cat(latents_hr_list, dim=-1)
        valid_frames = int(total_samples / hop_length)
        full_hr = full_hr[..., :valid_frames]
        
        stats['status'] = 'success'
        stats['peak_memory'] = torch.cuda.max_memory_allocated(device) / 1024**3
        stats['realtime_factor'] = duration_sec / stats['encoding_time']
        
        result_queue.put((gpu_id, stats))

    except Exception as e:
        print(f"{log_prefix} âŒ Error: {e}")
        traceback.print_exc()
        result_queue.put((gpu_id, {'status': 'error', 'msg': str(e)}))

# ==================== Main ====================
def main():
    try: mp.set_start_method('spawn', force=True)
    except: pass
        
    print("=" * 60)
    print("ğŸš€ JaT V6 Performance Test (Optimized)")
    print(f"âš™ï¸  Config: GPU Resample | Batch={DAC_BATCH_SIZE}")
    print("=" * 60)
    
    os.makedirs(DEBUG_DIR, exist_ok=True)
    all_files = []
    for d in SOURCE_DIRS:
        if os.path.exists(d):
            all_files.extend([str(p) for p in Path(d).rglob('*') if p.suffix.lower() in ['.wav', '.flac', '.mp3']])
    
    if len(all_files) < 2: return
    random.shuffle(all_files)
    test_files = all_files[:2]
    
    ctx = mp.get_context('spawn')
    queue = ctx.Queue()
    procs = []
    
    for i in range(2):
        p = ctx.Process(target=benchmark_worker, args=(i, test_files[i], queue))
        p.start()
        procs.append(p)
        
    for p in procs: p.join()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š")
    print("=" * 60)
    while not queue.empty():
        gpu_id, s = queue.get()
        if s['status'] == 'success':
            print(f"GPU {gpu_id}: âœ…")
            print(f"  - åŠ è½½æ—¶é—´:   {s['audio_load_time']:.2f}s (åŸ: 225s -> ç°: <2s)")
            print(f"  - ç¼–ç æ—¶é—´:   {s['encoding_time']:.2f}s (åŸ: 287s -> ç°: ~30s)")
            print(f"  - å®æ—¶å€ç‡:   {s['realtime_factor']:.2f}x Realtime")
            print(f"  - æ˜¾å­˜å³°å€¼:   {s['peak_memory']:.2f} GB")

if __name__ == '__main__':
    main()