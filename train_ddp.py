import os
import argparse
import json
import random
import math
import time
from pathlib import Path
from datetime import timedelta, datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import AdamW
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

# å¼•å…¥ V2 æ¨¡å‹
# ç¡®ä¿æ‚¨çš„ src ç›®å½•åœ¨ python path ä¸­
from src.models.jat_audiosr_v2 import JaT_AudioSR_V2

# ==============================================================================
# âš™ï¸ é…ç½®ä¸­å¿ƒ (Configuration)
# ==============================================================================

class TrainConfig:
    # --- ç¡¬ä»¶ä¸å¹¶è¡Œ ---
    seed = 42
    num_workers = 8         # é™ä½workeræ•°é‡é¿å…å†…å­˜çˆ†ç‚¸ï¼ˆ90MBÃ—16workers=å·¨å¤§ï¼‰
    
    # --- æ•°æ® ---
    data_dir = "data_processed_v13_final"  # æŒ‡å‘ prepare_dataset_v5 ç”Ÿæˆçš„ç›®å½•
    stats_file = "global_stats_separated.json"  # åˆ†ç¦»çš„ HR/LR ç»Ÿè®¡é‡
    target_duration = 16.0                 # åˆ‡ç‰‡é•¿åº¦ (ç§’)
    dac_sample_rate = 44100
    dac_hop_length = 512    # DAC é»˜è®¤å‹ç¼©ç‡
    
    # è®¡ç®— Latent å¸§æ•°: 16 * 44100 / 512 â‰ˆ 1378
    target_frames = int(target_duration * dac_sample_rate / dac_hop_length)

    # --- æ¨¡å‹ (V2 Specs) ---
    model_params = {
        'input_channels': 1024,
        'cond_channels': 1024,
        'patch_len': 4,
        'hidden_size': 1024,
        'depth': 16,
        'num_q_heads': 16,
        'num_kv_heads': 4,
        'bottleneck_dim': 512,
        'mlp_ratio': 4.0,
        'dropout': 0.1  # ğŸ”¥ ä¿®å¤è¿‡æ‹Ÿåˆï¼šå¼€å¯Dropout
    }

    # --- è®­ç»ƒè¶…å‚ ---
    batch_size = 72         # å•å¡ Batch (æ€» Batch = 48)
    lr = 5e-5               # ç¨³å¥å­¦ä¹ ç‡
    weight_decay = 0.1
    warmup_steps = 1000     # é¢„çƒ­æ­¥æ•°
    num_epochs = 1000       # è·‘æ»¡ä¸ºæ­¢
    grad_clip = 1.0
    
    # --- ä¿å­˜ä¸æ—¥å¿— ---
    save_dir_base = "checkpoints/v2_full_run"  # åŸºç¡€ç›®å½•
    log_dir_base = "runs/v2_full_run"          # åŸºç¡€ç›®å½•
    save_interval_steps = 1000  # æ¯ 1000 æ­¥ä¿å­˜ä¸€ä¸ª interval checkpoint

# ==============================================================================
# ğŸ› ï¸ å·¥å…·å‡½æ•°
# ==============================================================================

def get_timestamp_folder():
    """ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å¤¹åï¼ˆæ ¼å¼ï¼šMMDDHHMMï¼‰"""
    now = datetime.now()
    return now.strftime("%m%d%H%M")

def find_latest_checkpoint_dir(base_dir):
    """
    åœ¨base_dirä¸‹æŸ¥æ‰¾æœ€æ–°çš„æ—¶é—´æˆ³å­æ–‡ä»¶å¤¹
    è¿”å›ï¼š(æœ€æ–°æ–‡ä»¶å¤¹è·¯å¾„, checkpointè·¯å¾„) æˆ– (None, None)
    """
    if not os.path.exists(base_dir):
        return None, None

    # æŸ¥æ‰¾æ‰€æœ‰æ—¶é—´æˆ³æ ¼å¼çš„å­æ–‡ä»¶å¤¹ï¼ˆ8ä½æ•°å­—ï¼‰
    subdirs = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path) and item.isdigit() and len(item) == 8:
            subdirs.append((item, item_path))

    if not subdirs:
        return None, None

    # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œå–æœ€æ–°çš„
    subdirs.sort(reverse=True)
    latest_folder = subdirs[0][1]

    # æ£€æŸ¥æ˜¯å¦æœ‰last.pt
    checkpoint_path = os.path.join(latest_folder, "last.pt")
    if os.path.exists(checkpoint_path):
        return latest_folder, checkpoint_path

    return latest_folder, None

def setup_ddp():
    """åˆå§‹åŒ– DDP ç¯å¢ƒ"""
    if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        local_rank = int(os.environ["LOCAL_RANK"])
    else:
        # é»˜è®¤å•æœºå¤šå¡
        rank = 0
        world_size = torch.cuda.device_count()
        local_rank = 0
        os.environ["MASTER_ADDR"] = "localhost"
        os.environ["MASTER_PORT"] = "12355"
        os.environ["RANK"] = "0"
        os.environ["WORLD_SIZE"] = str(world_size)

    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend="nccl", init_method="env://", world_size=world_size, rank=rank)
    return rank, local_rank, world_size

def cleanup_ddp():
    dist.destroy_process_group()

def u_shaped_timestep_sampling(batch_size, device, alpha=0.5):
    """V2 æ ¸å¿ƒ: U-shaped Timestep Sampling"""
    u = torch.rand(batch_size, device=device)
    t = torch.where(
        u < 0.5,
        (2 * u) ** alpha / 2,
        1 - ((2 * (1 - u)) ** alpha) / 2
    )
    return t

def load_global_stats(stats_path, device):
    """åŠ è½½å¹¶å¹¿æ’­å…¨å±€ç»Ÿè®¡é‡"""
    with open(stats_path, 'r') as f:
        data = json.load(f)
    
    # è½¬æ¢ä¸º Tensor (1, C, 1) ç”¨äºå¹¿æ’­
    hr_mean = torch.tensor(data['hr_mean']).view(1, -1, 1).to(device)
    hr_std = torch.tensor(data['hr_std']).view(1, -1, 1).to(device)
    lr_mean = torch.tensor(data['lr_mean']).view(1, -1, 1).to(device)
    lr_std = torch.tensor(data['lr_std']).view(1, -1, 1).to(device)
    
    return hr_mean, hr_std, lr_mean, lr_std

# ==============================================================================
# ğŸ’¾ æ•°æ®é›† (Dataset)
# ==============================================================================

class LatentDataset(Dataset):
    """
    è®­ç»ƒæ•°æ®é›†ï¼šä½¿ç”¨LRU cacheç¼“å­˜mmapæ–‡ä»¶å¼•ç”¨
    ç”±äºå·²ä¿®å¤Float Bombï¼Œç°åœ¨æ¯ä¸ªæ ·æœ¬åªæœ‰~11MBï¼Œç¼“å­˜å®Œå…¨å¯è¡Œ
    """
    def __init__(self, data_dir, split, target_frames, samples_per_epoch_multiplier=6, cache_size=128):
        super().__init__()
        self.target_frames = target_frames
        self.multiplier = samples_per_epoch_multiplier

        # æ‰«ææ‰€æœ‰ .pt æ–‡ä»¶
        search_path = Path(data_dir) / split
        self.files = sorted(list(search_path.glob("*.pt")))

        if len(self.files) == 0:
            raise ValueError(f"No .pt files found in {search_path}")

        print(f"[{split}] Found {len(self.files)} files (x{self.multiplier} samples/epoch = {len(self.files) * self.multiplier} total samples)")
        print(f"[{split}] Using LRU cache (size={cache_size}, ~{cache_size * 11 / 1024:.1f}GB per worker)")

        # LRUç¼“å­˜mmapçš„æ–‡ä»¶å¼•ç”¨ï¼ˆä¸ç¼“å­˜è½¬æ¢åçš„æ•°æ®ï¼‰
        from functools import lru_cache
        @lru_cache(maxsize=cache_size)
        def load_file_mmap(path_str):
            """ç¼“å­˜mmapåŠ è½½çš„æ–‡ä»¶ï¼ˆå†…å­˜å ç”¨æå°ï¼Œåªæ˜¯å¼•ç”¨ï¼‰"""
            data = torch.load(path_str, map_location='cpu', mmap=True, weights_only=False)
            return data['hr_latent'], data['lr_latent']  # ä¿æŒFP16

        self._load_file = load_file_mmap

    def __len__(self):
        return len(self.files) * self.multiplier

    def __getitem__(self, idx):
        # æ˜ å°„åˆ°å®é™…æ–‡ä»¶ç´¢å¼•
        file_idx = idx % len(self.files)
        path = self.files[file_idx]

        # ä»cacheåŠ è½½mmapå¼•ç”¨ï¼ˆFP16ï¼Œå‡ ä¹ä¸å å†…å­˜ï¼‰
        hr, lr = self._load_file(str(path))

        length = hr.shape[-1]

        # ğŸ”¥ å…³é”®ä¿®å¤2: å…ˆåˆ‡ç‰‡ï¼ˆFP16ï¼‰ï¼Œå†è½¬æ¢ï¼ˆFP32ï¼‰
        if length < self.target_frames:
            # A. çŸ­éŸ³é¢‘: å¾ªç¯æ‹¼æ¥
            repeats = math.ceil(self.target_frames / length)
            hr = hr.repeat(1, repeats)[..., :self.target_frames]
            lr = lr.repeat(1, repeats)[..., :self.target_frames]
        else:
            # B. é•¿éŸ³é¢‘: éšæœºåˆ‡ç‰‡
            start = random.randint(0, length - self.target_frames)
            hr = hr[..., start : start + self.target_frames]
            lr = lr[..., start : start + self.target_frames]

        # æœ€åæ‰è½¬FP32ï¼ˆåªè½¬æ¢16sç‰‡æ®µï¼Œå†…å­˜å ç”¨å°ï¼‰
        hr = hr.float()
        lr = lr.float()

        return hr, lr

class ValidationDataset(Dataset):
    """éªŒè¯é›†ä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·ï¼Œä¿è¯å¯é‡å¤æ€§ï¼Œå¸¦LRU cache"""
    def __init__(self, data_dir, split, target_frames, samples_per_epoch_multiplier=6, cache_size=128):
        super().__init__()
        self.target_frames = target_frames
        self.multiplier = samples_per_epoch_multiplier
        search_path = Path(data_dir) / split
        self.files = sorted(list(search_path.glob("*.pt")))

        print(f"[{split}] Found {len(self.files)} files (x{self.multiplier} samples/epoch = {len(self.files) * self.multiplier} total samples)")
        print(f"[{split}] Using LRU cache (size={cache_size}, ~{cache_size * 11 / 1024:.1f}GB per worker)")

        # LRUç¼“å­˜mmapå¼•ç”¨
        from functools import lru_cache
        @lru_cache(maxsize=cache_size)
        def load_file_mmap(path_str):
            data = torch.load(path_str, map_location='cpu', mmap=True, weights_only=False)
            return data['hr_latent'], data['lr_latent']

        self._load_file = load_file_mmap

    def __len__(self):
        return len(self.files) * self.multiplier

    def __getitem__(self, idx):
        # æ˜ å°„åˆ°å®é™…æ–‡ä»¶å’Œé‡‡æ ·ä½ç½®
        file_idx = idx % len(self.files)
        sample_idx = idx // len(self.files)  # 0, 1, 2, ..., multiplier-1
        path = self.files[file_idx]

        # ä»cacheåŠ è½½mmapå¼•ç”¨
        hr, lr = self._load_file(str(path))

        length = hr.shape[-1]

        # ğŸ”¥ å…³é”®ä¿®å¤2: å…ˆåˆ‡ç‰‡ï¼ˆFP16ï¼‰ï¼Œå†è½¬æ¢ï¼ˆFP32ï¼‰
        if length < self.target_frames:
            # çŸ­éŸ³é¢‘: å¾ªç¯æ‹¼æ¥
            repeats = math.ceil(self.target_frames / length)
            hr = hr.repeat(1, repeats)[..., :self.target_frames]
            lr = lr.repeat(1, repeats)[..., :self.target_frames]
        else:
            # é•¿éŸ³é¢‘: ç¡®å®šæ€§é‡‡æ ·ï¼ˆæ ¹æ®sample_idxå‡åŒ€åˆ†å¸ƒï¼‰
            # å°†éŸ³é¢‘åˆ†æˆ multiplier ä¸ªåŒºé—´ï¼Œæ¯æ¬¡ä»å¯¹åº”åŒºé—´é‡‡æ ·
            if self.multiplier == 1:
                # åªé‡‡æ ·1æ¬¡ï¼šcenter crop
                start = (length - self.target_frames) // 2
            else:
                # å¤šæ¬¡é‡‡æ ·ï¼šå‡åŒ€åˆ†å¸ƒ
                segment_length = max(length - self.target_frames, 1)
                start = int(segment_length * sample_idx / (self.multiplier - 1))
                start = min(start, length - self.target_frames)

            hr = hr[..., start : start + self.target_frames]
            lr = lr[..., start : start + self.target_frames]

        # æœ€åæ‰è½¬FP32ï¼ˆåªè½¬æ¢16sç‰‡æ®µï¼‰
        hr = hr.float()
        lr = lr.float()

        return hr, lr

# ==============================================================================
# ğŸš€ è®­ç»ƒä¸»æµç¨‹
# ==============================================================================

def main():
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='JaT V2 Training with DDP')
    parser.add_argument('--resume', type=str, nargs='?', const='auto', default=None,
                        help='Resume training. Use --resume for auto (latest), or --resume <path> for specific checkpoint')
    args = parser.parse_args()

    # 2. DDP åˆå§‹åŒ–
    rank, local_rank, world_size = setup_ddp()
    is_master = (rank == 0)

    cfg = TrainConfig()

    # 3. æ™ºèƒ½æ—¶é—´æˆ³ç›®å½•è®¾ç½®
    resume_path = None
    timestamp_folder = None
    save_dir = ""  # åˆå§‹åŒ–ï¼Œé¿å…ç±»å‹æ£€æŸ¥è­¦å‘Š
    log_dir = ""   # åˆå§‹åŒ–ï¼Œé¿å…ç±»å‹æ£€æŸ¥è­¦å‘Š

    if args.resume:
        if args.resume == 'auto':
            # è‡ªåŠ¨ä»æœ€æ–°æ—¶é—´æˆ³æ–‡ä»¶å¤¹æ¢å¤
            latest_dir, latest_ckpt = find_latest_checkpoint_dir(cfg.save_dir_base)
            if latest_ckpt:
                resume_path = latest_ckpt
                save_dir = latest_dir
                log_dir = latest_dir.replace("checkpoints", "runs")
                if is_master:
                    timestamp_folder = os.path.basename(latest_dir)
                    print(f"ğŸ”„ Auto resume from latest: {timestamp_folder}")
                    print(f"   Checkpoint: {resume_path}")
            else:
                if is_master:
                    print("âš ï¸  No checkpoint found, starting new training")
                timestamp_folder = get_timestamp_folder()
                save_dir = os.path.join(cfg.save_dir_base, timestamp_folder)
                log_dir = os.path.join(cfg.log_dir_base, timestamp_folder)
                if is_master:
                    print(f"ğŸ“ New training session: {timestamp_folder}")
        else:
            # æ‰‹åŠ¨æŒ‡å®šcheckpointè·¯å¾„
            resume_path = args.resume
            if not os.path.exists(resume_path):
                raise FileNotFoundError(f"Checkpoint not found: {resume_path}")
            # ä»checkpointè·¯å¾„æå–ç›®å½•
            save_dir = str(Path(resume_path).parent)
            log_dir = save_dir.replace("checkpoints", "runs")
            if is_master:
                timestamp_folder = os.path.basename(save_dir)
                print(f"ğŸ“‚ Manual resume from: {timestamp_folder}")
                print(f"   Checkpoint: {resume_path}")
    else:
        # æ–°è®­ç»ƒï¼Œåˆ›å»ºæ—¶é—´æˆ³æ–‡ä»¶å¤¹
        timestamp_folder = get_timestamp_folder()
        save_dir = os.path.join(cfg.save_dir_base, timestamp_folder)
        log_dir = os.path.join(cfg.log_dir_base, timestamp_folder)
        if is_master:
            print(f"ğŸ“ New training session: {timestamp_folder}")

    # åˆ›å»ºç›®å½•
    if is_master:
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)
        writer = SummaryWriter(log_dir=log_dir)
        print("="*60)
        print(f"ğŸš€ JaT V2 Training Started")
        print(f"   Save Dir: {save_dir}")
        print(f"   Log Dir: {log_dir}")
        print(f"   GPUs: {world_size} | Batch/GPU: {cfg.batch_size} | Total Batch: {cfg.batch_size * world_size}")
        print(f"   Target Duration: {cfg.target_duration}s ({cfg.target_frames} frames)")
        print(f"   Resume Mode: {'ON' if resume_path else 'OFF'}")
        print("="*60)
    
    # 3. å‡†å¤‡æ•°æ®
    train_dataset = LatentDataset(cfg.data_dir, "train", cfg.target_frames)
    val_dataset = ValidationDataset(cfg.data_dir, "val", cfg.target_frames)
    
    train_sampler = DistributedSampler(train_dataset, shuffle=True)
    val_sampler = DistributedSampler(val_dataset, shuffle=False) # éªŒè¯é›†ä¸éœ€è¦ shuffle
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=cfg.batch_size,
        sampler=train_sampler,
        num_workers=cfg.num_workers,
        pin_memory=True, # åŠ é€Ÿ Host -> Device
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=cfg.batch_size,
        sampler=val_sampler,
        num_workers=cfg.num_workers,
        pin_memory=True,
        drop_last=True
    )
    
    # 4. åŠ è½½ç»Ÿè®¡é‡ (æ¯ä¸ªè¿›ç¨‹éƒ½åŠ è½½)
    stats_path = os.path.join(cfg.data_dir, cfg.stats_file)
    hr_mean, hr_std, lr_mean, lr_std = load_global_stats(stats_path, f"cuda:{local_rank}")
    
    # 5. æ¨¡å‹åˆå§‹åŒ–
    model = JaT_AudioSR_V2(**cfg.model_params).to(local_rank)
    
    # 6. ä¼˜åŒ–å™¨ & è°ƒåº¦å™¨
    optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)
    
    # æ‰‹åŠ¨å®ç° Cosine with Warmupï¼Œæ–¹ä¾¿çŠ¶æ€ä¿å­˜
    def get_lr(step, total_steps, warmup_steps, base_lr):
        if step < warmup_steps:
            return base_lr * (step / max(1, warmup_steps))
        else:
            progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)
            return base_lr * 0.5 * (1.0 + math.cos(math.pi * progress))

    # 7. æ··åˆç²¾åº¦ Scaler
    scaler = torch.amp.GradScaler('cuda')
    
    # 8. æ¢å¤è®­ç»ƒé€»è¾‘ (Perfect Resume)
    start_epoch = 0
    global_step = 0
    best_val_loss = float('inf')

    if resume_path and os.path.exists(resume_path):
        if is_master:
            print(f"â™»ï¸  Resuming from {resume_path}...")

        # map_location ç¡®ä¿åŠ è½½åˆ°å½“å‰ GPU
        # weights_only=False: PyTorch 2.6+éœ€è¦ï¼Œå› ä¸ºcheckpointåŒ…å«RNGçŠ¶æ€ç­‰
        checkpoint = torch.load(resume_path, map_location=f"cuda:{local_rank}", weights_only=False)

        # å¤„ç† torch.compile() çš„ _orig_mod. å‰ç¼€ï¼ˆå…¼å®¹å·²ç¼–è¯‘çš„æ£€æŸ¥ç‚¹ï¼‰
        state_dict = checkpoint['model_state_dict']
        if any(k.startswith('_orig_mod.') for k in state_dict.keys()):
            state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}
            if is_master:
                print("ğŸ”§ Removed '_orig_mod.' prefix from compiled checkpoint")

        model.load_state_dict(state_dict)
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        start_epoch = checkpoint['epoch'] + 1
        global_step = checkpoint['global_step']
        best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        
        # æ¢å¤ RNG çŠ¶æ€ (Python, Numpy, PyTorch, CUDA)
        if 'rng_state' in checkpoint:
            try:
                random.setstate(checkpoint['rng_state']['python'])
                np.random.set_state(checkpoint['rng_state']['numpy'])

                # PyTorch RNG çŠ¶æ€éœ€è¦æ˜¯ ByteTensor (dtype=torch.uint8)
                torch_rng = checkpoint['rng_state']['torch']
                if torch_rng.dtype != torch.uint8:
                    torch_rng = torch_rng.to(torch.uint8)
                torch.set_rng_state(torch_rng.cpu())

                # CUDA RNG çŠ¶æ€æ¢å¤ï¼ˆå…¼å®¹æ–°æ—§æ ¼å¼ï¼‰
                if 'cuda_all' in checkpoint['rng_state']:
                    # æ–°æ ¼å¼ï¼šå¤šGPUçŠ¶æ€
                    cuda_states = checkpoint['rng_state']['cuda_all']
                    # ç¡®ä¿æ¯ä¸ªçŠ¶æ€éƒ½æ˜¯æ­£ç¡®ç±»å‹
                    cuda_states = [s.to(torch.uint8).cpu() if s.dtype != torch.uint8 else s.cpu()
                                   for s in cuda_states]
                    torch.cuda.set_rng_state_all(cuda_states)
                elif 'cuda' in checkpoint['rng_state']:
                    # æ—§æ ¼å¼ï¼šå•GPUçŠ¶æ€ï¼ˆå‘åå…¼å®¹ï¼‰
                    cuda_state = checkpoint['rng_state']['cuda']
                    if cuda_state.dtype != torch.uint8:
                        cuda_state = cuda_state.to(torch.uint8)
                    torch.cuda.set_rng_state(cuda_state.cpu())

                if is_master:
                    print("ğŸ² RNG states restored")
            except Exception as e:
                if is_master:
                    print(f"âš ï¸  Warning: Could not restore RNG states: {e}")
                    print("   Training will continue with random initialization")
            
        if is_master:
            print(f"âœ… Resumed at Epoch {start_epoch}, Step {global_step}")

    # 9. torch.compile ä¼˜åŒ–ï¼ˆä¿å®ˆæ¨¡å¼ï¼‰
    if is_master:
        print("ğŸ”¥ Compiling model with torch.compile (mode='default')...")

    model = torch.compile(model, mode='default', backend='inductor')

    if is_master:
        print("âœ… Model compiled successfully")

    # 10. åŒ…è£… DDP
    model = DDP(model, device_ids=[local_rank], find_unused_parameters=False) # JaTæ— æœªä½¿ç”¨å‚æ•°ï¼ŒFalseæ›´å¿«

    # ==========================================================================
    # ğŸ‹ï¸ è®­ç»ƒå¾ªç¯
    # ==========================================================================
    
    total_steps_approx = len(train_loader) * cfg.num_epochs
    
    for epoch in range(start_epoch, cfg.num_epochs):
        model.train()
        train_sampler.set_epoch(epoch) # DDP å¿…é¡»
        
        start_time = time.time()
        epoch_loss = 0.0

        # åˆ›å»º tqdm è¿›åº¦æ¡ï¼ˆåªåœ¨ GPU0 æ˜¾ç¤ºï¼‰
        pbar = tqdm(total=len(train_loader), disable=(not is_master),
                    desc=f"Epoch {epoch}/{cfg.num_epochs}",
                    leave=False, dynamic_ncols=True)

        for hr, lr in train_loader:
            # LR Scheduler Step
            current_lr = get_lr(global_step, total_steps_approx, cfg.warmup_steps, cfg.lr)
            for param_group in optimizer.param_groups:
                param_group['lr'] = current_lr
                
            # æ•°æ®æ¬è¿
            hr = hr.to(local_rank, non_blocking=True)
            lr = lr.to(local_rank, non_blocking=True)
            
            optimizer.zero_grad(set_to_none=True)

            with torch.amp.autocast('cuda'):
                # A. å½’ä¸€åŒ– (Normalize)
                hr_norm = (hr - hr_mean) / hr_std
                lr_norm = (lr - lr_mean) / lr_std

                # B. é‡‡æ · Timesteps (U-shaped)
                B = hr.shape[0]
                t = u_shaped_timestep_sampling(B, hr.device) # [B]

                # C. åŠ å™ª (Flow Matching: z_t = t*x + (1-t)*noise)
                noise = torch.randn_like(hr_norm)
                t_view = t.view(-1, 1, 1)
                z_t = t_view * hr_norm + (1 - t_view) * noise

                # D. æ¨¡å‹é¢„æµ‹ (Pred x0)
                pred_x0 = model(z_t, t, lr_norm)

                # E. Loss (MSE)
                loss = F.mse_loss(pred_x0, hr_norm)

                # ğŸ“Š è®¡ç®—è®­ç»ƒæŒ‡æ ‡ (Phase 1)
                with torch.no_grad():
                    # é¢„æµ‹è´¨é‡ç»Ÿè®¡
                    pred_mean = pred_x0.mean().item()
                    pred_std = pred_x0.std().item()

                    # ä¿¡å™ªæ¯” (SNR in dB)
                    signal_power = (hr_norm ** 2).mean()
                    noise_power = ((pred_x0 - hr_norm) ** 2).mean()
                    snr_db = 10 * torch.log10(signal_power / (noise_power + 1e-8))
                    snr_db = snr_db.item()
            
            # åå‘ä¼ æ’­ & ä¼˜åŒ–
            scaler.scale(loss).backward()

            # ğŸ“Š è®¡ç®—æ¢¯åº¦èŒƒæ•° (Phase 1)
            scaler.unscale_(optimizer)
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)
            grad_norm = grad_norm.item()
            
            scaler.step(optimizer)
            scaler.update()
            
            # è®°å½•
            loss_val = loss.item()
            epoch_loss += loss_val

            if is_master:
                # æ›´æ–° tqdm è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f'{loss_val:.5f}',
                    'lr': f'{current_lr:.2e}',
                    'step': global_step
                })
                pbar.update(1)

                # TensorBoard æ—¥å¿— (æ¯10æ­¥ä¸€æ¬¡)
                if global_step % 10 == 0:
                    writer.add_scalar("Train/Loss", loss_val, global_step)
                    writer.add_scalar("Train/LR", current_lr, global_step)
                    # Phase 1 æŒ‡æ ‡
                    writer.add_scalar("Train/GradNorm", grad_norm, global_step)
                    writer.add_scalar("Train/SNR_dB", snr_db, global_step)
                    writer.add_scalar("Train/PredictionMean", pred_mean, global_step)
                    writer.add_scalar("Train/PredictionStd", pred_std, global_step)

                # ä¿å­˜ Interval Checkpoint
                if global_step > 0 and global_step % cfg.save_interval_steps == 0:
                    save_path = os.path.join(save_dir, f"interval_step_{global_step}.pt")
                    save_checkpoint(
                        model, optimizer, scaler, epoch, global_step, best_val_loss, save_path
                    )

            global_step += 1

        # End of Epoch
        if is_master:
            pbar.close()  # å…³é—­è¿›åº¦æ¡
            avg_epoch_loss = epoch_loss / len(train_loader)
            time_elapsed = str(timedelta(seconds=int(time.time() - start_time)))
            print(f"\nâœ… Epoch {epoch} Done in {time_elapsed} | Avg Loss: {avg_epoch_loss:.5f}")
            
            # ä¿å­˜ Last Checkpoint (æ¯ä¸ª Epoch æ›´æ–°)
            save_path = os.path.join(save_dir, "last.pt")
            save_checkpoint(
                model, optimizer, scaler, epoch, global_step, best_val_loss, save_path
            )

        # ======================================================================
        # ğŸ§ª éªŒè¯å¾ªç¯ (æ¯ä¸ª Epoch ä¸€æ¬¡)
        # ======================================================================
        if is_master: print(f"ğŸ§ª Validating Epoch {epoch}...")

        val_loss, val_loss_std = validate(model, val_loader, hr_mean, hr_std, lr_mean, lr_std, local_rank)

        if is_master:
            writer.add_scalar("Val/Loss", val_loss, epoch)
            writer.add_scalar("Val/Loss_Std", val_loss_std, epoch)  # Phase 1 æŒ‡æ ‡
            print(f"ğŸ“‰ Val Loss: {val_loss:.5f} Â± {val_loss_std:.5f} (Best: {best_val_loss:.5f})")

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                save_path = os.path.join(save_dir, "best.pt")
                # Beståªå­˜çŠ¶æ€ï¼Œå¦‚æœç¡¬ç›˜å¤Ÿï¼Œä¹Ÿå¯ä»¥å­˜å®Œæ•´
                save_checkpoint(
                    model, optimizer, scaler, epoch, global_step, best_val_loss, save_path
                )
                print(f"ğŸ† New Best Model Saved!")
                
        dist.barrier() # åŒæ­¥ç­‰å¾…ä¸»è¿›ç¨‹ä¿å­˜å®Œæ¯•

    cleanup_ddp()

# ==============================================================================
# ğŸ§© è¾…åŠ©åŠŸèƒ½
# ==============================================================================

def validate(model, loader, hr_mean, hr_std, lr_mean, lr_std, device):
    """
    DDP éªŒè¯ï¼šè®¡ç®—æ‰€æœ‰å¡çš„å¹³å‡ Loss å’Œæ ‡å‡†å·®
    è¿”å›: (avg_loss, loss_std)
    """
    model.eval()
    total_loss = torch.zeros(1).to(device)
    total_steps = torch.zeros(1).to(device)
    # æ”¶é›†æ¯ä¸ªbatchçš„lossç”¨äºè®¡ç®—æ ‡å‡†å·®
    all_losses = []

    with torch.no_grad():
        for hr, lr in loader:
            hr = hr.to(device, non_blocking=True)
            lr = lr.to(device, non_blocking=True)

            # Normalize
            hr_norm = (hr - hr_mean) / hr_std
            lr_norm = (lr - lr_mean) / lr_std

            # Fixed Timestep sampling for deterministic validation (Optional)
            # ä¹Ÿå¯ä»¥ç»§ç»­éšæœºï¼Œå› ä¸ºæ•°æ®é‡å¤Ÿå¤§
            t = torch.rand(hr.shape[0], device=device)

            noise = torch.randn_like(hr_norm)
            t_view = t.view(-1, 1, 1)
            z_t = t_view * hr_norm + (1 - t_view) * noise

            # éªŒè¯é›†ä¸€å®šè¦ç”¨ autocast ä¿æŒä¸€è‡´æ€§
            with torch.amp.autocast('cuda'):
                pred_x0 = model(z_t, t, lr_norm)
                loss = F.mse_loss(pred_x0, hr_norm)

            total_loss += loss
            total_steps += 1
            all_losses.append(loss.item())

    # DDP All-Reduce: æ±‡æ€»æ‰€æœ‰å¡çš„ Loss
    dist.all_reduce(total_loss, op=dist.ReduceOp.SUM)
    dist.all_reduce(total_steps, op=dist.ReduceOp.SUM)

    avg_loss = total_loss / total_steps

    # è®¡ç®—æ ‡å‡†å·® (åªåœ¨å½“å‰GPUä¸Šè®¡ç®—ï¼Œå› ä¸ºDDPä¼šå¯¼è‡´ä¸åŒGPUçœ‹åˆ°ä¸åŒæ•°æ®)
    if len(all_losses) > 1:
        loss_std = torch.tensor(all_losses).std().item()
    else:
        loss_std = 0.0

    model.train()
    return avg_loss.item(), loss_std

def save_checkpoint(model, optimizer, scaler, epoch, step, best_loss, path):
    """ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€ (Perfect Resume)"""
    # è·å–åŸå§‹ model (å»é™¤ DDP wrapper)
    model_state = model.module.state_dict() if hasattr(model, "module") else model.state_dict()

    # ç§»é™¤ torch.compile() çš„ _orig_mod. å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if any(k.startswith('_orig_mod.') for k in model_state.keys()):
        model_state = {k.replace('_orig_mod.', ''): v for k, v in model_state.items()}

    # æ”¶é›† RNG çŠ¶æ€ï¼ˆå¤šGPUç¯å¢ƒï¼‰
    rng_state = {
        'python': random.getstate(),
        'numpy': np.random.get_state(),
        'torch': torch.get_rng_state(),
        'cuda_all': torch.cuda.get_rng_state_all()  # ä¿å­˜æ‰€æœ‰GPUçŠ¶æ€
    }
    
    state = {
        'epoch': epoch,
        'global_step': step,
        'best_val_loss': best_loss,
        'model_state_dict': model_state,
        'optimizer_state_dict': optimizer.state_dict(),
        'scaler_state_dict': scaler.state_dict(),
        'rng_state': rng_state,
        'config': TrainConfig.model_params # ä¿å­˜é…ç½®æ–¹ä¾¿æ¨ç†
    }
    
    torch.save(state, path)

if __name__ == "__main__":
    # å¿…é¡»ä¸º DDP è®¾ç½® spawn å¯åŠ¨æ–¹å¼
    # è™½ç„¶ torch.distributed.launch ä¹Ÿå¯ä»¥ï¼Œä½†ç›´æ¥ python è¿è¡Œæ›´æ–¹ä¾¿
    os.environ["OMP_NUM_THREADS"] = "1"
    
    # è·å–å¯ç”¨ GPU æ•°é‡å¹¶å¯åŠ¨å¤šè¿›ç¨‹
    n_gpus = torch.cuda.device_count()
    if n_gpus < 2:
        print(f"âš ï¸ Warning: Found {n_gpus} GPUs. DDP meant for multi-gpu.")
    
    # ä½¿ç”¨ torchrun é£æ ¼å¯åŠ¨ (æ¨è):
    # python -m torch.distributed.launch --nproc_per_node=2 train_ddp.py
    # æˆ–è€…ç›´æ¥è¿è¡Œæ­¤è„šæœ¬ (ä¼šè‡ªåŠ¨ fallback åˆ°å•æœºå•å¡æˆ–éœ€è¦æ‰‹åŠ¨ torchrun)
    
    # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œæ£€æµ‹å¦‚æœå·²ç»æ˜¯ rank ç¯å¢ƒç›´æ¥ mainï¼Œå¦åˆ™æç¤º
    if "LOCAL_RANK" in os.environ:
        main()
    else:
        print("è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒ (è‡ªåŠ¨åˆ©ç”¨åŒå¡):")
        print(f"torchrun --nproc_per_node={n_gpus} train_ddp.py")
#```

### ğŸƒâ€â™‚ï¸ å¯åŠ¨å‘½ä»¤ (Start Command)

#ç”±äºä½¿ç”¨äº† DDPï¼Œè¯·åŠ¡å¿…ä½¿ç”¨ `torchrun` å¯åŠ¨ï¼Œè¿™æ · PyTorch ä¼šè‡ªåŠ¨åˆ†é… Rank å’Œç«¯å£ï¼š

#```bash
#cd /home/husrcf/Code/AIAA/JaT
#conda activate AIAA

# å¯åŠ¨åŒå¡è®­ç»ƒ
#torchrun --nproc_per_node=2 train_ddp.py

### ğŸ”¬ ç›‘æ§è®­ç»ƒ (Monitor)

#åœ¨å¦ä¸€ä¸ªç»ˆç«¯æ‰“å¼€ TensorBoardï¼š

#```bash
#tensorboard --logdir=runs/v2_full_run --port 6006 --bind_all