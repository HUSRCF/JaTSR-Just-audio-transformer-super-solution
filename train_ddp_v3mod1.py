"""
JaT-AudioSR V3 MOD1 Training Script (Latent Perceptual Loss Edition)

V3 MOD1 Improvements (åŸºäº2024-2025æœ€æ–°ç ”ç©¶):
1. âœ… Dropout 0.1 (Attention + MLP)
2. âœ… DropPath 0.05 (Stochastic Depth, çº¿æ€§é€’å¢)
3. âœ… Conditional Noise 0.05 (åŸºäºbatch stdçš„è‡ªé€‚åº”å™ªå£°)
4. ğŸ”¥ Frequency-Domain Latent Loss (FFTé«˜é¢‘å¢å¼ºï¼Œå‡å°‘"æ¯›åˆºæ„Ÿ")
5. ğŸ”¥ Multi-Scale Latent Loss (å¤šæ—¶é—´å°ºåº¦ç»“æ„æ•æ‰)
6. âœ… Phase 1 TensorBoardæŒ‡æ ‡ + æ–°å¢Latent Lossç›‘æ§
7. âœ… æ™ºèƒ½æ£€æŸ¥ç‚¹ç®¡ç† (æ—¶é—´æˆ³æ–‡ä»¶å¤¹ + auto-resume)

Latent Perceptual Lossç‰¹æ€§:
- âœ… åœ¨Latent Spaceç›´æ¥æ–½åŠ æ„ŸçŸ¥çº¦æŸ (æ— éœ€DACè§£ç )
- âœ… è®¡ç®—å¼€é”€æå° (~2-3ms/step vs Mel lossçš„~15ms)
- âœ… ä¸torch.compile()å®Œå…¨å…¼å®¹
- âœ… æ”¹å–„é«˜é¢‘ç»†èŠ‚ï¼Œå‡å°‘éŸ³é¢‘artifact

å‚è€ƒæ–‡çŒ®:
- Boosting Latent Diffusion with Perceptual Objectives (2024)
- FreSca: Scaling in Frequency Space (2024)
- Smooth Diffusion: Crafting Smooth Latent Spaces (CVPR 2024)
"""

import os
import argparse
import json
import random
import math
import time
from pathlib import Path
from datetime import timedelta, datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import AdamW
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

# å¼•å…¥ V2 æ¨¡å‹
# ç¡®ä¿æ‚¨çš„ src ç›®å½•åœ¨ python path ä¸­
from src.models.jat_audiosr_v2 import JaT_AudioSR_V2

# ==============================================================================
# ğŸ¯ Latent Perceptual Loss Modules (2024-2025 SOTA)
# ==============================================================================

class FrequencyDomainLatentLoss(nn.Module):
    """
    é¢‘åŸŸLatentæŸå¤± - åœ¨FFTåŸŸæ–½åŠ çº¦æŸï¼Œå¼ºè°ƒé«˜é¢‘ç»†èŠ‚

    å‚è€ƒ: FreSca: Scaling in Frequency Space (2024)
    https://arxiv.org/html/2504.02154v2

    ç‰¹æ€§:
    - çº¯PyTorchæ“ä½œ (torch.fft)
    - é«˜é¢‘å¢å¼ºæƒé‡ (å‡å°‘"æ¯›åˆºæ„Ÿ")
    - å¹…åº¦ + ç›¸ä½åŒé‡çº¦æŸ
    - å®Œå…¨å…¼å®¹torch.compile()
    """
    def __init__(self, high_freq_weight=2.0):
        super().__init__()
        self.high_freq_weight = high_freq_weight

    def forward(self, pred_latent, target_latent):
        """
        Args:
            pred_latent: [B, C, T] é¢„æµ‹çš„latent
            target_latent: [B, C, T] GT latent
        Returns:
            loss: scalar
        """
        # ğŸ”¥ æ ¸å¿ƒä¿®å¤: å¼ºåˆ¶è½¬ä¸º FP32
        # 1. è§£å†³ cuFFT FP16 å¿…é¡»æ˜¯ 2 çš„å¹‚çš„é™åˆ¶ (1378 ä¸æ˜¯ 2 çš„å¹‚)
        # 2. è§£å†³ FP16 ä¸‹ç›¸ä½è®¡ç®—ç²¾åº¦æä½çš„é—®é¢˜
        pred_latent = pred_latent.float()
        target_latent = target_latent.float()

        # æ²¿æ—¶é—´ç»´åº¦åšFFT (å®ä¿¡å·ç”¨rfft)
        pred_fft = torch.fft.rfft(pred_latent, dim=-1)  # [B, C, T//2+1] complex
        target_fft = torch.fft.rfft(target_latent, dim=-1)

        # 1. Complex L1 Loss (éšå¼åŒ…å«å¹…åº¦+ç›¸ä½ï¼Œæ•°å€¼æ›´ç¨³å®š)
        # ä¼˜åŠ¿ï¼šè‡ªåŠ¨å¿½ç•¥ä½å¹…åº¦é¢‘æ®µçš„ç›¸ä½å™ªå£°ï¼Œä¸ä¼šæœ‰æ¢¯åº¦æ··ä¹±é—®é¢˜
        complex_l1_loss = torch.abs(pred_fft - target_fft).mean()

        # 2. å¹…åº¦è°±æŸå¤± (æ˜¾å¼çº¦æŸå¹…åº¦)
        pred_mag = torch.abs(pred_fft)
        target_mag = torch.abs(target_fft)
        mag_loss = F.l1_loss(pred_mag, target_mag)

        # 3. é«˜é¢‘å¢å¼º (çº¿æ€§æƒé‡é€’å¢ï¼Œå¼ºè°ƒé«˜é¢‘ç»†èŠ‚)
        freq_bins = pred_mag.shape[-1]
        freq_weights = torch.linspace(1.0, self.high_freq_weight, freq_bins,
                                       device=pred_latent.device)
        freq_weights = freq_weights.view(1, 1, -1)  # [1, 1, F]

        weighted_mag_loss = (freq_weights * torch.abs(pred_mag - target_mag)).mean()

        # ç»„åˆæŸå¤±ï¼šComplex L1 + å¹…åº¦ + é«˜é¢‘å¢å¼º
        # Complex L1å·²åŒ…å«ç›¸ä½ä¿¡æ¯ï¼ˆéšå¼å¹…åº¦åŠ æƒï¼‰ï¼Œæ— éœ€é¢å¤–ç›¸ä½æŸå¤±
        # é™ä½ mag_loss æƒé‡é¿å…ä¸ complex_l1 å†—ä½™ï¼Œä¸»è¦ä¾èµ– weighted_mag_loss åšé«˜é¢‘å¢å¼º
        total_loss = 0.5 * complex_l1_loss + 0.2 * mag_loss + 0.5 * weighted_mag_loss

        return total_loss

        # ğŸ“ å¤‡æ³¨ï¼šå¹…åº¦åŠ æƒç›¸ä½æŸå¤±çš„å®ç°æ–¹æ¡ˆï¼ˆæœªä½¿ç”¨ï¼Œä½†ä½œä¸ºå‚è€ƒï¼‰
        # pred_phase = torch.angle(pred_fft)
        # target_phase = torch.angle(target_fft)
        # mag_weight = target_mag / (target_mag.sum(dim=-1, keepdim=True) + 1e-8)
        # cosine_sim = F.cosine_similarity(...)
        # phase_loss = (mag_weight * (1.0 - cosine_sim)).sum(dim=-1).mean()


class MultiScaleLatentLoss(nn.Module):
    """
    å¤šå°ºåº¦LatentæŸå¤± - åœ¨ä¸åŒæ—¶é—´å°ºåº¦ä¸Šè®¡ç®—æŸå¤±

    å‚è€ƒ: Smooth Diffusion: Crafting Smooth Latent Spaces (CVPR 2024)
    https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_Smooth_Diffusion_CVPR_2024_paper.pdf

    ç‰¹æ€§:
    - å¤šæ—¶é—´å°ºåº¦ (åŸå§‹ + 2x, 4xé™é‡‡æ ·)
    - æ•æ‰ç²—ç²’åº¦åˆ°ç»†ç²’åº¦ç»“æ„
    - æ”¹å–„latent spaceå¹³æ»‘æ€§
    - æå°è®¡ç®—å¼€é”€
    """
    def __init__(self, scales=[1, 2, 4]):
        super().__init__()
        self.scales = scales

        # é¢„åˆ›å»ºé™é‡‡æ ·å±‚ (Average Pooling)
        self.pools = nn.ModuleList([
            nn.AvgPool1d(kernel_size=s, stride=s)
            for s in scales[1:]  # è·³è¿‡scale=1
        ])

    def forward(self, pred_latent, target_latent):
        """
        Args:
            pred_latent: [B, C, T]
            target_latent: [B, C, T]
        Returns:
            loss: scalar
        """
        total_loss = 0.0

        # 1. åŸå§‹å°ºåº¦ (scale=1)
        total_loss += F.l1_loss(pred_latent, target_latent)

        # 2. é™é‡‡æ ·å°ºåº¦
        for pool in self.pools:
            pred_down = pool(pred_latent)
            target_down = pool(target_latent)
            total_loss += F.l1_loss(pred_down, target_down)

        # å¹³å‡ (é¿å…scaleæ•°é‡å½±å“lossæ•°å€¼)
        return total_loss / len(self.scales)


class CombinedLatentPerceptualLoss(nn.Module):
    """
    ç»„åˆLatentæ„ŸçŸ¥æŸå¤± - Frequency + Multi-Scale

    ç‰¹æ€§:
    - é¢‘åŸŸé«˜é¢‘å¢å¼º (é’ˆå¯¹"æ¯›åˆºæ„Ÿ")
    - å¤šå°ºåº¦æ—¶é—´ç»“æ„
    - å¯è°ƒæƒé‡å¹³è¡¡
    """
    def __init__(self, freq_weight=1.0, ms_weight=0.5):
        super().__init__()
        self.freq_loss = FrequencyDomainLatentLoss(high_freq_weight=2.0)
        self.ms_loss = MultiScaleLatentLoss(scales=[1, 2, 4])
        self.freq_weight = freq_weight
        self.ms_weight = ms_weight

    def forward(self, pred_latent, target_latent):
        """
        Args:
            pred_latent: [B, C, T]
            target_latent: [B, C, T]
        Returns:
            loss: scalar
            loss_dict: dict with individual losses for logging
        """
        freq_loss = self.freq_loss(pred_latent, target_latent)
        ms_loss = self.ms_loss(pred_latent, target_latent)

        total = self.freq_weight * freq_loss + self.ms_weight * ms_loss

        # è¿”å›ç»„åˆæŸå¤± + å•ç‹¬æŸå¤± (ç”¨äºç›‘æ§)
        loss_dict = {
            'freq_loss': freq_loss.item(),
            'ms_loss': ms_loss.item(),
            'total_latent_loss': total.item()
        }

        return total, loss_dict

# ==============================================================================
# âš™ï¸ é…ç½®ä¸­å¿ƒ (Configuration)
# ==============================================================================

class TrainConfig:
    # --- ç¡¬ä»¶ä¸å¹¶è¡Œ ---
    seed = 42
    num_workers = 16         
    
    # --- æ•°æ® ---
    data_dir = "data_processed_v13_final"  # æŒ‡å‘ prepare_dataset_v5 ç”Ÿæˆçš„ç›®å½•
    stats_file = "global_stats_separated.json"  # åˆ†ç¦»çš„ HR/LR ç»Ÿè®¡é‡
    target_duration = 16.0                 # åˆ‡ç‰‡é•¿åº¦ (ç§’)
    dac_sample_rate = 44100
    dac_hop_length = 512    # DAC é»˜è®¤å‹ç¼©ç‡
    
    # è®¡ç®— Latent å¸§æ•°: 16 * 44100 / 512 â‰ˆ 1378
    target_frames = int(target_duration * dac_sample_rate / dac_hop_length)

    # --- æ¨¡å‹ (V2 Specs) ---
    model_params = {
        'input_channels': 1024,
        'cond_channels': 1024,
        'patch_len': 4,
        'hidden_size': 1280,
        'depth': 28,
        'num_q_heads': 20,
        'num_kv_heads': 4,
        'bottleneck_dim': 512,
        'mlp_ratio': 4.0,
        'dropout': 0.1,         # ğŸ”¥ ä¿®å¤è¿‡æ‹Ÿåˆï¼šå¼€å¯Dropout
        'drop_path_rate': 0.05  # ğŸ”¥ Stochastic Depthï¼šçº¿æ€§é€’å¢ 0â†’0.05
    }

    # --- æ­£åˆ™åŒ–å¢å¼º (V2) ---
    condition_noise_ratio = 0.05  # Conditional Noise: 5%ç›¸å¯¹å™ªå£°
    use_adaptive_noise = True     # ä½¿ç”¨è‡ªé€‚åº”å™ªå£°ï¼ˆåŸºäºbatch stdï¼‰

    # --- Latent Perceptual Loss (MOD1 æ–°å¢) ---
    use_latent_perceptual_loss = True  # å¯ç”¨Latentæ„ŸçŸ¥æŸå¤±
    latent_loss_weight = 0.3           # LatentæŸå¤±æƒé‡ (ç›¸å¯¹MSE)
    freq_loss_weight = 0.5             # é¢‘åŸŸæŸå¤±æƒé‡
    ms_loss_weight = 0.5               # å¤šå°ºåº¦æŸå¤±æƒé‡
    high_freq_emphasis = 2.0           # é«˜é¢‘å¢å¼ºå€æ•° (1.0-3.0)

    # --- è®­ç»ƒè¶…å‚ ---
    batch_size = 28         # å•å¡ Batch (æ€» Batch = 60)
    lr = 5e-5               # ç¨³å¥å­¦ä¹ ç‡
    weight_decay = 0.1
    warmup_steps = 1000     # é¢„çƒ­æ­¥æ•°
    num_epochs = 300       # è·‘æ»¡ä¸ºæ­¢
    grad_clip = 1.0

    # --- ä¿å­˜ä¸æ—¥å¿— ---
    save_dir_base = "checkpoints/v3mod1_full_run"  # MOD1åŸºç¡€ç›®å½•
    log_dir_base = "runs/v3mod1_full_run"          # MOD1åŸºç¡€ç›®å½•
    save_interval_steps = 1000  # æ¯ 1000 æ­¥ä¿å­˜ä¸€ä¸ª interval checkpoint

# ==============================================================================
# ğŸ› ï¸ å·¥å…·å‡½æ•°
# ==============================================================================

def get_timestamp_folder():
    """ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å¤¹åï¼ˆæ ¼å¼ï¼šMMDDHHMMï¼‰"""
    now = datetime.now()
    return now.strftime("%m%d%H%M")

def find_latest_checkpoint_dir(base_dir):
    """
    åœ¨base_dirä¸‹æŸ¥æ‰¾æœ€æ–°çš„æ—¶é—´æˆ³å­æ–‡ä»¶å¤¹
    è¿”å›ï¼š(æœ€æ–°æ–‡ä»¶å¤¹è·¯å¾„, checkpointè·¯å¾„) æˆ– (None, None)
    """
    if not os.path.exists(base_dir):
        return None, None

    # æŸ¥æ‰¾æ‰€æœ‰æ—¶é—´æˆ³æ ¼å¼çš„å­æ–‡ä»¶å¤¹ï¼ˆ8ä½æ•°å­—ï¼‰
    subdirs = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path) and item.isdigit() and len(item) == 8:
            subdirs.append((item, item_path))

    if not subdirs:
        return None, None

    # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œå–æœ€æ–°çš„
    subdirs.sort(reverse=True)
    latest_folder = subdirs[0][1]

    # æ£€æŸ¥æ˜¯å¦æœ‰last.pt
    checkpoint_path = os.path.join(latest_folder, "last.pt")
    if os.path.exists(checkpoint_path):
        return latest_folder, checkpoint_path

    return latest_folder, None

def setup_ddp():
    """åˆå§‹åŒ– DDP ç¯å¢ƒ"""
    if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        local_rank = int(os.environ["LOCAL_RANK"])
    else:
        # é»˜è®¤å•æœºå¤šå¡
        rank = 0
        world_size = torch.cuda.device_count()
        local_rank = 0
        os.environ["MASTER_ADDR"] = "localhost"
        os.environ["MASTER_PORT"] = "12355"
        os.environ["RANK"] = "0"
        os.environ["WORLD_SIZE"] = str(world_size)

    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend="nccl", init_method="env://", world_size=world_size, rank=rank)
    return rank, local_rank, world_size

def cleanup_ddp():
    dist.destroy_process_group()

def u_shaped_timestep_sampling(batch_size, device, alpha=0.5):
    """V2 æ ¸å¿ƒ: U-shaped Timestep Sampling"""
    u = torch.rand(batch_size, device=device)
    t = torch.where(
        u < 0.5,
        (2 * u) ** alpha / 2,
        1 - ((2 * (1 - u)) ** alpha) / 2
    )
    return t

def load_global_stats(stats_path, device):
    """åŠ è½½å¹¶å¹¿æ’­å…¨å±€ç»Ÿè®¡é‡"""
    with open(stats_path, 'r') as f:
        data = json.load(f)
    
    # è½¬æ¢ä¸º Tensor (1, C, 1) ç”¨äºå¹¿æ’­
    hr_mean = torch.tensor(data['hr_mean']).view(1, -1, 1).to(device)
    hr_std = torch.tensor(data['hr_std']).view(1, -1, 1).to(device)
    lr_mean = torch.tensor(data['lr_mean']).view(1, -1, 1).to(device)
    lr_std = torch.tensor(data['lr_std']).view(1, -1, 1).to(device)
    
    return hr_mean, hr_std, lr_mean, lr_std

# ==============================================================================
# ğŸ’¾ æ•°æ®é›† (Dataset)
# ==============================================================================

class LatentDataset(Dataset):
    """
    è®­ç»ƒæ•°æ®é›†ï¼šä½¿ç”¨LRU cacheç¼“å­˜mmapæ–‡ä»¶å¼•ç”¨
    ç”±äºå·²ä¿®å¤Float Bombï¼Œç°åœ¨æ¯ä¸ªæ ·æœ¬åªæœ‰~11MBï¼Œç¼“å­˜å®Œå…¨å¯è¡Œ
    """
    def __init__(self, data_dir, split, target_frames, samples_per_epoch_multiplier=6, cache_size=128):
        super().__init__()
        self.target_frames = target_frames
        self.multiplier = samples_per_epoch_multiplier

        # æ‰«ææ‰€æœ‰ .pt æ–‡ä»¶
        search_path = Path(data_dir) / split
        self.files = sorted(list(search_path.glob("*.pt")))

        if len(self.files) == 0:
            raise ValueError(f"No .pt files found in {search_path}")

        print(f"[{split}] Found {len(self.files)} files (x{self.multiplier} samples/epoch = {len(self.files) * self.multiplier} total samples)")
        print(f"[{split}] Using LRU cache (size={cache_size}, ~{cache_size * 11 / 1024:.1f}GB per worker)")

        # LRUç¼“å­˜mmapçš„æ–‡ä»¶å¼•ç”¨ï¼ˆä¸ç¼“å­˜è½¬æ¢åçš„æ•°æ®ï¼‰
        from functools import lru_cache
        @lru_cache(maxsize=cache_size)
        def load_file_mmap(path_str):
            """ç¼“å­˜mmapåŠ è½½çš„æ–‡ä»¶ï¼ˆå†…å­˜å ç”¨æå°ï¼Œåªæ˜¯å¼•ç”¨ï¼‰"""
            data = torch.load(path_str, map_location='cpu', mmap=True, weights_only=False)
            return data['hr_latent'], data['lr_latent']  # ä¿æŒFP16

        self._load_file = load_file_mmap

    def __len__(self):
        return len(self.files) * self.multiplier

    def __getitem__(self, idx):
        # æ˜ å°„åˆ°å®é™…æ–‡ä»¶ç´¢å¼•
        file_idx = idx % len(self.files)
        path = self.files[file_idx]

        # ä»cacheåŠ è½½mmapå¼•ç”¨ï¼ˆFP16ï¼Œå‡ ä¹ä¸å å†…å­˜ï¼‰
        hr, lr = self._load_file(str(path))

        length = hr.shape[-1]

        # ğŸ”¥ å…³é”®ä¿®å¤2: å…ˆåˆ‡ç‰‡ï¼ˆFP16ï¼‰ï¼Œå†è½¬æ¢ï¼ˆFP32ï¼‰
        if length < self.target_frames:
            # A. çŸ­éŸ³é¢‘: å¾ªç¯æ‹¼æ¥
            repeats = math.ceil(self.target_frames / length)
            hr = hr.repeat(1, repeats)[..., :self.target_frames]
            lr = lr.repeat(1, repeats)[..., :self.target_frames]
        else:
            # B. é•¿éŸ³é¢‘: éšæœºåˆ‡ç‰‡
            start = random.randint(0, length - self.target_frames)
            hr = hr[..., start : start + self.target_frames]
            lr = lr[..., start : start + self.target_frames]

        # æœ€åæ‰è½¬FP32ï¼ˆåªè½¬æ¢16sç‰‡æ®µï¼Œå†…å­˜å ç”¨å°ï¼‰
        hr = hr.float()
        lr = lr.float()

        return hr, lr

class ValidationDataset(Dataset):
    """éªŒè¯é›†ä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·ï¼Œä¿è¯å¯é‡å¤æ€§ï¼Œå¸¦LRU cache"""
    def __init__(self, data_dir, split, target_frames, samples_per_epoch_multiplier=6, cache_size=128):
        super().__init__()
        self.target_frames = target_frames
        self.multiplier = samples_per_epoch_multiplier
        search_path = Path(data_dir) / split
        self.files = sorted(list(search_path.glob("*.pt")))

        print(f"[{split}] Found {len(self.files)} files (x{self.multiplier} samples/epoch = {len(self.files) * self.multiplier} total samples)")
        print(f"[{split}] Using LRU cache (size={cache_size}, ~{cache_size * 11 / 1024:.1f}GB per worker)")

        # LRUç¼“å­˜mmapå¼•ç”¨
        from functools import lru_cache
        @lru_cache(maxsize=cache_size)
        def load_file_mmap(path_str):
            data = torch.load(path_str, map_location='cpu', mmap=True, weights_only=False)
            return data['hr_latent'], data['lr_latent']

        self._load_file = load_file_mmap

    def __len__(self):
        return len(self.files) * self.multiplier

    def __getitem__(self, idx):
        # æ˜ å°„åˆ°å®é™…æ–‡ä»¶å’Œé‡‡æ ·ä½ç½®
        file_idx = idx % len(self.files)
        sample_idx = idx // len(self.files)  # 0, 1, 2, ..., multiplier-1
        path = self.files[file_idx]

        # ä»cacheåŠ è½½mmapå¼•ç”¨
        hr, lr = self._load_file(str(path))

        length = hr.shape[-1]

        # ğŸ”¥ å…³é”®ä¿®å¤2: å…ˆåˆ‡ç‰‡ï¼ˆFP16ï¼‰ï¼Œå†è½¬æ¢ï¼ˆFP32ï¼‰
        if length < self.target_frames:
            # çŸ­éŸ³é¢‘: å¾ªç¯æ‹¼æ¥
            repeats = math.ceil(self.target_frames / length)
            hr = hr.repeat(1, repeats)[..., :self.target_frames]
            lr = lr.repeat(1, repeats)[..., :self.target_frames]
        else:
            # é•¿éŸ³é¢‘: ç¡®å®šæ€§é‡‡æ ·ï¼ˆæ ¹æ®sample_idxå‡åŒ€åˆ†å¸ƒï¼‰
            # å°†éŸ³é¢‘åˆ†æˆ multiplier ä¸ªåŒºé—´ï¼Œæ¯æ¬¡ä»å¯¹åº”åŒºé—´é‡‡æ ·
            if self.multiplier == 1:
                # åªé‡‡æ ·1æ¬¡ï¼šcenter crop
                start = (length - self.target_frames) // 2
            else:
                # å¤šæ¬¡é‡‡æ ·ï¼šå‡åŒ€åˆ†å¸ƒ
                segment_length = max(length - self.target_frames, 1)
                start = int(segment_length * sample_idx / (self.multiplier - 1))
                start = min(start, length - self.target_frames)

            hr = hr[..., start : start + self.target_frames]
            lr = lr[..., start : start + self.target_frames]

        # æœ€åæ‰è½¬FP32ï¼ˆåªè½¬æ¢16sç‰‡æ®µï¼‰
        hr = hr.float()
        lr = lr.float()

        return hr, lr

# ==============================================================================
# ğŸš€ è®­ç»ƒä¸»æµç¨‹
# ==============================================================================

def main():
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='JaT V2 Training with DDP')
    parser.add_argument('--resume', type=str, nargs='?', const='auto', default=None,
                        help='Resume training. Use --resume for auto (latest), or --resume <path> for specific checkpoint')
    args = parser.parse_args()

    # 2. DDP åˆå§‹åŒ–
    rank, local_rank, world_size = setup_ddp()
    is_master = (rank == 0)

    cfg = TrainConfig()

    # 3. æ™ºèƒ½æ—¶é—´æˆ³ç›®å½•è®¾ç½®
    resume_path = None
    timestamp_folder = None
    save_dir = ""  # åˆå§‹åŒ–ï¼Œé¿å…ç±»å‹æ£€æŸ¥è­¦å‘Š
    log_dir = ""   # åˆå§‹åŒ–ï¼Œé¿å…ç±»å‹æ£€æŸ¥è­¦å‘Š

    if args.resume:
        if args.resume == 'auto':
            # è‡ªåŠ¨ä»æœ€æ–°æ—¶é—´æˆ³æ–‡ä»¶å¤¹æ¢å¤
            latest_dir, latest_ckpt = find_latest_checkpoint_dir(cfg.save_dir_base)
            if latest_ckpt:
                resume_path = latest_ckpt
                save_dir = latest_dir
                log_dir = latest_dir.replace("checkpoints", "runs")
                if is_master:
                    timestamp_folder = os.path.basename(latest_dir)
                    print(f"ğŸ”„ Auto resume from latest: {timestamp_folder}")
                    print(f"   Checkpoint: {resume_path}")
            else:
                if is_master:
                    print("âš ï¸  No checkpoint found, starting new training")
                timestamp_folder = get_timestamp_folder()
                save_dir = os.path.join(cfg.save_dir_base, timestamp_folder)
                log_dir = os.path.join(cfg.log_dir_base, timestamp_folder)
                if is_master:
                    print(f"ğŸ“ New training session: {timestamp_folder}")
        else:
            # æ‰‹åŠ¨æŒ‡å®šcheckpointè·¯å¾„
            resume_path = args.resume
            if not os.path.exists(resume_path):
                raise FileNotFoundError(f"Checkpoint not found: {resume_path}")
            # ä»checkpointè·¯å¾„æå–ç›®å½•
            save_dir = str(Path(resume_path).parent)
            log_dir = save_dir.replace("checkpoints", "runs")
            if is_master:
                timestamp_folder = os.path.basename(save_dir)
                print(f"ğŸ“‚ Manual resume from: {timestamp_folder}")
                print(f"   Checkpoint: {resume_path}")
    else:
        # æ–°è®­ç»ƒï¼Œåˆ›å»ºæ—¶é—´æˆ³æ–‡ä»¶å¤¹
        timestamp_folder = get_timestamp_folder()
        save_dir = os.path.join(cfg.save_dir_base, timestamp_folder)
        log_dir = os.path.join(cfg.log_dir_base, timestamp_folder)
        if is_master:
            print(f"ğŸ“ New training session: {timestamp_folder}")

    # åˆ›å»ºç›®å½•
    if is_master:
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)
        writer = SummaryWriter(log_dir=log_dir)
        print("="*60)
        print(f"ğŸš€ JaT V2 Training Started")
        print(f"   Save Dir: {save_dir}")
        print(f"   Log Dir: {log_dir}")
        print(f"   GPUs: {world_size} | Batch/GPU: {cfg.batch_size} | Total Batch: {cfg.batch_size * world_size}")
        print(f"   Target Duration: {cfg.target_duration}s ({cfg.target_frames} frames)")
        print(f"   Resume Mode: {'ON' if resume_path else 'OFF'}")
        print("="*60)
    
    # 3. å‡†å¤‡æ•°æ®
    train_dataset = LatentDataset(cfg.data_dir, "train", cfg.target_frames)
    val_dataset = ValidationDataset(cfg.data_dir, "val", cfg.target_frames)
    
    train_sampler = DistributedSampler(train_dataset, shuffle=True)
    val_sampler = DistributedSampler(val_dataset, shuffle=False) # éªŒè¯é›†ä¸éœ€è¦ shuffle
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=cfg.batch_size,
        sampler=train_sampler,
        num_workers=cfg.num_workers,
        pin_memory=True, # åŠ é€Ÿ Host -> Device
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=cfg.batch_size,
        sampler=val_sampler,
        num_workers=cfg.num_workers,
        pin_memory=True,
        drop_last=True
    )
    
    # 4. åŠ è½½ç»Ÿè®¡é‡ (æ¯ä¸ªè¿›ç¨‹éƒ½åŠ è½½)
    stats_path = os.path.join(cfg.data_dir, cfg.stats_file)
    hr_mean, hr_std, lr_mean, lr_std = load_global_stats(stats_path, f"cuda:{local_rank}")
    
    # 5. æ¨¡å‹åˆå§‹åŒ–
    model = JaT_AudioSR_V2(**cfg.model_params).to(local_rank)
    
    # 6. ä¼˜åŒ–å™¨ & è°ƒåº¦å™¨
    optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    # æ‰‹åŠ¨å®ç° Cosine with Warmupï¼Œæ–¹ä¾¿çŠ¶æ€ä¿å­˜
    def get_lr(step, total_steps, warmup_steps, base_lr):
        if step < warmup_steps:
            return base_lr * (step / max(1, warmup_steps))
        else:
            progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)
            return base_lr * 0.5 * (1.0 + math.cos(math.pi * progress))

    # 6.5 Latent Perceptual Loss (MOD1 æ–°å¢)
    latent_loss_fn = None
    if cfg.use_latent_perceptual_loss:
        latent_loss_fn = CombinedLatentPerceptualLoss(
            freq_weight=cfg.freq_loss_weight,
            ms_weight=cfg.ms_loss_weight
        ).to(local_rank)

        # è®¾ä¸ºè¯„ä¼°æ¨¡å¼ (æ— éœ€è®­ç»ƒå‚æ•°ï¼Œä»…ç”¨äºæ¢¯åº¦ä¼ é€’)
        latent_loss_fn.eval()

        if is_master:
            print("=" * 60)
            print("ğŸ¯ Latent Perceptual Loss Enabled (MOD1)")
            print("=" * 60)
            print(f"  Latent Loss Weight: {cfg.latent_loss_weight}")
            print(f"  Frequency Loss Weight: {cfg.freq_loss_weight}")
            print(f"  Multi-Scale Loss Weight: {cfg.ms_loss_weight}")
            print(f"  High-Freq Emphasis: {cfg.high_freq_emphasis}x")
            print("=" * 60)

    # 7. æ··åˆç²¾åº¦ Scaler
    scaler = torch.amp.GradScaler('cuda')
    
    # 8. æ¢å¤è®­ç»ƒé€»è¾‘ (Perfect Resume)
    start_epoch = 0
    global_step = 0
    best_val_loss = float('inf')

    if resume_path and os.path.exists(resume_path):
        if is_master:
            print(f"â™»ï¸  Resuming from {resume_path}...")

        # map_location ç¡®ä¿åŠ è½½åˆ°å½“å‰ GPU
        # weights_only=False: PyTorch 2.6+éœ€è¦ï¼Œå› ä¸ºcheckpointåŒ…å«RNGçŠ¶æ€ç­‰
        checkpoint = torch.load(resume_path, map_location=f"cuda:{local_rank}", weights_only=False)

        # å¤„ç† torch.compile() çš„ _orig_mod. å‰ç¼€ï¼ˆå…¼å®¹å·²ç¼–è¯‘çš„æ£€æŸ¥ç‚¹ï¼‰
        state_dict = checkpoint['model_state_dict']
        if any(k.startswith('_orig_mod.') for k in state_dict.keys()):
            state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}
            if is_master:
                print("ğŸ”§ Removed '_orig_mod.' prefix from compiled checkpoint")

        model.load_state_dict(state_dict)
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        start_epoch = checkpoint['epoch'] + 1
        global_step = checkpoint['global_step']
        best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        
        # æ¢å¤ RNG çŠ¶æ€ (Python, Numpy, PyTorch, CUDA)
        if 'rng_state' in checkpoint:
            try:
                random.setstate(checkpoint['rng_state']['python'])
                np.random.set_state(checkpoint['rng_state']['numpy'])

                # PyTorch RNG çŠ¶æ€éœ€è¦æ˜¯ ByteTensor (dtype=torch.uint8)
                torch_rng = checkpoint['rng_state']['torch']
                if torch_rng.dtype != torch.uint8:
                    torch_rng = torch_rng.to(torch.uint8)
                torch.set_rng_state(torch_rng.cpu())

                # CUDA RNG çŠ¶æ€æ¢å¤ï¼ˆå…¼å®¹æ–°æ—§æ ¼å¼ï¼‰
                if 'cuda_all' in checkpoint['rng_state']:
                    # æ–°æ ¼å¼ï¼šå¤šGPUçŠ¶æ€
                    cuda_states = checkpoint['rng_state']['cuda_all']
                    # ç¡®ä¿æ¯ä¸ªçŠ¶æ€éƒ½æ˜¯æ­£ç¡®ç±»å‹
                    cuda_states = [s.to(torch.uint8).cpu() if s.dtype != torch.uint8 else s.cpu()
                                   for s in cuda_states]
                    torch.cuda.set_rng_state_all(cuda_states)
                elif 'cuda' in checkpoint['rng_state']:
                    # æ—§æ ¼å¼ï¼šå•GPUçŠ¶æ€ï¼ˆå‘åå…¼å®¹ï¼‰
                    cuda_state = checkpoint['rng_state']['cuda']
                    if cuda_state.dtype != torch.uint8:
                        cuda_state = cuda_state.to(torch.uint8)
                    torch.cuda.set_rng_state(cuda_state.cpu())

                if is_master:
                    print("ğŸ² RNG states restored")
            except Exception as e:
                if is_master:
                    print(f"âš ï¸  Warning: Could not restore RNG states: {e}")
                    print("   Training will continue with random initialization")
            
        if is_master:
            print(f"âœ… Resumed at Epoch {start_epoch}, Step {global_step}")

    # 9. torch.compile ä¼˜åŒ–ï¼ˆä¿å®ˆæ¨¡å¼ï¼‰
    if is_master:
        print("ğŸ”¥ Compiling model with torch.compile (mode='default')...")

    model = torch.compile(model, mode='default', backend='inductor')

    if is_master:
        print("âœ… Model compiled successfully")

    # 10. åŒ…è£… DDP
    model = DDP(model, device_ids=[local_rank], find_unused_parameters=False) # JaTæ— æœªä½¿ç”¨å‚æ•°ï¼ŒFalseæ›´å¿«

    # ==========================================================================
    # ğŸ‹ï¸ è®­ç»ƒå¾ªç¯
    # ==========================================================================
    
    total_steps_approx = len(train_loader) * cfg.num_epochs
    
    for epoch in range(start_epoch, cfg.num_epochs):
        model.train()
        train_sampler.set_epoch(epoch) # DDP å¿…é¡»
        
        start_time = time.time()
        epoch_loss = 0.0

        # åˆ›å»º tqdm è¿›åº¦æ¡ï¼ˆåªåœ¨ GPU0 æ˜¾ç¤ºï¼‰
        pbar = tqdm(total=len(train_loader), disable=(not is_master),
                    desc=f"Epoch {epoch}/{cfg.num_epochs}",
                    leave=False, dynamic_ncols=True)

        for hr, lr in train_loader:
            # LR Scheduler Step
            current_lr = get_lr(global_step, total_steps_approx, cfg.warmup_steps, cfg.lr)
            for param_group in optimizer.param_groups:
                param_group['lr'] = current_lr
                
            # æ•°æ®æ¬è¿
            hr = hr.to(local_rank, non_blocking=True)
            lr = lr.to(local_rank, non_blocking=True)
            
            optimizer.zero_grad(set_to_none=True)

            with torch.amp.autocast('cuda'):
                # A. å½’ä¸€åŒ– (Normalize)
                hr_norm = (hr - hr_mean) / hr_std
                lr_norm = (lr - lr_mean) / lr_std

                # A2. Conditional Noise Augmentation (é˜²æ­¢è¿‡åº¦ä¾èµ–LRè¾“å…¥)
                if cfg.condition_noise_ratio > 0:
                    if cfg.use_adaptive_noise:
                        # è‡ªé€‚åº”æ–¹æ¡ˆï¼šåŸºäºbatchå®é™…æ ‡å‡†å·®
                        with torch.no_grad():
                            lr_batch_std = lr_norm.std().clamp(0.5, 2.0)  # ä¿æŠ¤å¼‚å¸¸å€¼
                    else:
                        # ç®€åŒ–æ–¹æ¡ˆï¼šå‡è®¾å½’ä¸€åŒ–å®Œç¾ï¼Œstdâ‰ˆ1
                        lr_batch_std = 1.0

                    # ç”Ÿæˆç›¸å¯¹å™ªå£°å¹¶æ·»åŠ åˆ°LRæ¡ä»¶
                    cond_noise = torch.randn_like(lr_norm) * (cfg.condition_noise_ratio * lr_batch_std)
                    lr_norm = lr_norm + cond_noise

                # B. é‡‡æ · Timesteps (U-shaped)
                B = hr.shape[0]
                t = u_shaped_timestep_sampling(B, hr.device) # [B]

                # C. åŠ å™ª (Flow Matching: z_t = t*x + (1-t)*noise)
                noise = torch.randn_like(hr_norm)
                t_view = t.view(-1, 1, 1)
                z_t = t_view * hr_norm + (1 - t_view) * noise

                # D. æ¨¡å‹é¢„æµ‹ (Pred x0)
                pred_x0 = model(z_t, t, lr_norm)

                # E. Loss (MSE + Latent Perceptual)
                mse_loss = F.mse_loss(pred_x0, hr_norm)

                # E2. Latent Perceptual Loss (MOD1)
                if cfg.use_latent_perceptual_loss and latent_loss_fn is not None:
                    latent_perc_loss, latent_loss_dict = latent_loss_fn(pred_x0, hr_norm)
                    # ç»„åˆæŸå¤±
                    loss = mse_loss + cfg.latent_loss_weight * latent_perc_loss
                else:
                    loss = mse_loss
                    latent_loss_dict = {}

                # ğŸ“Š è®¡ç®—è®­ç»ƒæŒ‡æ ‡ (Phase 1 + MOD1)
                with torch.no_grad():
                    # é¢„æµ‹è´¨é‡ç»Ÿè®¡
                    pred_mean = pred_x0.mean().item()
                    pred_std = pred_x0.std().item()

                    # ä¿¡å™ªæ¯” (SNR in dB)
                    signal_power = (hr_norm ** 2).mean()
                    noise_power = ((pred_x0 - hr_norm) ** 2).mean()
                    snr_db = 10 * torch.log10(signal_power / (noise_power + 1e-8))
                    snr_db = snr_db.item()

                    # Conditional Noiseç›‘æ§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if cfg.condition_noise_ratio > 0:
                        # è®°å½•å®é™…æ·»åŠ çš„å™ªå£°æ ‡å‡†å·®ï¼ˆç”¨äºéªŒè¯ï¼‰
                        if cfg.use_adaptive_noise:
                            actual_noise_std = (cfg.condition_noise_ratio * lr_batch_std).item()
                        else:
                            actual_noise_std = cfg.condition_noise_ratio
            
            # åå‘ä¼ æ’­ & ä¼˜åŒ–
            scaler.scale(loss).backward()

            # ğŸ“Š è®¡ç®—æ¢¯åº¦èŒƒæ•° (Phase 1)
            scaler.unscale_(optimizer)
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)
            grad_norm = grad_norm.item()
            
            scaler.step(optimizer)
            scaler.update()
            
            # è®°å½•
            loss_val = loss.item()
            epoch_loss += loss_val

            if is_master:
                # æ›´æ–° tqdm è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f'{loss_val:.5f}',
                    'lr': f'{current_lr:.2e}',
                    'step': global_step
                })
                pbar.update(1)

                # TensorBoard æ—¥å¿— (æ¯10æ­¥ä¸€æ¬¡)
                if global_step % 10 == 0:
                    writer.add_scalar("Train/Loss", loss_val, global_step)
                    writer.add_scalar("Train/LR", current_lr, global_step)
                    # Phase 1 æŒ‡æ ‡
                    writer.add_scalar("Train/GradNorm", grad_norm, global_step)
                    writer.add_scalar("Train/SNR_dB", snr_db, global_step)
                    writer.add_scalar("Train/PredictionMean", pred_mean, global_step)
                    writer.add_scalar("Train/PredictionStd", pred_std, global_step)
                    # V2 æ­£åˆ™åŒ–æŒ‡æ ‡
                    if cfg.condition_noise_ratio > 0:
                        writer.add_scalar("Train/CondNoiseStd", actual_noise_std, global_step)
                    # MOD1 Latent Perceptual LossæŒ‡æ ‡
                    if cfg.use_latent_perceptual_loss and latent_loss_dict:
                        writer.add_scalar("Train/MSE_Loss", mse_loss.item(), global_step)
                        writer.add_scalar("Train/LatentPerc_FreqLoss", latent_loss_dict['freq_loss'], global_step)
                        writer.add_scalar("Train/LatentPerc_MSLoss", latent_loss_dict['ms_loss'], global_step)
                        writer.add_scalar("Train/LatentPerc_TotalLoss", latent_loss_dict['total_latent_loss'], global_step)

                # ä¿å­˜ Interval Checkpoint
                if global_step > 0 and global_step % cfg.save_interval_steps == 0:
                    save_path = os.path.join(save_dir, f"interval_step_{global_step}.pt")
                    save_checkpoint(
                        model, optimizer, scaler, epoch, global_step, best_val_loss, save_path
                    )

            global_step += 1

        # End of Epoch
        if is_master:
            pbar.close()  # å…³é—­è¿›åº¦æ¡
            avg_epoch_loss = epoch_loss / len(train_loader)
            time_elapsed = str(timedelta(seconds=int(time.time() - start_time)))
            print(f"\nâœ… Epoch {epoch} Done in {time_elapsed} | Avg Loss: {avg_epoch_loss:.5f}")
            
            # ä¿å­˜ Last Checkpoint (æ¯ä¸ª Epoch æ›´æ–°)
            save_path = os.path.join(save_dir, "last.pt")
            save_checkpoint(
                model, optimizer, scaler, epoch, global_step, best_val_loss, save_path
            )

        # ======================================================================
        # ğŸ§ª éªŒè¯å¾ªç¯ (æ¯ä¸ª Epoch ä¸€æ¬¡)
        # ======================================================================
        if is_master: print(f"ğŸ§ª Validating Epoch {epoch}...")

        val_loss, val_loss_std, val_latent_metrics = validate(
            model, val_loader, hr_mean, hr_std, lr_mean, lr_std, local_rank,
            cfg=cfg, latent_loss_fn=latent_loss_fn
        )

        if is_master:
            writer.add_scalar("Val/Loss", val_loss, epoch)
            writer.add_scalar("Val/Loss_Std", val_loss_std, epoch)  # Phase 1 æŒ‡æ ‡
            # MOD1: Latent Perceptual LosséªŒè¯æŒ‡æ ‡
            if val_latent_metrics:
                writer.add_scalar("Val/MSE_Loss", val_latent_metrics['mse_loss'], epoch)
                writer.add_scalar("Val/LatentPerc_FreqLoss", val_latent_metrics['freq_loss'], epoch)
                writer.add_scalar("Val/LatentPerc_MSLoss", val_latent_metrics['ms_loss'], epoch)
                writer.add_scalar("Val/LatentPerc_TotalLoss", val_latent_metrics['total_latent_loss'], epoch)
            print(f"ğŸ“‰ Val Loss: {val_loss:.5f} Â± {val_loss_std:.5f} (Best: {best_val_loss:.5f})")

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                save_path = os.path.join(save_dir, "best.pt")
                # Beståªå­˜çŠ¶æ€ï¼Œå¦‚æœç¡¬ç›˜å¤Ÿï¼Œä¹Ÿå¯ä»¥å­˜å®Œæ•´
                save_checkpoint(
                    model, optimizer, scaler, epoch, global_step, best_val_loss, save_path
                )
                print(f"ğŸ† New Best Model Saved!")
                
        dist.barrier() # åŒæ­¥ç­‰å¾…ä¸»è¿›ç¨‹ä¿å­˜å®Œæ¯•

    cleanup_ddp()

# ==============================================================================
# ğŸ§© è¾…åŠ©åŠŸèƒ½
# ==============================================================================

def validate(model, loader, hr_mean, hr_std, lr_mean, lr_std, device, cfg=None, latent_loss_fn=None):
    """
    DDP éªŒè¯ï¼šè®¡ç®—æ‰€æœ‰å¡çš„å¹³å‡ Loss å’Œæ ‡å‡†å·®
    MOD1: æ”¯æŒlatent perceptual loss
    è¿”å›: (avg_loss, loss_std, latent_metrics_dict)
    """
    model.eval()
    total_loss = torch.zeros(1).to(device)
    total_steps = torch.zeros(1).to(device)
    # æ”¶é›†æ¯ä¸ªbatchçš„lossç”¨äºè®¡ç®—æ ‡å‡†å·®
    all_losses = []

    # MOD1: Latent lossç´¯åŠ å™¨
    total_mse = torch.zeros(1).to(device)
    total_freq_loss = torch.zeros(1).to(device)
    total_ms_loss = torch.zeros(1).to(device)
    total_latent_loss = torch.zeros(1).to(device)

    with torch.no_grad():
        for hr, lr in loader:
            hr = hr.to(device, non_blocking=True)
            lr = lr.to(device, non_blocking=True)

            # Normalize
            hr_norm = (hr - hr_mean) / hr_std
            lr_norm = (lr - lr_mean) / lr_std

            # Fixed Timestep sampling for deterministic validation (Optional)
            # ä¹Ÿå¯ä»¥ç»§ç»­éšæœºï¼Œå› ä¸ºæ•°æ®é‡å¤Ÿå¤§
            t = torch.rand(hr.shape[0], device=device)

            noise = torch.randn_like(hr_norm)
            t_view = t.view(-1, 1, 1)
            z_t = t_view * hr_norm + (1 - t_view) * noise

            # éªŒè¯é›†ä¸€å®šè¦ç”¨ autocast ä¿æŒä¸€è‡´æ€§
            with torch.amp.autocast('cuda'):
                pred_x0 = model(z_t, t, lr_norm)
                mse_loss = F.mse_loss(pred_x0, hr_norm)

                # MOD1: Latent Perceptual Loss
                if cfg is not None and cfg.use_latent_perceptual_loss and latent_loss_fn is not None:
                    latent_perc_loss, latent_loss_dict = latent_loss_fn(pred_x0, hr_norm)
                    loss = mse_loss + cfg.latent_loss_weight * latent_perc_loss

                    # ç´¯åŠ å„é¡¹æŒ‡æ ‡
                    total_mse += mse_loss.detach()
                    total_freq_loss += latent_loss_dict['freq_loss']
                    total_ms_loss += latent_loss_dict['ms_loss']
                    total_latent_loss += latent_loss_dict['total_latent_loss']
                else:
                    loss = mse_loss

            total_loss += loss.detach()  # ä½¿ç”¨detach()é˜²æ­¢æ˜¾å­˜æ³„æ¼
            total_steps += 1
            all_losses.append(loss.item())

    # DDP All-Reduce: æ±‡æ€»æ‰€æœ‰å¡çš„ Loss
    dist.all_reduce(total_loss, op=dist.ReduceOp.SUM)
    dist.all_reduce(total_steps, op=dist.ReduceOp.SUM)

    # MOD1: All-Reduce latent metrics
    if cfg is not None and cfg.use_latent_perceptual_loss and latent_loss_fn is not None:
        dist.all_reduce(total_mse, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_freq_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_ms_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_latent_loss, op=dist.ReduceOp.SUM)

    avg_loss = total_loss / total_steps

    # è®¡ç®—æ ‡å‡†å·® (åªåœ¨å½“å‰GPUä¸Šè®¡ç®—ï¼Œå› ä¸ºDDPä¼šå¯¼è‡´ä¸åŒGPUçœ‹åˆ°ä¸åŒæ•°æ®)
    if len(all_losses) > 1:
        loss_std = torch.tensor(all_losses).std().item()
    else:
        loss_std = 0.0

    # MOD1: æ„å»ºlatent metricså­—å…¸
    latent_metrics = {}
    if cfg is not None and cfg.use_latent_perceptual_loss and latent_loss_fn is not None:
        latent_metrics = {
            'mse_loss': (total_mse / total_steps).item(),
            'freq_loss': (total_freq_loss / total_steps).item(),
            'ms_loss': (total_ms_loss / total_steps).item(),
            'total_latent_loss': (total_latent_loss / total_steps).item()
        }

    model.train()
    return avg_loss.item(), loss_std, latent_metrics

def save_checkpoint(model, optimizer, scaler, epoch, step, best_loss, path):
    """ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€ (Perfect Resume)"""
    # è·å–åŸå§‹ model (å»é™¤ DDP wrapper)
    model_state = model.module.state_dict() if hasattr(model, "module") else model.state_dict()

    # ç§»é™¤ torch.compile() çš„ _orig_mod. å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if any(k.startswith('_orig_mod.') for k in model_state.keys()):
        model_state = {k.replace('_orig_mod.', ''): v for k, v in model_state.items()}

    # æ”¶é›† RNG çŠ¶æ€ï¼ˆå¤šGPUç¯å¢ƒï¼‰
    rng_state = {
        'python': random.getstate(),
        'numpy': np.random.get_state(),
        'torch': torch.get_rng_state(),
        'cuda_all': torch.cuda.get_rng_state_all()  # ä¿å­˜æ‰€æœ‰GPUçŠ¶æ€
    }
    
    state = {
        'epoch': epoch,
        'global_step': step,
        'best_val_loss': best_loss,
        'model_state_dict': model_state,
        'optimizer_state_dict': optimizer.state_dict(),
        'scaler_state_dict': scaler.state_dict(),
        'rng_state': rng_state,
        'config': TrainConfig.model_params # ä¿å­˜é…ç½®æ–¹ä¾¿æ¨ç†
    }
    
    torch.save(state, path)

if __name__ == "__main__":
    # å¿…é¡»ä¸º DDP è®¾ç½® spawn å¯åŠ¨æ–¹å¼
    # è™½ç„¶ torch.distributed.launch ä¹Ÿå¯ä»¥ï¼Œä½†ç›´æ¥ python è¿è¡Œæ›´æ–¹ä¾¿
    os.environ["OMP_NUM_THREADS"] = "1"
    
    # è·å–å¯ç”¨ GPU æ•°é‡å¹¶å¯åŠ¨å¤šè¿›ç¨‹
    n_gpus = torch.cuda.device_count()
    if n_gpus < 2:
        print(f"âš ï¸ Warning: Found {n_gpus} GPUs. DDP meant for multi-gpu.")
    
    # ä½¿ç”¨ torchrun é£æ ¼å¯åŠ¨ (æ¨è):
    # python -m torch.distributed.launch --nproc_per_node=2 train_ddp.py
    # æˆ–è€…ç›´æ¥è¿è¡Œæ­¤è„šæœ¬ (ä¼šè‡ªåŠ¨ fallback åˆ°å•æœºå•å¡æˆ–éœ€è¦æ‰‹åŠ¨ torchrun)
    
    # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œæ£€æµ‹å¦‚æœå·²ç»æ˜¯ rank ç¯å¢ƒç›´æ¥ mainï¼Œå¦åˆ™æç¤º
    if "LOCAL_RANK" in os.environ:
        main()
    else:
        print("è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒ (è‡ªåŠ¨åˆ©ç”¨åŒå¡):")
        print(f"torchrun --nproc_per_node={n_gpus} train_ddp.py")
#```

### ğŸƒâ€â™‚ï¸ å¯åŠ¨å‘½ä»¤ (Start Command)

#ç”±äºä½¿ç”¨äº† DDPï¼Œè¯·åŠ¡å¿…ä½¿ç”¨ `torchrun` å¯åŠ¨ï¼Œè¿™æ · PyTorch ä¼šè‡ªåŠ¨åˆ†é… Rank å’Œç«¯å£ï¼š

#```bash
#cd /home/husrcf/Code/AIAA/JaT
#conda activate AIAA

# å¯åŠ¨åŒå¡è®­ç»ƒ
#torchrun --nproc_per_node=2 train_ddp.py

### ğŸ”¬ ç›‘æ§è®­ç»ƒ (Monitor)

#åœ¨å¦ä¸€ä¸ªç»ˆç«¯æ‰“å¼€ TensorBoardï¼š

#```bash
#tensorboard --logdir=runs/v2_full_run --port 6006 --bind_all