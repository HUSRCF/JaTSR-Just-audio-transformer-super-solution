"""
JaT-AudioSR V3 MOD3 Training Script (Charbonnier Loss + Configurable Weights)

V3 MOD3 Improvements (åŸºäº2024-2025æœ€æ–°ç ”ç©¶):
1. ğŸ”¥ Charbonnier Loss æ›¿ä»£ MSE (æ›´é²æ£’ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)
2. ğŸ”¥ ç»Ÿä¸€Lossé…ç½®åŒº (æ‰€æœ‰æƒé‡é›†ä¸­ç®¡ç†)

V3 MOD2 Improvements (ç»§æ‰¿):
1. âœ… Dropout 0.1 (Attention + MLP)
2. âœ… DropPath 0.05 (Stochastic Depth, çº¿æ€§é€’å¢)
3. âœ… Conditional Noise 0.05 (åŸºäºbatch stdçš„è‡ªé€‚åº”å™ªå£°)
4. ğŸ”¥ Frequency-Domain Latent Loss (FFTé«˜é¢‘å¢å¼ºï¼Œå‡å°‘"æ¯›åˆºæ„Ÿ")
5. ğŸ”¥ Multi-Scale Latent Loss (å¤šæ—¶é—´å°ºåº¦ç»“æ„æ•æ‰)
6. âœ… Phase 1 TensorBoardæŒ‡æ ‡ + æ–°å¢Latent Lossç›‘æ§
7. âœ… æ™ºèƒ½æ£€æŸ¥ç‚¹ç®¡ç† (æ—¶é—´æˆ³æ–‡ä»¶å¤¹ + auto-resume)

Latent Perceptual Lossç‰¹æ€§:
- âœ… åœ¨Latent Spaceç›´æ¥æ–½åŠ æ„ŸçŸ¥çº¦æŸ (æ— éœ€DACè§£ç )
- âœ… è®¡ç®—å¼€é”€æå° (~2-3ms/step vs Mel lossçš„~15ms)
- âœ… ä¸torch.compile()å®Œå…¨å…¼å®¹
- âœ… æ”¹å–„é«˜é¢‘ç»†èŠ‚ï¼Œå‡å°‘éŸ³é¢‘artifact

å‚è€ƒæ–‡çŒ®:
- Boosting Latent Diffusion with Perceptual Objectives (2024)
- FreSca: Scaling in Frequency Space (2024)
- Smooth Diffusion: Crafting Smooth Latent Spaces (CVPR 2024)
"""

import os
import argparse
import json
import random
import math
import time
from pathlib import Path
from datetime import timedelta, datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import AdamW
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

# å¼•å…¥ V2 æ¨¡å‹
# ç¡®ä¿æ‚¨çš„ src ç›®å½•åœ¨ python path ä¸­
from src.models.jat_audiosr_v2 import JaT_AudioSR_V2

# ==============================================================================
# ğŸ¯ Loss Functions (MOD3: Charbonnier Loss)
# ==============================================================================

def charbonnier_loss(pred, target, eps=1e-6):
    """
    Charbonnier Loss (é²æ£’ L1 æŸå¤±) - çº¯PyTorchå®ç°

    å…¬å¼: L = mean(sqrt((pred - target)^2 + eps))

    ä¼˜ç‚¹:
    1. âœ… æ¯”MSEæ›´é²æ£’: å¯¹å¼‚å¸¸å€¼less sensitive
    2. âœ… é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸: å³ä½¿è¯¯å·®å¾ˆå¤§ï¼Œæ¢¯åº¦ä¹Ÿæœ‰ç•Œ
    3. âœ… å¹³æ»‘å¯å¾®: epsç¡®ä¿åœ¨0å¤„å¯å¾®
    4. âœ… torch.compile()å…¼å®¹: ä»…ä½¿ç”¨PyTorchåŸç”Ÿæ“ä½œ

    å‚è€ƒ:
    - "Deep Laplacian Pyramid Networks" (CVPR 2017)
    - Charbonnier et al. "Deterministic edge-preserving regularization" (1997)
    - Kornia implementation: https://kornia.readthedocs.io/en/latest/losses.html

    Args:
        pred: [B, C, T] é¢„æµ‹å€¼
        target: [B, C, T] ç›®æ ‡å€¼
        eps: å¹³æ»‘å› å­ (default: 1e-6)

    Returns:
        loss: scalar tensor
    """
    # çº¯PyTorchæ“ä½œï¼Œ100% torch.compile()å…¼å®¹
    diff_squared = (pred - target) ** 2  # å…ƒç´ wiseå¹³æ–¹å·®
    loss = torch.sqrt(diff_squared + eps).mean()  # å¼€æ–¹åå–å‡å€¼
    return loss


# ==============================================================================
# ğŸ¯ Latent Perceptual Loss Modules (2024-2025 SOTA)
# ==============================================================================

class FrequencyDomainLatentLoss(nn.Module):
    """
    é¢‘åŸŸLatentæŸå¤± - æ”¹è¿›ç‰ˆ (Fixé‡‘å±éŸ³å’Œå™ªç‚¹)

    å‚è€ƒ:
    - FreSca: Scaling in Frequency Space (2024)
      https://arxiv.org/html/2504.02154v2
    - Multi-resolution STFT Loss (HiFi-GAN, BigVGAN)

    æ”¹è¿›ç‚¹ï¼š
    1. âœ… å¯¹æ•°å¹…åº¦æŸå¤±ï¼šL1(Log(Mag)) ä»£æ›¿ L1(Mag)
       - é¿å…çº¿æ€§æŸå¤±å¯¼è‡´çš„é«˜é¢‘å™ªç‚¹/ç»¿é›¾
       - æ›´ç¬¦åˆäººè€³çš„å¯¹æ•°æ„ŸçŸ¥ç‰¹æ€§
    2. âœ… Smart Phaseçº¦æŸï¼šä»…åœ¨ä½é¢‘çº¦æŸç›¸ä½
       - é¿å…å…¨é¢‘æ®µç›¸ä½çº¦æŸå¯¼è‡´çš„é‡‘å±éŸ³
       - é«˜é¢‘ç›¸ä½éšæœºæ€§ä¸å½±å“æ„ŸçŸ¥
    3. âœ… ç§»é™¤é«˜é¢‘åŠ æƒï¼šé¿å…è¿‡åº¦å¼ºè°ƒé«˜é¢‘

    ç‰¹æ€§:
    - çº¯PyTorchæ“ä½œ (torch.fft)
    - å®Œå…¨å…¼å®¹torch.compile()
    """
    def __init__(self, low_freq_phase_ratio=0.3):
        super().__init__()
        self.low_freq_phase_ratio = low_freq_phase_ratio  # åªå¯¹ä½30%é¢‘ç‡çº¦æŸç›¸ä½

    def forward(self, pred_latent, target_latent):
        """
        Args:
            pred_latent: [B, C, T] é¢„æµ‹çš„latent
            target_latent: [B, C, T] GT latent
        Returns:
            loss: scalar
        """
        # ğŸ”¥ æ ¸å¿ƒä¿®å¤: å¼ºåˆ¶è½¬ä¸º FP32
        # 1. è§£å†³ cuFFT FP16 å¿…é¡»æ˜¯ 2 çš„å¹‚çš„é™åˆ¶ (1378 ä¸æ˜¯ 2 çš„å¹‚)
        # 2. è§£å†³ FP16 ä¸‹ç›¸ä½è®¡ç®—ç²¾åº¦æä½çš„é—®é¢˜
        pred_latent = pred_latent.float()
        target_latent = target_latent.float()

        # æ²¿æ—¶é—´ç»´åº¦åšFFT (å®ä¿¡å·ç”¨rfft)
        pred_fft = torch.fft.rfft(pred_latent, dim=-1)  # [B, C, F] complex
        target_fft = torch.fft.rfft(target_latent, dim=-1)

        # --- 1. å¯¹æ•°å¹…åº¦æŸå¤± (ä¸»è¦) ---
        pred_mag = torch.abs(pred_fft)
        target_mag = torch.abs(target_fft)

        # è½¬æ¢åˆ°å¯¹æ•°åŸŸ (dB scale)
        # log(mag + eps) é¿å… log(0)
        eps = 1e-7
        pred_log_mag = torch.log(pred_mag + eps)
        target_log_mag = torch.log(target_mag + eps)

        # L1 Loss in log space
        log_mag_loss = F.l1_loss(pred_log_mag, target_log_mag)

        # --- 2. Smart Phaseçº¦æŸ (ä»…ä½é¢‘) ---
        freq_bins = pred_fft.shape[-1]
        low_freq_bin = int(freq_bins * self.low_freq_phase_ratio)

        # åªå¯¹ä½é¢‘éƒ¨åˆ†çº¦æŸç›¸ä½ (Complex L1)
        pred_low = pred_fft[..., :low_freq_bin]
        target_low = target_fft[..., :low_freq_bin]
        low_freq_phase_loss = torch.abs(pred_low - target_low).mean()

        # --- 3. ç»„åˆæŸå¤± ---
        # ä¸»è¦ä¾èµ–å¯¹æ•°å¹…åº¦ï¼Œè¾…ä»¥ä½é¢‘ç›¸ä½
        total_loss = 1.0 * log_mag_loss + 0.1 * low_freq_phase_loss

        return total_loss


class MultiScaleLatentLoss(nn.Module):
    """
    å¤šå°ºåº¦LatentæŸå¤± - åœ¨ä¸åŒæ—¶é—´å°ºåº¦ä¸Šè®¡ç®—æŸå¤±

    å‚è€ƒ: Smooth Diffusion: Crafting Smooth Latent Spaces (CVPR 2024)
    https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_Smooth_Diffusion_CVPR_2024_paper.pdf

    ç‰¹æ€§:
    - å¤šæ—¶é—´å°ºåº¦ (åŸå§‹ + 2x, 4xé™é‡‡æ ·)
    - æ•æ‰ç²—ç²’åº¦åˆ°ç»†ç²’åº¦ç»“æ„
    - æ”¹å–„latent spaceå¹³æ»‘æ€§
    - æå°è®¡ç®—å¼€é”€
    """
    def __init__(self, scales=[1, 2, 4]):
        super().__init__()
        self.scales = scales

        # é¢„åˆ›å»ºé™é‡‡æ ·å±‚ (Average Pooling)
        self.pools = nn.ModuleList([
            nn.AvgPool1d(kernel_size=s, stride=s)
            for s in scales[1:]  # è·³è¿‡scale=1
        ])

    def forward(self, pred_latent, target_latent):
        """
        Args:
            pred_latent: [B, C, T]
            target_latent: [B, C, T]
        Returns:
            loss: scalar
        """
        total_loss = 0.0

        # 1. åŸå§‹å°ºåº¦ (scale=1)
        total_loss += F.l1_loss(pred_latent, target_latent)

        # 2. é™é‡‡æ ·å°ºåº¦
        for pool in self.pools:
            pred_down = pool(pred_latent)
            target_down = pool(target_latent)
            total_loss += F.l1_loss(pred_down, target_down)

        # å¹³å‡ (é¿å…scaleæ•°é‡å½±å“lossæ•°å€¼)
        return total_loss / len(self.scales)


class HybridConsistencyLoss(nn.Module):
    """
    Hybrid Consistency Loss (Pro Max) - MOD2 æ ¸å¿ƒåˆ›æ–°

    æ ¸å¿ƒæ€è·¯ï¼šå¼ºåˆ¶ Downsample(Generated_HR) â‰ˆ Input_LR
    åœ¨ latent space é€šè¿‡é¢‘åŸŸæˆªæ–­æ¨¡æ‹Ÿé™é‡‡æ ·æ•ˆæœ

    ç­–ç•¥ï¼š
    1. Strict Band (0 ~ strict_cutoff): å¤æ•°L1æŸå¤± (å¹…åº¦+ç›¸ä½)
       - ç‰©ç†æ„ä¹‰ï¼šåŸºé¢‘æ³¢å½¢å¿…é¡»å®Œå…¨ä¸€è‡´
    2. Transition Band (strict ~ soft): å¹…åº¦L1æŸå¤± + çº¿æ€§è¡°å‡æƒé‡ (1.0 -> 0.0)
       - ç‰©ç†æ„ä¹‰ï¼šèƒ½é‡å¹³æ»‘è¿‡æ¸¡ï¼Œæ¶ˆé™¤ cutoff å¤„çš„æŒ¯é“ƒ
    3. High Band (soft ~ 1.0): æ— æŸå¤±
       - ç‰©ç†æ„ä¹‰ï¼šå…è®¸æ¨¡å‹è‡ªç”±ç”Ÿæˆé«˜é¢‘ç»†èŠ‚

    ç‰©ç†ä¾æ®ï¼š
    - HR 44.1kHz â†’ é™é‡‡æ · 16kHz â†’ ä¸Šé‡‡æ · 44.1kHz â†’ DAC encode = LR latent
    - 16kHz / 44.1kHz â‰ˆ 0.36
    - é™é‡‡æ ·æˆªæ–­ > 16kHz é¢‘ç‡ï¼Œåœ¨é¢‘åŸŸè¡¨ç°ä¸º 0.36Fs æˆªæ­¢
    - Anti-aliasing filter åœ¨ cutoff é™„è¿‘æœ‰è¿‡æ¸¡å¸¦ï¼ˆä¸æ˜¯ç¡¬æˆªæ–­ï¼‰

    æ”¹è¿›ç‚¹ï¼š
    - âœ… çº¿æ€§è¡°å‡æƒé‡ï¼šé¿å… soft_bin å¤„çš„ç¡¬è·³å˜
    - âœ… æ¨¡æ‹ŸçœŸå® Anti-aliasing Filter çš„å¹³æ»‘è¿‡æ¸¡
    - âœ… é˜²æ­¢æ¨¡å‹åœ¨ cutoff å¤„äº§ç”Ÿé¢‘è°±æ–­å±‚

    é¢„æœŸæ•ˆæœï¼š
    - é˜²æ­¢ç”Ÿæˆä¸ LR ä¸ä¸€è‡´çš„"å¹»è§‰"é«˜é¢‘
    - æ”¹å–„ Mel L1 loss (ä» 4.30 é™åˆ° 3.8-4.0)
    - é™ä½ LSD (ä» 13 é™åˆ° 9-10 dB)

    å‚è€ƒ:
    - Image SR: Bicubic Downsampling Consistency
    - Nyquist-Shannon Sampling Theorem
    - Anti-aliasing Filter Design
    """
    def __init__(self, strict_cutoff=0.30, soft_cutoff=0.36):
        super().__init__()
        self.strict_cutoff = strict_cutoff
        self.soft_cutoff = soft_cutoff

    def forward(self, pred_hr_latent, lr_latent):
        """
        Args:
            pred_hr_latent: [B, C, T] ç”Ÿæˆçš„ HR latent
            lr_latent: [B, C, T] è¾“å…¥çš„ LR latent (å·²é™é‡‡æ ·-ä¸Šé‡‡æ ·)
        Returns:
            loss: scalar
        """
        # ğŸ”¥ å¼ºåˆ¶ FP32 (ä¸ FrequencyDomainLatentLoss ä¿æŒä¸€è‡´)
        # é¿å…åŠç²¾åº¦ FFT çš„æ•°å€¼è¯¯å·®å’Œ cuFFT é™åˆ¶
        pred_hr_latent = pred_hr_latent.float()
        lr_latent = lr_latent.float()

        # FFT åˆ°é¢‘åŸŸ (Real FFT)
        # [B, C, T] -> [B, C, F] (Complex)
        pred_fft = torch.fft.rfft(pred_hr_latent, dim=-1)
        lr_fft = torch.fft.rfft(lr_latent, dim=-1)

        freq_bins = pred_fft.shape[-1]
        strict_bin = int(freq_bins * self.strict_cutoff)
        soft_bin = int(freq_bins * self.soft_cutoff)

        # --- 1. ä¸¥æ ¼ä½é¢‘åŒº (0 - 0.3Fs): Complex L1 Loss ---
        # çº¦æŸï¼šå¹…åº¦ + ç›¸ä½å¿…é¡»å®Œå…¨ä¸€è‡´
        # ç‰©ç†æ„ä¹‰ï¼šåŸºé¢‘æ³¢å½¢ä¸èƒ½å˜
        pred_strict = pred_fft[..., :strict_bin]
        lr_strict = lr_fft[..., :strict_bin]

        # Complex L1: |a - b| (å¤æ•°çš„æ¬§å‡ é‡Œå¾—è·ç¦»)
        strict_loss = torch.abs(pred_strict - lr_strict).mean()

        # --- 2. è¿‡æ¸¡å¸¦ (0.3Fs - 0.36Fs): Magnitude L1 + Linear Decay ---
        # çº¦æŸï¼šåªçº¦æŸèƒ½é‡å¤§å°ï¼Œå…è®¸ç›¸ä½æ¼‚ç§»
        # ç‰©ç†æ„ä¹‰ï¼šæ¶ˆé™¤ Cutoff å¤„çš„æŒ¯é“ƒï¼Œå®ç°èƒ½é‡å¹³æ»‘äº¤æ¥
        if soft_bin > strict_bin:
            pred_trans = torch.abs(pred_fft[..., strict_bin:soft_bin])
            lr_trans = torch.abs(lr_fft[..., strict_bin:soft_bin])

            # ğŸ”¥ æ ¸å¿ƒæ”¹è¿›ï¼šåŠ¨æ€ç”Ÿæˆè¡°å‡æƒé‡ [1.0, 0.9, ..., 0.1, 0.0]
            # æ¨¡æ‹Ÿ Anti-aliasing Filter çš„å¹³æ»‘è¿‡æ¸¡
            band_width = soft_bin - strict_bin
            decay_mask = torch.linspace(1.0, 0.0, steps=band_width, device=pred_fft.device)
            decay_mask = decay_mask.view(1, 1, -1)  # å¹¿æ’­åˆ° [1, 1, band_width]

            # åŠ æƒ L1 Loss
            trans_diff = torch.abs(pred_trans - lr_trans)
            transition_loss = (trans_diff * decay_mask).mean()
        else:
            # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœ soft_bin <= strict_binï¼Œè¿‡æ¸¡å¸¦ä¸ºç©º
            transition_loss = torch.tensor(0.0, device=pred_fft.device)

        # --- 3. é«˜é¢‘åŒº (0.36Fs - 0.5Fs): æ— çº¦æŸ ---
        # å…è®¸æ¨¡å‹è‡ªç”±ç”Ÿæˆé«˜é¢‘ç»†èŠ‚

        # --- 4. ç»„åˆæŸå¤± ---
        # strict å’Œ transition ç­‰æƒé‡ï¼ˆå› ä¸º decay_mask å·²ç»è®© transition å¹³å‡æƒé‡ â‰ˆ 0.5ï¼‰
        total_loss = 1.0 * strict_loss + 1.0 * transition_loss

        return total_loss


class CombinedLatentPerceptualLoss(nn.Module):
    """
    ç»„åˆLatentæ„ŸçŸ¥æŸå¤± - Frequency + Multi-Scale + Consistency (MOD2)

    ç‰¹æ€§:
    - é¢‘åŸŸé«˜é¢‘å¢å¼º (é’ˆå¯¹"æ¯›åˆºæ„Ÿ")
    - å¤šå°ºåº¦æ—¶é—´ç»“æ„
    - Hybrid Consistency (é˜²æ­¢å¹»è§‰é«˜é¢‘ï¼Œæ”¹å–„ Mel L1)
    """
    def __init__(self, freq_weight=0.5, ms_weight=0.5, consistency_weight=0.3, low_freq_phase_ratio=0.3):
        super().__init__()
        self.freq_loss = FrequencyDomainLatentLoss(low_freq_phase_ratio=low_freq_phase_ratio)
        self.ms_loss = MultiScaleLatentLoss(scales=[1, 2, 4])
        self.consistency_loss = HybridConsistencyLoss(strict_cutoff=0.30, soft_cutoff=0.36)

        self.freq_weight = freq_weight
        self.ms_weight = ms_weight
        self.consistency_weight = consistency_weight

    def forward(self, pred_latent, target_latent, lr_latent):
        """
        Args:
            pred_latent: [B, C, T] é¢„æµ‹çš„ HR latent
            target_latent: [B, C, T] GT HR latent
            lr_latent: [B, C, T] è¾“å…¥çš„ LR latent (MOD2 æ–°å¢)
        Returns:
            loss: scalar
            loss_dict: dict with individual losses for logging
        """
        freq_loss = self.freq_loss(pred_latent, target_latent)
        ms_loss = self.ms_loss(pred_latent, target_latent)

        # ğŸ”¥ MOD2 æ ¸å¿ƒï¼šConsistency Loss
        consistency_loss = self.consistency_loss(pred_latent, lr_latent)

        total = (self.freq_weight * freq_loss +
                 self.ms_weight * ms_loss +
                 self.consistency_weight * consistency_loss)

        # è¿”å›ç»„åˆæŸå¤± + å•ç‹¬æŸå¤± (ç”¨äºç›‘æ§)
        loss_dict = {
            'freq_loss': freq_loss.item(),
            'ms_loss': ms_loss.item(),
            'consistency_loss': consistency_loss.item(),  # MOD2 æ–°å¢
            'total_latent_loss': total.item()
        }

        return total, loss_dict

# ==============================================================================
# âš™ï¸ é…ç½®ä¸­å¿ƒ (Configuration)
# ==============================================================================

class TrainConfig:
    # --- ç¡¬ä»¶ä¸å¹¶è¡Œ ---
    seed = 42
    num_workers = 16         
    
    # --- æ•°æ® ---
    data_dir = "data_processed_v13_final"  # æŒ‡å‘ prepare_dataset_v5 ç”Ÿæˆçš„ç›®å½•
    stats_file = "global_stats_separated.json"  # åˆ†ç¦»çš„ HR/LR ç»Ÿè®¡é‡
    target_duration = 16.0                 # åˆ‡ç‰‡é•¿åº¦ (ç§’)
    dac_sample_rate = 44100
    dac_hop_length = 512    # DAC é»˜è®¤å‹ç¼©ç‡
    
    # è®¡ç®— Latent å¸§æ•°: 16 * 44100 / 512 â‰ˆ 1378
    target_frames = int(target_duration * dac_sample_rate / dac_hop_length)

    # --- æ¨¡å‹ (V2 Specs) ---
    model_params = {
        'input_channels': 1024,
        'cond_channels': 1024,
        'patch_len': 4,
        'hidden_size': 1280,
        'depth': 28,
        'num_q_heads': 20,
        'num_kv_heads': 4,
        'bottleneck_dim': 512,
        'mlp_ratio': 4.0,
        'dropout': 0.1,         # ğŸ”¥ ä¿®å¤è¿‡æ‹Ÿåˆï¼šå¼€å¯Dropout
        'drop_path_rate': 0.05  # ğŸ”¥ Stochastic Depthï¼šçº¿æ€§é€’å¢ 0â†’0.05
    }

    # --- æ­£åˆ™åŒ–å¢å¼º (V2) ---
    condition_noise_ratio = 0.05  # Conditional Noise: 5%ç›¸å¯¹å™ªå£°
    use_adaptive_noise = True     # ä½¿ç”¨è‡ªé€‚åº”å™ªå£°ï¼ˆåŸºäºbatch stdï¼‰

    # ==============================================================================
    # ğŸ”¥ MOD3: ç»Ÿä¸€Lossé…ç½®åŒº (Centralized Loss Configuration)
    # ==============================================================================
    #
    # æ‰€æœ‰lossæƒé‡é›†ä¸­åœ¨è¿™é‡Œï¼Œæ–¹ä¾¿è°ƒå‚å’Œå®éªŒ
    #

    # --- ä¸»Lossç±»å‹ (MOD3 æ–°å¢) ---
    use_charbonnier_loss = True        # ğŸ”¥ ä½¿ç”¨Charbonnier Lossæ›¿ä»£MSE (æ›´é²æ£’)
    charbonnier_eps = 1e-6             # Charbonnier Losså¹³æ»‘å› å­

    # --- Latent Perceptual Loss å¼€å…³ ---
    use_latent_perceptual_loss = True  # å¯ç”¨Latentæ„ŸçŸ¥æŸå¤±

    # --- Loss æƒé‡é…ç½® (å¯ç‹¬ç«‹è°ƒèŠ‚) ---
    # ä¸»é‡å»ºæŸå¤±æƒé‡ (Charbonnieræˆ–MSEï¼Œæ ¹æ®use_charbonnier_losså†³å®š)
    reconstruction_weight = 1.0        # ä¸»æŸå¤±æƒé‡ (é€šå¸¸å›ºå®šä¸º1.0)

    # Latent Perceptual Lossç»„ä»¶æƒé‡
    latent_loss_weight = 0.3           # Latentæ„ŸçŸ¥æŸå¤±æ€»æƒé‡ (ç›¸å¯¹reconstruction)
    freq_loss_weight = 0.5             # é¢‘åŸŸæŸå¤±å­æƒé‡ (åœ¨latent losså†…éƒ¨)
    ms_loss_weight = 0.5               # å¤šå°ºåº¦æŸå¤±å­æƒé‡ (åœ¨latent losså†…éƒ¨)
    consistency_weight = 0.1           # ğŸ”¥ Hybrid Consistency Losså­æƒé‡ (æ•°å€¼å°ºåº¦å¤§ï¼Œæƒé‡å°)

    # --- Frequency Domain Loss å‚æ•° ---
    low_freq_phase_ratio = 0.3         # ä»…å¯¹ä½30%é¢‘ç‡çº¦æŸç›¸ä½ (é¿å…é‡‘å±éŸ³)

    # --- Consistency Loss å‚æ•° ---
    strict_cutoff = 0.30               # ä¸¥æ ¼çº¦æŸé¢‘ç‡æˆªæ­¢ç‚¹ (0-0.3Fs)
    soft_cutoff = 0.36                 # è½¯è¿‡æ¸¡æˆªæ­¢ç‚¹ (16kHz/44.1kHz)

    # ==============================================================================
    # End of Loss Configuration
    # ==============================================================================

    # --- è®­ç»ƒè¶…å‚ ---
    batch_size = 28         # å•å¡ Batch (æ€» Batch = 56)
    lr = 5e-5               # ç¨³å¥å­¦ä¹ ç‡
    weight_decay = 0.1
    warmup_steps = 1000     # é¢„çƒ­æ­¥æ•°
    num_epochs = 300       # è·‘æ»¡ä¸ºæ­¢
    grad_clip = 1.0

    # --- ä¿å­˜ä¸æ—¥å¿— ---
    save_dir_base = "checkpoints/v3mod3_full_run"  # ğŸ”¥ MOD3åŸºç¡€ç›®å½•
    log_dir_base = "runs/v3mod3_full_run"          # ğŸ”¥ MOD3åŸºç¡€ç›®å½•
    save_interval_steps = 1000  # æ¯ 1000 æ­¥ä¿å­˜ä¸€ä¸ª interval checkpoint

# ==============================================================================
# ğŸ› ï¸ å·¥å…·å‡½æ•°
# ==============================================================================

def get_timestamp_folder():
    """ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å¤¹åï¼ˆæ ¼å¼ï¼šMMDDHHMMï¼‰"""
    now = datetime.now()
    return now.strftime("%m%d%H%M")

def find_latest_checkpoint_dir(base_dir):
    """
    åœ¨base_dirä¸‹æŸ¥æ‰¾æœ€æ–°çš„æ—¶é—´æˆ³å­æ–‡ä»¶å¤¹
    è¿”å›ï¼š(æœ€æ–°æ–‡ä»¶å¤¹è·¯å¾„, checkpointè·¯å¾„) æˆ– (None, None)
    """
    if not os.path.exists(base_dir):
        return None, None

    # æŸ¥æ‰¾æ‰€æœ‰æ—¶é—´æˆ³æ ¼å¼çš„å­æ–‡ä»¶å¤¹ï¼ˆ8ä½æ•°å­—ï¼‰
    subdirs = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path) and item.isdigit() and len(item) == 8:
            subdirs.append((item, item_path))

    if not subdirs:
        return None, None

    # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œå–æœ€æ–°çš„
    subdirs.sort(reverse=True)
    latest_folder = subdirs[0][1]

    # æ£€æŸ¥æ˜¯å¦æœ‰last.pt
    checkpoint_path = os.path.join(latest_folder, "last.pt")
    if os.path.exists(checkpoint_path):
        return latest_folder, checkpoint_path

    return latest_folder, None

def setup_ddp():
    """åˆå§‹åŒ– DDP ç¯å¢ƒ"""
    if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        local_rank = int(os.environ["LOCAL_RANK"])
    else:
        # é»˜è®¤å•æœºå¤šå¡
        rank = 0
        world_size = torch.cuda.device_count()
        local_rank = 0
        os.environ["MASTER_ADDR"] = "localhost"
        os.environ["MASTER_PORT"] = "12355"
        os.environ["RANK"] = "0"
        os.environ["WORLD_SIZE"] = str(world_size)

    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend="nccl", init_method="env://", world_size=world_size, rank=rank)
    return rank, local_rank, world_size

def cleanup_ddp():
    dist.destroy_process_group()

def u_shaped_timestep_sampling(batch_size, device, alpha=0.5):
    """V2 æ ¸å¿ƒ: U-shaped Timestep Sampling"""
    u = torch.rand(batch_size, device=device)
    t = torch.where(
        u < 0.5,
        (2 * u) ** alpha / 2,
        1 - ((2 * (1 - u)) ** alpha) / 2
    )
    return t

def load_global_stats(stats_path, device):
    """åŠ è½½å¹¶å¹¿æ’­å…¨å±€ç»Ÿè®¡é‡"""
    with open(stats_path, 'r') as f:
        data = json.load(f)
    
    # è½¬æ¢ä¸º Tensor (1, C, 1) ç”¨äºå¹¿æ’­
    hr_mean = torch.tensor(data['hr_mean']).view(1, -1, 1).to(device)
    hr_std = torch.tensor(data['hr_std']).view(1, -1, 1).to(device)
    lr_mean = torch.tensor(data['lr_mean']).view(1, -1, 1).to(device)
    lr_std = torch.tensor(data['lr_std']).view(1, -1, 1).to(device)
    
    return hr_mean, hr_std, lr_mean, lr_std

# ==============================================================================
# ğŸ’¾ æ•°æ®é›† (Dataset)
# ==============================================================================

class LatentDataset(Dataset):
    """
    è®­ç»ƒæ•°æ®é›†ï¼šä½¿ç”¨LRU cacheç¼“å­˜mmapæ–‡ä»¶å¼•ç”¨
    ç”±äºå·²ä¿®å¤Float Bombï¼Œç°åœ¨æ¯ä¸ªæ ·æœ¬åªæœ‰~11MBï¼Œç¼“å­˜å®Œå…¨å¯è¡Œ
    """
    def __init__(self, data_dir, split, target_frames, samples_per_epoch_multiplier=6, cache_size=128):
        super().__init__()
        self.target_frames = target_frames
        self.multiplier = samples_per_epoch_multiplier

        # æ‰«ææ‰€æœ‰ .pt æ–‡ä»¶
        search_path = Path(data_dir) / split
        self.files = sorted(list(search_path.glob("*.pt")))

        if len(self.files) == 0:
            raise ValueError(f"No .pt files found in {search_path}")

        print(f"[{split}] Found {len(self.files)} files (x{self.multiplier} samples/epoch = {len(self.files) * self.multiplier} total samples)")
        print(f"[{split}] Using LRU cache (size={cache_size}, ~{cache_size * 11 / 1024:.1f}GB per worker)")

        # LRUç¼“å­˜mmapçš„æ–‡ä»¶å¼•ç”¨ï¼ˆä¸ç¼“å­˜è½¬æ¢åçš„æ•°æ®ï¼‰
        from functools import lru_cache
        @lru_cache(maxsize=cache_size)
        def load_file_mmap(path_str):
            """ç¼“å­˜mmapåŠ è½½çš„æ–‡ä»¶ï¼ˆå†…å­˜å ç”¨æå°ï¼Œåªæ˜¯å¼•ç”¨ï¼‰"""
            data = torch.load(path_str, map_location='cpu', mmap=True, weights_only=False)
            return data['hr_latent'], data['lr_latent']  # ä¿æŒFP16

        self._load_file = load_file_mmap

    def __len__(self):
        return len(self.files) * self.multiplier

    def __getitem__(self, idx):
        # æ˜ å°„åˆ°å®é™…æ–‡ä»¶ç´¢å¼•
        file_idx = idx % len(self.files)
        path = self.files[file_idx]

        # ä»cacheåŠ è½½mmapå¼•ç”¨ï¼ˆFP16ï¼Œå‡ ä¹ä¸å å†…å­˜ï¼‰
        hr, lr = self._load_file(str(path))

        length = hr.shape[-1]

        # ğŸ”¥ å…³é”®ä¿®å¤2: å…ˆåˆ‡ç‰‡ï¼ˆFP16ï¼‰ï¼Œå†è½¬æ¢ï¼ˆFP32ï¼‰
        if length < self.target_frames:
            # A. çŸ­éŸ³é¢‘: å¾ªç¯æ‹¼æ¥
            repeats = math.ceil(self.target_frames / length)
            hr = hr.repeat(1, repeats)[..., :self.target_frames]
            lr = lr.repeat(1, repeats)[..., :self.target_frames]
        else:
            # B. é•¿éŸ³é¢‘: éšæœºåˆ‡ç‰‡
            start = random.randint(0, length - self.target_frames)
            hr = hr[..., start : start + self.target_frames]
            lr = lr[..., start : start + self.target_frames]

        # æœ€åæ‰è½¬FP32ï¼ˆåªè½¬æ¢16sç‰‡æ®µï¼Œå†…å­˜å ç”¨å°ï¼‰
        hr = hr.float()
        lr = lr.float()

        return hr, lr

class ValidationDataset(Dataset):
    """éªŒè¯é›†ä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·ï¼Œä¿è¯å¯é‡å¤æ€§ï¼Œå¸¦LRU cache"""
    def __init__(self, data_dir, split, target_frames, samples_per_epoch_multiplier=6, cache_size=128):
        super().__init__()
        self.target_frames = target_frames
        self.multiplier = samples_per_epoch_multiplier
        search_path = Path(data_dir) / split
        self.files = sorted(list(search_path.glob("*.pt")))

        print(f"[{split}] Found {len(self.files)} files (x{self.multiplier} samples/epoch = {len(self.files) * self.multiplier} total samples)")
        print(f"[{split}] Using LRU cache (size={cache_size}, ~{cache_size * 11 / 1024:.1f}GB per worker)")

        # LRUç¼“å­˜mmapå¼•ç”¨
        from functools import lru_cache
        @lru_cache(maxsize=cache_size)
        def load_file_mmap(path_str):
            data = torch.load(path_str, map_location='cpu', mmap=True, weights_only=False)
            return data['hr_latent'], data['lr_latent']

        self._load_file = load_file_mmap

    def __len__(self):
        return len(self.files) * self.multiplier

    def __getitem__(self, idx):
        # æ˜ å°„åˆ°å®é™…æ–‡ä»¶å’Œé‡‡æ ·ä½ç½®
        file_idx = idx % len(self.files)
        sample_idx = idx // len(self.files)  # 0, 1, 2, ..., multiplier-1
        path = self.files[file_idx]

        # ä»cacheåŠ è½½mmapå¼•ç”¨
        hr, lr = self._load_file(str(path))

        length = hr.shape[-1]

        # ğŸ”¥ å…³é”®ä¿®å¤2: å…ˆåˆ‡ç‰‡ï¼ˆFP16ï¼‰ï¼Œå†è½¬æ¢ï¼ˆFP32ï¼‰
        if length < self.target_frames:
            # çŸ­éŸ³é¢‘: å¾ªç¯æ‹¼æ¥
            repeats = math.ceil(self.target_frames / length)
            hr = hr.repeat(1, repeats)[..., :self.target_frames]
            lr = lr.repeat(1, repeats)[..., :self.target_frames]
        else:
            # é•¿éŸ³é¢‘: ç¡®å®šæ€§é‡‡æ ·ï¼ˆæ ¹æ®sample_idxå‡åŒ€åˆ†å¸ƒï¼‰
            # å°†éŸ³é¢‘åˆ†æˆ multiplier ä¸ªåŒºé—´ï¼Œæ¯æ¬¡ä»å¯¹åº”åŒºé—´é‡‡æ ·
            if self.multiplier == 1:
                # åªé‡‡æ ·1æ¬¡ï¼šcenter crop
                start = (length - self.target_frames) // 2
            else:
                # å¤šæ¬¡é‡‡æ ·ï¼šå‡åŒ€åˆ†å¸ƒ
                segment_length = max(length - self.target_frames, 1)
                start = int(segment_length * sample_idx / (self.multiplier - 1))
                start = min(start, length - self.target_frames)

            hr = hr[..., start : start + self.target_frames]
            lr = lr[..., start : start + self.target_frames]

        # æœ€åæ‰è½¬FP32ï¼ˆåªè½¬æ¢16sç‰‡æ®µï¼‰
        hr = hr.float()
        lr = lr.float()

        return hr, lr

# ==============================================================================
# ğŸš€ è®­ç»ƒä¸»æµç¨‹
# ==============================================================================

def main():
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='JaT V2 Training with DDP')
    parser.add_argument('--resume', type=str, nargs='?', const='auto', default=None,
                        help='Resume training. Use --resume for auto (latest), or --resume <path> for specific checkpoint')
    args = parser.parse_args()

    # 2. DDP åˆå§‹åŒ–
    rank, local_rank, world_size = setup_ddp()
    is_master = (rank == 0)

    cfg = TrainConfig()

    # 3. æ™ºèƒ½æ—¶é—´æˆ³ç›®å½•è®¾ç½®
    resume_path = None
    timestamp_folder = None
    save_dir = ""  # åˆå§‹åŒ–ï¼Œé¿å…ç±»å‹æ£€æŸ¥è­¦å‘Š
    log_dir = ""   # åˆå§‹åŒ–ï¼Œé¿å…ç±»å‹æ£€æŸ¥è­¦å‘Š

    if args.resume:
        if args.resume == 'auto':
            # è‡ªåŠ¨ä»æœ€æ–°æ—¶é—´æˆ³æ–‡ä»¶å¤¹æ¢å¤
            latest_dir, latest_ckpt = find_latest_checkpoint_dir(cfg.save_dir_base)
            if latest_ckpt:
                resume_path = latest_ckpt
                save_dir = latest_dir
                log_dir = latest_dir.replace("checkpoints", "runs")
                if is_master:
                    timestamp_folder = os.path.basename(latest_dir)
                    print(f"ğŸ”„ Auto resume from latest: {timestamp_folder}")
                    print(f"   Checkpoint: {resume_path}")
            else:
                if is_master:
                    print("âš ï¸  No checkpoint found, starting new training")
                timestamp_folder = get_timestamp_folder()
                save_dir = os.path.join(cfg.save_dir_base, timestamp_folder)
                log_dir = os.path.join(cfg.log_dir_base, timestamp_folder)
                if is_master:
                    print(f"ğŸ“ New training session: {timestamp_folder}")
        else:
            # æ‰‹åŠ¨æŒ‡å®šcheckpointè·¯å¾„
            resume_path = args.resume
            if not os.path.exists(resume_path):
                raise FileNotFoundError(f"Checkpoint not found: {resume_path}")
            # ä»checkpointè·¯å¾„æå–ç›®å½•
            save_dir = str(Path(resume_path).parent)
            log_dir = save_dir.replace("checkpoints", "runs")
            if is_master:
                timestamp_folder = os.path.basename(save_dir)
                print(f"ğŸ“‚ Manual resume from: {timestamp_folder}")
                print(f"   Checkpoint: {resume_path}")
    else:
        # æ–°è®­ç»ƒï¼Œåˆ›å»ºæ—¶é—´æˆ³æ–‡ä»¶å¤¹
        timestamp_folder = get_timestamp_folder()
        save_dir = os.path.join(cfg.save_dir_base, timestamp_folder)
        log_dir = os.path.join(cfg.log_dir_base, timestamp_folder)
        if is_master:
            print(f"ğŸ“ New training session: {timestamp_folder}")

    # åˆ›å»ºç›®å½•
    if is_master:
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)
        writer = SummaryWriter(log_dir=log_dir)
        print("="*60)
        print(f"ğŸš€ JaT V2 Training Started")
        print(f"   Save Dir: {save_dir}")
        print(f"   Log Dir: {log_dir}")
        print(f"   GPUs: {world_size} | Batch/GPU: {cfg.batch_size} | Total Batch: {cfg.batch_size * world_size}")
        print(f"   Target Duration: {cfg.target_duration}s ({cfg.target_frames} frames)")
        print(f"   Resume Mode: {'ON' if resume_path else 'OFF'}")
        print("="*60)
    
    # 3. å‡†å¤‡æ•°æ®
    train_dataset = LatentDataset(cfg.data_dir, "train", cfg.target_frames)
    val_dataset = ValidationDataset(cfg.data_dir, "val", cfg.target_frames)
    
    train_sampler = DistributedSampler(train_dataset, shuffle=True)
    val_sampler = DistributedSampler(val_dataset, shuffle=False) # éªŒè¯é›†ä¸éœ€è¦ shuffle
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=cfg.batch_size,
        sampler=train_sampler,
        num_workers=cfg.num_workers,
        pin_memory=True, # åŠ é€Ÿ Host -> Device
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=cfg.batch_size,
        sampler=val_sampler,
        num_workers=cfg.num_workers,
        pin_memory=True,
        drop_last=True
    )
    
    # 4. åŠ è½½ç»Ÿè®¡é‡ (æ¯ä¸ªè¿›ç¨‹éƒ½åŠ è½½)
    stats_path = os.path.join(cfg.data_dir, cfg.stats_file)
    hr_mean, hr_std, lr_mean, lr_std = load_global_stats(stats_path, f"cuda:{local_rank}")
    
    # 5. æ¨¡å‹åˆå§‹åŒ–
    model = JaT_AudioSR_V2(**cfg.model_params).to(local_rank)
    
    # 6. ä¼˜åŒ–å™¨ & è°ƒåº¦å™¨
    optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    # æ‰‹åŠ¨å®ç° Cosine with Warmupï¼Œæ–¹ä¾¿çŠ¶æ€ä¿å­˜
    def get_lr(step, total_steps, warmup_steps, base_lr):
        if step < warmup_steps:
            return base_lr * (step / max(1, warmup_steps))
        else:
            progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)
            return base_lr * 0.5 * (1.0 + math.cos(math.pi * progress))

    # 6.5 Latent Perceptual Loss (MOD2 æ–°å¢)
    latent_loss_fn = None
    if cfg.use_latent_perceptual_loss:
        latent_loss_fn = CombinedLatentPerceptualLoss(
            freq_weight=cfg.freq_loss_weight,
            ms_weight=cfg.ms_loss_weight,
            consistency_weight=cfg.consistency_weight,  # MOD2 æ–°å¢
            low_freq_phase_ratio=cfg.low_freq_phase_ratio  # MOD2: Fixé‡‘å±éŸ³
        ).to(local_rank)

        # è®¾ä¸ºè¯„ä¼°æ¨¡å¼ (æ— éœ€è®­ç»ƒå‚æ•°ï¼Œä»…ç”¨äºæ¢¯åº¦ä¼ é€’)
        latent_loss_fn.eval()

        if is_master:
            print("=" * 60)
            print("ğŸ”¥ MOD3: Charbonnier Loss + Unified Config")
            print("=" * 60)
            loss_type = "Charbonnier" if cfg.use_charbonnier_loss else "MSE"
            print(f"  Main Reconstruction Loss: {loss_type}")
            if cfg.use_charbonnier_loss:
                print(f"  Charbonnier Epsilon: {cfg.charbonnier_eps}")
            print(f"  Reconstruction Weight: {cfg.reconstruction_weight}")
            print("")
            print("ğŸ¯ Latent Perceptual Loss Enabled (MOD2)")
            print(f"  Latent Loss Weight: {cfg.latent_loss_weight}")
            print(f"  Frequency Loss Weight: {cfg.freq_loss_weight}")
            print(f"  Multi-Scale Loss Weight: {cfg.ms_loss_weight}")
            print(f"  ğŸ”¥ Consistency Loss Weight: {cfg.consistency_weight}")  # MOD2
            print(f"  Low-Freq Phase Ratio: {cfg.low_freq_phase_ratio} (Fixé‡‘å±éŸ³)")
            print(f"  Strict Cutoff: {cfg.strict_cutoff}Fs, Soft Cutoff: {cfg.soft_cutoff}Fs")
            print("=" * 60)

    # 7. æ··åˆç²¾åº¦ Scaler
    scaler = torch.amp.GradScaler('cuda')
    
    # 8. æ¢å¤è®­ç»ƒé€»è¾‘ (Perfect Resume)
    start_epoch = 0
    global_step = 0
    best_val_loss = float('inf')

    if resume_path and os.path.exists(resume_path):
        if is_master:
            print(f"â™»ï¸  Resuming from {resume_path}...")

        # map_location ç¡®ä¿åŠ è½½åˆ°å½“å‰ GPU
        # weights_only=False: PyTorch 2.6+éœ€è¦ï¼Œå› ä¸ºcheckpointåŒ…å«RNGçŠ¶æ€ç­‰
        checkpoint = torch.load(resume_path, map_location=f"cuda:{local_rank}", weights_only=False)

        # å¤„ç† torch.compile() çš„ _orig_mod. å‰ç¼€ï¼ˆå…¼å®¹å·²ç¼–è¯‘çš„æ£€æŸ¥ç‚¹ï¼‰
        state_dict = checkpoint['model_state_dict']
        if any(k.startswith('_orig_mod.') for k in state_dict.keys()):
            state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}
            if is_master:
                print("ğŸ”§ Removed '_orig_mod.' prefix from compiled checkpoint")

        model.load_state_dict(state_dict)
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        start_epoch = checkpoint['epoch'] + 1
        global_step = checkpoint['global_step']
        best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        
        # æ¢å¤ RNG çŠ¶æ€ (Python, Numpy, PyTorch, CUDA)
        if 'rng_state' in checkpoint:
            try:
                random.setstate(checkpoint['rng_state']['python'])
                np.random.set_state(checkpoint['rng_state']['numpy'])

                # PyTorch RNG çŠ¶æ€éœ€è¦æ˜¯ ByteTensor (dtype=torch.uint8)
                torch_rng = checkpoint['rng_state']['torch']
                if torch_rng.dtype != torch.uint8:
                    torch_rng = torch_rng.to(torch.uint8)
                torch.set_rng_state(torch_rng.cpu())

                # CUDA RNG çŠ¶æ€æ¢å¤ï¼ˆå…¼å®¹æ–°æ—§æ ¼å¼ï¼‰
                if 'cuda_all' in checkpoint['rng_state']:
                    # æ–°æ ¼å¼ï¼šå¤šGPUçŠ¶æ€
                    cuda_states = checkpoint['rng_state']['cuda_all']
                    # ç¡®ä¿æ¯ä¸ªçŠ¶æ€éƒ½æ˜¯æ­£ç¡®ç±»å‹
                    cuda_states = [s.to(torch.uint8).cpu() if s.dtype != torch.uint8 else s.cpu()
                                   for s in cuda_states]
                    torch.cuda.set_rng_state_all(cuda_states)
                elif 'cuda' in checkpoint['rng_state']:
                    # æ—§æ ¼å¼ï¼šå•GPUçŠ¶æ€ï¼ˆå‘åå…¼å®¹ï¼‰
                    cuda_state = checkpoint['rng_state']['cuda']
                    if cuda_state.dtype != torch.uint8:
                        cuda_state = cuda_state.to(torch.uint8)
                    torch.cuda.set_rng_state(cuda_state.cpu())

                if is_master:
                    print("ğŸ² RNG states restored")
            except Exception as e:
                if is_master:
                    print(f"âš ï¸  Warning: Could not restore RNG states: {e}")
                    print("   Training will continue with random initialization")
            
        if is_master:
            print(f"âœ… Resumed at Epoch {start_epoch}, Step {global_step}")

    # 9. torch.compile ä¼˜åŒ–ï¼ˆä¿å®ˆæ¨¡å¼ï¼‰
    if is_master:
        print("ğŸ”¥ Compiling model with torch.compile (mode='default')...")

    model = torch.compile(model, mode='default', backend='inductor')

    if is_master:
        print("âœ… Model compiled successfully")

    # 10. åŒ…è£… DDP
    model = DDP(model, device_ids=[local_rank], find_unused_parameters=False) # JaTæ— æœªä½¿ç”¨å‚æ•°ï¼ŒFalseæ›´å¿«

    # ==========================================================================
    # ğŸ‹ï¸ è®­ç»ƒå¾ªç¯
    # ==========================================================================
    
    total_steps_approx = len(train_loader) * cfg.num_epochs
    
    for epoch in range(start_epoch, cfg.num_epochs):
        model.train()
        train_sampler.set_epoch(epoch) # DDP å¿…é¡»
        
        start_time = time.time()
        epoch_loss = 0.0

        # åˆ›å»º tqdm è¿›åº¦æ¡ï¼ˆåªåœ¨ GPU0 æ˜¾ç¤ºï¼‰
        pbar = tqdm(total=len(train_loader), disable=(not is_master),
                    desc=f"Epoch {epoch}/{cfg.num_epochs}",
                    leave=False, dynamic_ncols=True)

        for hr, lr in train_loader:
            # LR Scheduler Step
            current_lr = get_lr(global_step, total_steps_approx, cfg.warmup_steps, cfg.lr)
            for param_group in optimizer.param_groups:
                param_group['lr'] = current_lr
                
            # æ•°æ®æ¬è¿
            hr = hr.to(local_rank, non_blocking=True)
            lr = lr.to(local_rank, non_blocking=True)
            
            optimizer.zero_grad(set_to_none=True)

            with torch.amp.autocast('cuda'):
                # A. å½’ä¸€åŒ– (Normalize)
                hr_norm = (hr - hr_mean) / hr_std
                lr_norm = (lr - lr_mean) / lr_std

                # A2. Conditional Noise Augmentation (é˜²æ­¢è¿‡åº¦ä¾èµ–LRè¾“å…¥)
                # ğŸ”¥ MOD2: ä¿å­˜åŸå§‹ LR (ç”¨äº Consistency Loss)
                lr_norm_original = lr_norm.clone()

                if cfg.condition_noise_ratio > 0:
                    if cfg.use_adaptive_noise:
                        # è‡ªé€‚åº”æ–¹æ¡ˆï¼šåŸºäºbatchå®é™…æ ‡å‡†å·®
                        with torch.no_grad():
                            lr_batch_std = lr_norm.std().clamp(0.5, 2.0)  # ä¿æŠ¤å¼‚å¸¸å€¼
                    else:
                        # ç®€åŒ–æ–¹æ¡ˆï¼šå‡è®¾å½’ä¸€åŒ–å®Œç¾ï¼Œstdâ‰ˆ1
                        lr_batch_std = 1.0

                    # ç”Ÿæˆç›¸å¯¹å™ªå£°å¹¶æ·»åŠ åˆ°LRæ¡ä»¶
                    cond_noise = torch.randn_like(lr_norm) * (cfg.condition_noise_ratio * lr_batch_std)
                    lr_norm = lr_norm + cond_noise

                # B. é‡‡æ · Timesteps (U-shaped)
                B = hr.shape[0]
                t = u_shaped_timestep_sampling(B, hr.device) # [B]

                # C. åŠ å™ª (Flow Matching: z_t = t*x + (1-t)*noise)
                noise = torch.randn_like(hr_norm)
                t_view = t.view(-1, 1, 1)
                z_t = t_view * hr_norm + (1 - t_view) * noise

                # D. æ¨¡å‹é¢„æµ‹ (Pred x0)
                pred_x0 = model(z_t, t, lr_norm)

                # E. ä¸»é‡å»ºæŸå¤± (MOD3: Charbonnier or MSE)
                if cfg.use_charbonnier_loss:
                    reconstruction_loss = charbonnier_loss(pred_x0, hr_norm, eps=cfg.charbonnier_eps)
                else:
                    reconstruction_loss = F.mse_loss(pred_x0, hr_norm)

                # E2. Latent Perceptual Loss (MOD2)
                if cfg.use_latent_perceptual_loss and latent_loss_fn is not None:
                    # ğŸ”¥ MOD2: ä¼ å…¥åŸå§‹ LR (ç”¨äº Consistency Loss)
                    latent_perc_loss, latent_loss_dict = latent_loss_fn(pred_x0, hr_norm, lr_norm_original)
                    # ç»„åˆæŸå¤± (MOD3: ä½¿ç”¨ç»Ÿä¸€æƒé‡é…ç½®)
                    loss = (cfg.reconstruction_weight * reconstruction_loss +
                           cfg.latent_loss_weight * latent_perc_loss)
                else:
                    loss = cfg.reconstruction_weight * reconstruction_loss
                    latent_loss_dict = {}

                # ğŸ“Š è®¡ç®—è®­ç»ƒæŒ‡æ ‡ (Phase 1 + MOD1)
                with torch.no_grad():
                    # é¢„æµ‹è´¨é‡ç»Ÿè®¡
                    pred_mean = pred_x0.mean().item()
                    pred_std = pred_x0.std().item()

                    # ä¿¡å™ªæ¯” (SNR in dB)
                    signal_power = (hr_norm ** 2).mean()
                    noise_power = ((pred_x0 - hr_norm) ** 2).mean()
                    snr_db = 10 * torch.log10(signal_power / (noise_power + 1e-8))
                    snr_db = snr_db.item()

                    # Conditional Noiseç›‘æ§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if cfg.condition_noise_ratio > 0:
                        # è®°å½•å®é™…æ·»åŠ çš„å™ªå£°æ ‡å‡†å·®ï¼ˆç”¨äºéªŒè¯ï¼‰
                        if cfg.use_adaptive_noise:
                            actual_noise_std = (cfg.condition_noise_ratio * lr_batch_std).item()
                        else:
                            actual_noise_std = cfg.condition_noise_ratio
            
            # åå‘ä¼ æ’­ & ä¼˜åŒ–
            scaler.scale(loss).backward()

            # ğŸ“Š è®¡ç®—æ¢¯åº¦èŒƒæ•° (Phase 1)
            scaler.unscale_(optimizer)
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)
            grad_norm = grad_norm.item()
            
            scaler.step(optimizer)
            scaler.update()
            
            # è®°å½•
            loss_val = loss.item()
            epoch_loss += loss_val

            if is_master:
                # æ›´æ–° tqdm è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f'{loss_val:.5f}',
                    'lr': f'{current_lr:.2e}',
                    'step': global_step
                })
                pbar.update(1)

                # TensorBoard æ—¥å¿— (æ¯10æ­¥ä¸€æ¬¡)
                if global_step % 10 == 0:
                    writer.add_scalar("Train/Loss", loss_val, global_step)
                    writer.add_scalar("Train/LR", current_lr, global_step)
                    # Phase 1 æŒ‡æ ‡
                    writer.add_scalar("Train/GradNorm", grad_norm, global_step)
                    writer.add_scalar("Train/SNR_dB", snr_db, global_step)
                    writer.add_scalar("Train/PredictionMean", pred_mean, global_step)
                    writer.add_scalar("Train/PredictionStd", pred_std, global_step)
                    # V2 æ­£åˆ™åŒ–æŒ‡æ ‡
                    if cfg.condition_noise_ratio > 0:
                        writer.add_scalar("Train/CondNoiseStd", actual_noise_std, global_step)
                    # MOD3 ä¸»æŸå¤± + MOD2 Latent Perceptual LossæŒ‡æ ‡
                    if cfg.use_latent_perceptual_loss and latent_loss_dict:
                        # åŠ¨æ€æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„lossç±»å‹
                        loss_type = "Charbonnier" if cfg.use_charbonnier_loss else "MSE"
                        writer.add_scalar(f"Train/{loss_type}_Loss", reconstruction_loss.item(), global_step)
                        writer.add_scalar("Train/LatentPerc_FreqLoss", latent_loss_dict['freq_loss'], global_step)
                        writer.add_scalar("Train/LatentPerc_MSLoss", latent_loss_dict['ms_loss'], global_step)
                        writer.add_scalar("Train/LatentPerc_ConsistencyLoss", latent_loss_dict['consistency_loss'], global_step)  # MOD2
                        writer.add_scalar("Train/LatentPerc_TotalLoss", latent_loss_dict['total_latent_loss'], global_step)

                # ä¿å­˜ Interval Checkpoint
                if global_step > 0 and global_step % cfg.save_interval_steps == 0:
                    save_path = os.path.join(save_dir, f"interval_step_{global_step}.pt")
                    save_checkpoint(
                        model, optimizer, scaler, epoch, global_step, best_val_loss, save_path
                    )

            global_step += 1

        # End of Epoch
        if is_master:
            pbar.close()  # å…³é—­è¿›åº¦æ¡
            avg_epoch_loss = epoch_loss / len(train_loader)
            time_elapsed = str(timedelta(seconds=int(time.time() - start_time)))
            print(f"\nâœ… Epoch {epoch} Done in {time_elapsed} | Avg Loss: {avg_epoch_loss:.5f}")
            
            # ä¿å­˜ Last Checkpoint (æ¯ä¸ª Epoch æ›´æ–°)
            save_path = os.path.join(save_dir, "last.pt")
            save_checkpoint(
                model, optimizer, scaler, epoch, global_step, best_val_loss, save_path
            )

        # ======================================================================
        # ğŸ§ª éªŒè¯å¾ªç¯ (æ¯ä¸ª Epoch ä¸€æ¬¡)
        # ======================================================================
        if is_master: print(f"ğŸ§ª Validating Epoch {epoch}...")

        val_loss, val_loss_std, val_latent_metrics = validate(
            model, val_loader, hr_mean, hr_std, lr_mean, lr_std, local_rank,
            cfg=cfg, latent_loss_fn=latent_loss_fn
        )

        if is_master:
            writer.add_scalar("Val/Loss", val_loss, epoch)
            writer.add_scalar("Val/Loss_Std", val_loss_std, epoch)  # Phase 1 æŒ‡æ ‡
            # MOD2: Latent Perceptual LosséªŒè¯æŒ‡æ ‡
            if val_latent_metrics:
                writer.add_scalar("Val/MSE_Loss", val_latent_metrics['mse_loss'], epoch)
                writer.add_scalar("Val/LatentPerc_FreqLoss", val_latent_metrics['freq_loss'], epoch)
                writer.add_scalar("Val/LatentPerc_MSLoss", val_latent_metrics['ms_loss'], epoch)
                writer.add_scalar("Val/LatentPerc_ConsistencyLoss", val_latent_metrics['consistency_loss'], epoch)  # MOD2
                writer.add_scalar("Val/LatentPerc_TotalLoss", val_latent_metrics['total_latent_loss'], epoch)
            print(f"ğŸ“‰ Val Loss: {val_loss:.5f} Â± {val_loss_std:.5f} (Best: {best_val_loss:.5f})")

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                save_path = os.path.join(save_dir, "best.pt")
                # Beståªå­˜çŠ¶æ€ï¼Œå¦‚æœç¡¬ç›˜å¤Ÿï¼Œä¹Ÿå¯ä»¥å­˜å®Œæ•´
                save_checkpoint(
                    model, optimizer, scaler, epoch, global_step, best_val_loss, save_path
                )
                print(f"ğŸ† New Best Model Saved!")
                
        dist.barrier() # åŒæ­¥ç­‰å¾…ä¸»è¿›ç¨‹ä¿å­˜å®Œæ¯•

    cleanup_ddp()

# ==============================================================================
# ğŸ§© è¾…åŠ©åŠŸèƒ½
# ==============================================================================

def validate(model, loader, hr_mean, hr_std, lr_mean, lr_std, device, cfg=None, latent_loss_fn=None):
    """
    DDP éªŒè¯ï¼šè®¡ç®—æ‰€æœ‰å¡çš„å¹³å‡ Loss å’Œæ ‡å‡†å·®
    MOD1: æ”¯æŒlatent perceptual loss
    è¿”å›: (avg_loss, loss_std, latent_metrics_dict)
    """
    model.eval()
    total_loss = torch.zeros(1).to(device)
    total_steps = torch.zeros(1).to(device)
    # æ”¶é›†æ¯ä¸ªbatchçš„lossç”¨äºè®¡ç®—æ ‡å‡†å·®
    all_losses = []

    # MOD2: Latent lossç´¯åŠ å™¨
    total_mse = torch.zeros(1).to(device)
    total_freq_loss = torch.zeros(1).to(device)
    total_ms_loss = torch.zeros(1).to(device)
    total_consistency_loss = torch.zeros(1).to(device)  # MOD2
    total_latent_loss = torch.zeros(1).to(device)

    with torch.no_grad():
        for hr, lr in loader:
            hr = hr.to(device, non_blocking=True)
            lr = lr.to(device, non_blocking=True)

            # Normalize
            hr_norm = (hr - hr_mean) / hr_std
            lr_norm = (lr - lr_mean) / lr_std

            # Fixed Timestep sampling for deterministic validation (Optional)
            # ä¹Ÿå¯ä»¥ç»§ç»­éšæœºï¼Œå› ä¸ºæ•°æ®é‡å¤Ÿå¤§
            t = torch.rand(hr.shape[0], device=device)

            noise = torch.randn_like(hr_norm)
            t_view = t.view(-1, 1, 1)
            z_t = t_view * hr_norm + (1 - t_view) * noise

            # éªŒè¯é›†ä¸€å®šè¦ç”¨ autocast ä¿æŒä¸€è‡´æ€§
            with torch.amp.autocast('cuda'):
                pred_x0 = model(z_t, t, lr_norm)

                # MOD3: ä¸»é‡å»ºæŸå¤± (Charbonnier or MSE)
                if cfg is not None and cfg.use_charbonnier_loss:
                    reconstruction_loss = charbonnier_loss(pred_x0, hr_norm, eps=cfg.charbonnier_eps)
                else:
                    reconstruction_loss = F.mse_loss(pred_x0, hr_norm)

                # MOD2: Latent Perceptual Loss
                if cfg is not None and cfg.use_latent_perceptual_loss and latent_loss_fn is not None:
                    # ğŸ”¥ MOD2: éªŒè¯æ—¶ä¸æ·»åŠ æ¡ä»¶å™ªå£°ï¼Œç›´æ¥ä½¿ç”¨ lr_norm
                    latent_perc_loss, latent_loss_dict = latent_loss_fn(pred_x0, hr_norm, lr_norm)
                    loss = (cfg.reconstruction_weight * reconstruction_loss +
                           cfg.latent_loss_weight * latent_perc_loss)

                    # ç´¯åŠ å„é¡¹æŒ‡æ ‡
                    total_mse += reconstruction_loss.detach()  # ä¿æŒå˜é‡åä¸ºtotal_mseä¾¿äºå…¼å®¹
                    total_freq_loss += latent_loss_dict['freq_loss']
                    total_ms_loss += latent_loss_dict['ms_loss']
                    total_consistency_loss += latent_loss_dict['consistency_loss']  # MOD2
                    total_latent_loss += latent_loss_dict['total_latent_loss']
                else:
                    loss = cfg.reconstruction_weight * reconstruction_loss if cfg is not None else reconstruction_loss

            total_loss += loss.detach()  # ä½¿ç”¨detach()é˜²æ­¢æ˜¾å­˜æ³„æ¼
            total_steps += 1
            all_losses.append(loss.item())

    # DDP All-Reduce: æ±‡æ€»æ‰€æœ‰å¡çš„ Loss
    dist.all_reduce(total_loss, op=dist.ReduceOp.SUM)
    dist.all_reduce(total_steps, op=dist.ReduceOp.SUM)

    # MOD2: All-Reduce latent metrics
    if cfg is not None and cfg.use_latent_perceptual_loss and latent_loss_fn is not None:
        dist.all_reduce(total_mse, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_freq_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_ms_loss, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_consistency_loss, op=dist.ReduceOp.SUM)  # MOD2
        dist.all_reduce(total_latent_loss, op=dist.ReduceOp.SUM)

    avg_loss = total_loss / total_steps

    # è®¡ç®—æ ‡å‡†å·® (åªåœ¨å½“å‰GPUä¸Šè®¡ç®—ï¼Œå› ä¸ºDDPä¼šå¯¼è‡´ä¸åŒGPUçœ‹åˆ°ä¸åŒæ•°æ®)
    if len(all_losses) > 1:
        loss_std = torch.tensor(all_losses).std().item()
    else:
        loss_std = 0.0

    # MOD2: æ„å»ºlatent metricså­—å…¸
    latent_metrics = {}
    if cfg is not None and cfg.use_latent_perceptual_loss and latent_loss_fn is not None:
        latent_metrics = {
            'mse_loss': (total_mse / total_steps).item(),
            'freq_loss': (total_freq_loss / total_steps).item(),
            'ms_loss': (total_ms_loss / total_steps).item(),
            'consistency_loss': (total_consistency_loss / total_steps).item(),  # MOD2
            'total_latent_loss': (total_latent_loss / total_steps).item()
        }

    model.train()
    return avg_loss.item(), loss_std, latent_metrics

def save_checkpoint(model, optimizer, scaler, epoch, step, best_loss, path):
    """ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€ (Perfect Resume)"""
    # è·å–åŸå§‹ model (å»é™¤ DDP wrapper)
    model_state = model.module.state_dict() if hasattr(model, "module") else model.state_dict()

    # ç§»é™¤ torch.compile() çš„ _orig_mod. å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if any(k.startswith('_orig_mod.') for k in model_state.keys()):
        model_state = {k.replace('_orig_mod.', ''): v for k, v in model_state.items()}

    # æ”¶é›† RNG çŠ¶æ€ï¼ˆå¤šGPUç¯å¢ƒï¼‰
    rng_state = {
        'python': random.getstate(),
        'numpy': np.random.get_state(),
        'torch': torch.get_rng_state(),
        'cuda_all': torch.cuda.get_rng_state_all()  # ä¿å­˜æ‰€æœ‰GPUçŠ¶æ€
    }
    
    state = {
        'epoch': epoch,
        'global_step': step,
        'best_val_loss': best_loss,
        'model_state_dict': model_state,
        'optimizer_state_dict': optimizer.state_dict(),
        'scaler_state_dict': scaler.state_dict(),
        'rng_state': rng_state,
        'config': TrainConfig.model_params # ä¿å­˜é…ç½®æ–¹ä¾¿æ¨ç†
    }
    
    torch.save(state, path)

if __name__ == "__main__":
    # å¿…é¡»ä¸º DDP è®¾ç½® spawn å¯åŠ¨æ–¹å¼
    # è™½ç„¶ torch.distributed.launch ä¹Ÿå¯ä»¥ï¼Œä½†ç›´æ¥ python è¿è¡Œæ›´æ–¹ä¾¿
    os.environ["OMP_NUM_THREADS"] = "1"
    
    # è·å–å¯ç”¨ GPU æ•°é‡å¹¶å¯åŠ¨å¤šè¿›ç¨‹
    n_gpus = torch.cuda.device_count()
    if n_gpus < 2:
        print(f"âš ï¸ Warning: Found {n_gpus} GPUs. DDP meant for multi-gpu.")
    
    # ä½¿ç”¨ torchrun é£æ ¼å¯åŠ¨ (æ¨è):
    # python -m torch.distributed.launch --nproc_per_node=2 train_ddp.py
    # æˆ–è€…ç›´æ¥è¿è¡Œæ­¤è„šæœ¬ (ä¼šè‡ªåŠ¨ fallback åˆ°å•æœºå•å¡æˆ–éœ€è¦æ‰‹åŠ¨ torchrun)
    
    # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œæ£€æµ‹å¦‚æœå·²ç»æ˜¯ rank ç¯å¢ƒç›´æ¥ mainï¼Œå¦åˆ™æç¤º
    if "LOCAL_RANK" in os.environ:
        main()
    else:
        print("è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒ (è‡ªåŠ¨åˆ©ç”¨åŒå¡):")
        print(f"torchrun --nproc_per_node={n_gpus} train_ddp.py")
#```

### ğŸƒâ€â™‚ï¸ å¯åŠ¨å‘½ä»¤ (Start Command)

#ç”±äºä½¿ç”¨äº† DDPï¼Œè¯·åŠ¡å¿…ä½¿ç”¨ `torchrun` å¯åŠ¨ï¼Œè¿™æ · PyTorch ä¼šè‡ªåŠ¨åˆ†é… Rank å’Œç«¯å£ï¼š

#```bash
#cd /home/husrcf/Code/AIAA/JaT
#conda activate AIAA

# å¯åŠ¨åŒå¡è®­ç»ƒ
#torchrun --nproc_per_node=2 train_ddp.py

### ğŸ”¬ ç›‘æ§è®­ç»ƒ (Monitor)

#åœ¨å¦ä¸€ä¸ªç»ˆç«¯æ‰“å¼€ TensorBoardï¼š

#```bash
#tensorboard --logdir=runs/v2_full_run --port 6006 --bind_all